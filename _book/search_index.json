[["decision-trees.html", "4 Decision Trees", " 4 Decision Trees One of our favorite machine learning algorithms is decision trees. They are not the most complex models, but they are quite intuitive. A single decision tree generally doesn’t have great “out of the box” model performance, and even with considerable model tuning they are unlikely to perform as well as other approaches. They do, however, form the building blocks for more complicated models that do have high out-of-the-box model performance and can produce state-of-the-art level predictions. Decision trees are non-parametric, meaning they do not make any assumptions about the underlying data generating process. By contrast, models like linear regression assume that the underlying data were generated by a standard normal distribution (and, if this is not the case, the model will result is systematic biases - although we can also use transformations and other strategies to help; see Feature Engineering). Note that assuming an underlying data generating distribution is not a weakness of linear regression - often it’s a tenable assumption and can regularly lead to better model performance, if the assumption holds. But decision trees do not require that you make any such assumptions, which is particularly helpful when it’s difficult to assume a specific underlying data generating distribution. At their core, decision trees work by splitting the features into a series of yes/no decisions. These splits divide the feature space into a series of non-overlapping regions, where the cases are similar in each region. To understand how a given prediction is made, one simply “follows” the splits of the tree to the terminal node. 4.0.1 A simple decision tree Initially, we think it’s easiest to think about decision trees through a classification lens. One of the most common examples is the titanic dataset library(tidyverse) titanic &lt;- read_csv(here::here(&quot;data&quot;, &quot;titanic.csv&quot;)) titanic ## # A tibble: 891 x 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 0 3 Brau… male 22 1 0 A/5 2… 7.25 &lt;NA&gt; ## 2 2 1 1 Cumi… fema… 38 1 0 PC 17… 71.3 C85 ## 3 3 1 3 Heik… fema… 26 0 0 STON/… 7.92 &lt;NA&gt; ## 4 4 1 1 Futr… fema… 35 1 0 113803 53.1 C123 ## 5 5 0 3 Alle… male 35 0 0 373450 8.05 &lt;NA&gt; ## 6 6 0 3 Mora… male NA 0 0 330877 8.46 &lt;NA&gt; ## 7 7 0 1 McCa… male 54 0 0 17463 51.9 E46 ## 8 8 0 3 Pals… male 2 3 1 349909 21.1 &lt;NA&gt; ## 9 9 1 3 John… fema… 27 0 2 347742 11.1 &lt;NA&gt; ## 10 10 1 2 Nass… fema… 14 1 0 237736 30.1 &lt;NA&gt; ## # … with 881 more rows, and 1 more variable: Embarked &lt;chr&gt; "]]
