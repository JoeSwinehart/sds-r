[["index.html", "Social Data Science with R 1 Preface", " Social Data Science with R Daniel Anderson Brendan Cullen Ouafaa Hmaddi 2020-11-16 1 Preface Here’s an intro about why R is great and the cool things you can do with it and new problems you can address. "],["welcome.html", "2 Welcome", " 2 Welcome It’s a vast world out there. We’re going to cover a lot of ground. Don’t be scared. It’s going to be so. much. FUN! I’m just playing right now and specifically thinking about code styling. So here’s a code chunk for producing a very basic plot. # load the libraries library(tidyverse) # create a plot ggplot(mpg, aes(displ, cty)) + geom_point() + geom_smooth() We could of course create some summary data too. # create some summary data mpg %&gt;% group_by(cyl) %&gt;% summarize(mean_cty = mean(cty), sd_cty = sd(cty)) ## # A tibble: 4 x 3 ## cyl mean_cty sd_cty ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 21.0 3.50 ## 2 5 20.5 0.577 ## 3 6 16.2 1.77 ## 4 8 12.6 1.81 But the above doesn’t give us an example of a string, which we’ll also use a lot, so let’s try an example with that. mpg %&gt;% filter(manufacturer == &quot;audi&quot;) ## # A tibble: 18 x 11 ## manufacturer model displ year cyl trans drv cty hwy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 ## 2 audi a4 1.8 1999 4 manu… f 21 29 ## 3 audi a4 2 2008 4 manu… f 20 31 ## 4 audi a4 2 2008 4 auto… f 21 30 ## 5 audi a4 2.8 1999 6 auto… f 16 26 ## 6 audi a4 2.8 1999 6 manu… f 18 26 ## 7 audi a4 3.1 2008 6 auto… f 18 27 ## 8 audi a4 q… 1.8 1999 4 manu… 4 18 26 ## 9 audi a4 q… 1.8 1999 4 auto… 4 16 25 ## 10 audi a4 q… 2 2008 4 manu… 4 20 28 ## 11 audi a4 q… 2 2008 4 auto… 4 19 27 ## 12 audi a4 q… 2.8 1999 6 auto… 4 15 25 ## 13 audi a4 q… 2.8 1999 6 manu… 4 17 25 ## 14 audi a4 q… 3.1 2008 6 auto… 4 17 25 ## 15 audi a4 q… 3.1 2008 6 manu… 4 15 25 ## 16 audi a6 q… 2.8 1999 6 auto… 4 15 24 ## 17 audi a6 q… 3.1 2008 6 auto… 4 17 25 ## 18 audi a6 q… 4.2 2008 8 auto… 4 16 23 ## # … with 2 more variables: fl &lt;chr&gt;, class &lt;chr&gt; Finally, let’s write a quick function that has a logical in it (since we haven’t tested those yet). mn &lt;- function(x) { mean(x, na.rm = TRUE) } mn(mpg$hwy) ## [1] 23.44017 And overall, I’m pretty happy with how that’s all looking. Here’s an example div tip. We can play around with fonts and colors. The title of this divtip Here’s another div tip. To use for important definitions? "],["feature-engineering.html", "3 Feature Engineering", " 3 Feature Engineering Feature engineering is a fancy machine learning way of saying “prepare your data for analysis”. But it’s also more than just getting your data in the right format. It’s about transforming variables to help the model provide more stable predictions – for example, encoding, modeling, or omitting missing data; creating new variables from the available dataset; and otherwise helping improve the model performance through transformations and modifications to the existing data before analysis. One of the biggest challenges with feature engineering is what’s referred to as data leakage, which occurs when information from the test dataset leaks into the training dataset. Consider the simple example of normalizing or standardizing a variable (subtracting the variable mean from each value and then dividing by the standard deviation). Normalizing your variables is necessary for a wide range of predictive models, including any model using regularization methods, principal components analysis, and \\(k\\)-nearest neighbor models, among others. But when we normalize the variable, it is critical we do so relative to the training set, not relative to the full dataset. If we normalize the numeric variables in our full dataset and then divide it into test and training datasets, information from the test dataset has leaked into the training dataset. This seems simple enough - just wait to standardize until after splitting - but it becomes more complicated when we consider tuning a model. If we use a process like \\(k\\)-fold cross-validation, then we have to normalize within each fold to get an accurate representation of out-of-sample predictive accuracy. In this chapter, we’ll introduce feature engineering using the {recipes} package from the {tidymodels} suite of packages which, as we will see, helps you to be explicit about the decisions you’re making, while avoiding potential issues with data leakage. "],["basics-of-recipes.html", "3.1 Basics of {recipes}", " 3.1 Basics of {recipes} The {recipes} package is designed to replace the stats::model.matrix() function that you might be familiar with. For example, if you fit a model like the one below library(palmerpenguins) m1 &lt;- lm(bill_length_mm ~ species, data = penguins) summary(m1) ## ## Call: ## lm(formula = bill_length_mm ~ species, data = penguins) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.9338 -2.2049 0.0086 2.0662 12.0951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.7914 0.2409 161.05 &lt;2e-16 *** ## speciesChinstrap 10.0424 0.4323 23.23 &lt;2e-16 *** ## speciesGentoo 8.7135 0.3595 24.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.96 on 339 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.7078, Adjusted R-squared: 0.7061 ## F-statistic: 410.6 on 2 and 339 DF, p-value: &lt; 2.2e-16 You can see that our species column, which has the values Adelie, Gentoo, Chinstrap, is automatically dummy-coded for us, with the first level in the factor variable (Adelie) set as the reference group. The {recipes} package forces you to be a bit more explicit in these decisions. But it also has a much wider range of modifications it can make to the data. Using {recipes} also allows you to more easily separate the pre-processing and modeling stages of your data analysis workflow. In the above example, you may not have even realized stats::model.matrix() was doing anything for you because it’s wrapped within the stats::lm() modeling code. But with {recipes}, you make the modifications to your data first and then conduct your analysis. How does this separation work? With {recipes}, you can create a blueprint (or recipe) to apply to a given dataset, without actually applying those operations. You can then use this blueprint iteratively across sets of data (e.g., folds) as well as on new (potentially unseen) data that has the same structure (variables). This process helps avoid data leakage because all operations are carried forward and applied together, and no operations are conducted until explicitly requested. "],["creating-a-recipe.html", "3.2 Creating a recipe", " 3.2 Creating a recipe Let’s read in some data and begin creating a basic recipe. We’ll work with the simulated statewide testing data introduced previously. This is a fairly decent sized dataset, and since we’re just illustrating concepts here, we’ll pull a random sample of 2% of the total data to make everything run a bit quicker. We’ll also remove the classification variable, which is just a categorical version of score, our outcome. In the chunk below, we read in the data, sample a random 2% of the data (being careful to set a seed first so our results are reproducible), split it into training and test sets, and extract just the training dataset. We’ll hold off on splitting it into CV folds for now. library(tidyverse) library(tidymodels) set.seed(8675309) full_train &lt;- read_csv(&quot;https://github.com/uo-datasci-specialization/c4-ml-fall-2020/raw/master/data/train.csv&quot;) %&gt;% slice_sample(prop = 0.02) %&gt;% select(-classification) splt &lt;- initial_split(full_train) train &lt;- training(splt) A quick reminder, the data look like this And you can see the full data dictionary on the Kaggle website here. When creating recipes, we can still use the formula interface to define how the data will be modeled. In this case, we’ll say that the score column is predicted by everything else in the data frame. rec &lt;- recipe(score ~ ., data = train) Notice that I still declare the dataset (in this case, the training data), even though this is just a blueprint. It uses the dataset I provide to get the names of the columns, but it doesn’t actually do anything with this dataset (unless we ask it to). Let’s look at what this recipe looks like. rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 38 Notice it just states that this is a data recipe in which we have specified 1 outcome variable and 38 predictors. We can prep this recipe to learn more. prep(rec) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 38 ## ## Training data contained 2841 data points and 2841 incomplete rows. Notice we now get an additional message about how many rows are in the data, and how many of these rows contain missing (incomplete data). So the recipe is the blueprint, and we prep the recipe to get it to actually go into the data and conduct the operations. The dataset it has now, however, is just a placeholder than can be substituted in for any other dataset with an equivalent structure. But of course, modeling score as the outcome with everything else predicting it is not a reasonable choice in this case. For example, we have many ID variables, and we also have multiple categorical variables. For some methods (like tree-based models) it might be okay to leave these categorical variables as they are, but for others (like any model in the linear regression family) we’ll want to encode them somehow (e.g., dummy code). We can address these concerns by adding steps to our recipe. In the first step, we’ll update the role of all the ID variables so they are not included among the predictors. In the second, we will dummy code all nominal (i.e. categorical) variables. rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_dummy(all_nominal()) When updating the roles, we can change the variable label (text passed to the new_role argument) to be anything we want, so long as it’s not \"predictor\" or \"outcome\". Notice in the above I am also using helper functions to apply the operations to all variables of a specific type. There are five main helper functions for creating recipes: all_predictors(), all_outcomes(), all_nominal(), all_numeric() and has_role(). You can use these together, including with negation (e.g., -all_outcomes() to specify the operation should not apply to the outcome variable(s)) to select any set of variables you want to apply the operation to. Let’s try to prep this updated recipe. prep(rec) ## Error: Only one factor level in lang_cd Uh oh! We have an error. Our recipe is trying to dummy code the lang_cd variable, but it has only one level. It’s kind of hard to dummy-code a constant! Luckily, we can expand our recipe to first remove any zero-variance predictors, like so: rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_zv(all_predictors()) %&gt;% step_dummy(all_nominal()) The _zv part stands for “zero variance” and should take care of this problem. Let’s try again. prep(rec) ## Data Recipe ## ## Inputs: ## ## role #variables ## id vars 6 ## outcome 1 ## predictor 32 ## ## Training data contained 2841 data points and 2841 incomplete rows. ## ## Operations: ## ## Zero variance filter removed calc_admn_cd, lang_cd [trained] ## Dummy variables from gndr, ethnic_cd, tst_bnch, ... [trained] Beautiful! Note we do still get a warning here, but I’ve omitted it in the text (we’ll come back to this in the section on Missing data). Our recipe says we now have 6 ID variables, 1 outcome, and 32 predictors, with 2841 data points (rows of data). The calc_admn_cd and lang_cd variables have been removed because they have zero variance, and several variables have been dummy coded, including gndr and ethnic_cd, among others. Let’s dig just a bit deeper here though. What’s going on with these zero-variance variables? Let’s look back at the training data. train %&gt;% count(calc_admn_cd) ## # A tibble: 1 x 2 ## calc_admn_cd n ## &lt;lgl&gt; &lt;int&gt; ## 1 NA 2841 train %&gt;% count(lang_cd) ## # A tibble: 2 x 2 ## lang_cd n ## &lt;chr&gt; &lt;int&gt; ## 1 S 80 ## 2 &lt;NA&gt; 2761 So at least in our sample, calc_admn_cd really is just fully missing, which means it might as well be dropped because it’s providing us exactly nothing. But that’s not the case with lang_cd. It has two values, NA and \"S\" (for “Spanish”). This variable represents the language the test was administered in and the NA values are actually meaningful here because they are the the “default” administration, meaning English. So rather than dropping these, let’s mutate them to transform the NA values to \"E\" for English. We could reasonably do this inside or outside the recipe, but a good rule of thumb is, if it can go in the recipe, put it in the recipe. It can’t hurt, and doing operations outside of the recipe risks data leakage. rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_mutate(lang_cd = ifelse(is.na(lang_cd), &quot;E&quot;, lang_cd)) %&gt;% step_zv(all_predictors()) %&gt;% step_dummy(all_nominal()) Let’s take a look at what our data would actually look like when applying this recipe now. First, we’ll prep the recipe prepped &lt;- prep(rec) prepped ## Data Recipe ## ## Inputs: ## ## role #variables ## id vars 6 ## outcome 1 ## predictor 32 ## ## Training data contained 2841 data points and 2841 incomplete rows. ## ## Operations: ## ## Variable mutation for lang_cd [trained] ## Zero variance filter removed calc_admn_cd [trained] ## Dummy variables from gndr, ethnic_cd, tst_bnch, ... [trained] And we see that lang_cd is no longer being caught by the zero variance filter. Next we’ll bake the prepped recipe to actually apply it to our data. If we specify new_data = NULL, bake() will apply the operation to the data we originally specified in the recipe. But we can also pass new data as an additional argument and it will apply the operations to that data instead of the data specified in the recipe. bake(prepped, new_data = NULL) ## # A tibble: 2,841 x 106 ## id attnd_dist_inst… attnd_schl_inst… enrl_grd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 62576 2083 1353 7 ## 2 71424 2180 878 6 ## 3 179893 2244 1334 3 ## 4 136083 2142 4858 5 ## 5 196809 2212 1068 3 ## 6 13931 2088 581 8 ## 7 103344 1926 102 6 ## 8 105122 2142 766 6 ## 9 172543 1965 197 4 ## 10 45153 2083 542 6 ## # … with 2,831 more rows, and 102 more variables: ## # partic_dist_inst_id &lt;dbl&gt;, partic_schl_inst_id &lt;dbl&gt;, ## # lang_cd &lt;fct&gt;, ncessch &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, ## # score &lt;dbl&gt;, gndr_M &lt;dbl&gt;, ethnic_cd_B &lt;dbl&gt;, ## # ethnic_cd_H &lt;dbl&gt;, ethnic_cd_I &lt;dbl&gt;, ethnic_cd_M &lt;dbl&gt;, ## # ethnic_cd_P &lt;dbl&gt;, ethnic_cd_W &lt;dbl&gt;, tst_bnch_X2B &lt;dbl&gt;, ## # tst_bnch_X3B &lt;dbl&gt;, tst_bnch_G4 &lt;dbl&gt;, tst_bnch_G6 &lt;dbl&gt;, ## # tst_bnch_G7 &lt;dbl&gt;, tst_dt_X3.21.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X3.22.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X3.23.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X3.8.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X3.9.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.10.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.11.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.12.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.13.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.16.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.17.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.18.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.19.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.2.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.20.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.23.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.24.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.25.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.26.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.27.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.30.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.5.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.6.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X4.9.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.1.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.10.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.11.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.14.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.15.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.16.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.17.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.18.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.2.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.21.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.22.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.23.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.24.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.25.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.29.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.3.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.30.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.31.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.4.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.7.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.8.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X5.9.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X6.1.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X6.4.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X6.5.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X6.6.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X6.7.2018.0.00.00 &lt;dbl&gt;, ## # tst_dt_X6.8.2018.0.00.00 &lt;dbl&gt;, migrant_ed_fg_Y &lt;dbl&gt;, ## # ind_ed_fg_Y &lt;dbl&gt;, sp_ed_fg_Y &lt;dbl&gt;, tag_ed_fg_Y &lt;dbl&gt;, ## # econ_dsvntg_Y &lt;dbl&gt;, ayp_lep_B &lt;dbl&gt;, ayp_lep_E &lt;dbl&gt;, ## # ayp_lep_F &lt;dbl&gt;, ayp_lep_M &lt;dbl&gt;, ayp_lep_N &lt;dbl&gt;, ## # ayp_lep_S &lt;dbl&gt;, ayp_lep_W &lt;dbl&gt;, ayp_lep_X &lt;dbl&gt;, ## # ayp_lep_Y &lt;dbl&gt;, stay_in_dist_Y &lt;dbl&gt;, ## # stay_in_schl_Y &lt;dbl&gt;, dist_sped_Y &lt;dbl&gt;, ## # trgt_assist_fg_Y &lt;dbl&gt;, ayp_dist_partic_Y &lt;dbl&gt;, ## # ayp_schl_partic_Y &lt;dbl&gt;, ayp_dist_prfrm_Y &lt;dbl&gt;, ## # ayp_schl_prfrm_Y &lt;dbl&gt;, rc_dist_partic_Y &lt;dbl&gt;, ## # rc_schl_partic_Y &lt;dbl&gt;, rc_dist_prfrm_Y &lt;dbl&gt;, ## # rc_schl_prfrm_Y &lt;dbl&gt;, tst_atmpt_fg_Y &lt;dbl&gt;, ## # grp_rpt_dist_partic_Y &lt;dbl&gt;, grp_rpt_schl_partic_Y &lt;dbl&gt;, … And now we can actually see the dummy-coded categorical variables, along with the other operations we requested. For example, calc_admn_cd is not in the dataset. Notice the ID variables are output though, which makes sense because they are often neccessary for joining with other data sources. But it’s important to realize that they are output (i.e., all variables are returned, regardless of role) because if we passed this directly to a model they would be included as predictors. Note that there may be reasons you would want to include a school and/or district level ID variable in your modeling, but you certainly would not want redundant variables. We do still have one minor issue with this recipe though, which is pretty evident when looking at the column names of our baked dataset. The tst_dt variable, which specifies the date the test was taken, was treated as a categorical variable because it was read in as a character vector. That means all the dates are being dummy coded! Let’s fix this by just transforming it to a date within our step_mutate. rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), &quot;E&quot;, lang_cd)), tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% step_zv(all_predictors()) %&gt;% step_dummy(all_nominal()) And now when we prep/bake the dataset it’s still a date variable, which is what we probably want (it will be modeled as a numeric variable). rec %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 2,841 x 55 ## id attnd_dist_inst… attnd_schl_inst… enrl_grd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 62576 2083 1353 7 ## 2 71424 2180 878 6 ## 3 179893 2244 1334 3 ## 4 136083 2142 4858 5 ## 5 196809 2212 1068 3 ## 6 13931 2088 581 8 ## 7 103344 1926 102 6 ## 8 105122 2142 766 6 ## 9 172543 1965 197 4 ## 10 45153 2083 542 6 ## # … with 2,831 more rows, and 51 more variables: tst_dt &lt;dttm&gt;, ## # partic_dist_inst_id &lt;dbl&gt;, partic_schl_inst_id &lt;dbl&gt;, ## # ncessch &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, score &lt;dbl&gt;, ## # gndr_M &lt;dbl&gt;, ethnic_cd_B &lt;dbl&gt;, ethnic_cd_H &lt;dbl&gt;, ## # ethnic_cd_I &lt;dbl&gt;, ethnic_cd_M &lt;dbl&gt;, ethnic_cd_P &lt;dbl&gt;, ## # ethnic_cd_W &lt;dbl&gt;, tst_bnch_X2B &lt;dbl&gt;, tst_bnch_X3B &lt;dbl&gt;, ## # tst_bnch_G4 &lt;dbl&gt;, tst_bnch_G6 &lt;dbl&gt;, tst_bnch_G7 &lt;dbl&gt;, ## # migrant_ed_fg_Y &lt;dbl&gt;, ind_ed_fg_Y &lt;dbl&gt;, sp_ed_fg_Y &lt;dbl&gt;, ## # tag_ed_fg_Y &lt;dbl&gt;, econ_dsvntg_Y &lt;dbl&gt;, ayp_lep_B &lt;dbl&gt;, ## # ayp_lep_E &lt;dbl&gt;, ayp_lep_F &lt;dbl&gt;, ayp_lep_M &lt;dbl&gt;, ## # ayp_lep_N &lt;dbl&gt;, ayp_lep_S &lt;dbl&gt;, ayp_lep_W &lt;dbl&gt;, ## # ayp_lep_X &lt;dbl&gt;, ayp_lep_Y &lt;dbl&gt;, stay_in_dist_Y &lt;dbl&gt;, ## # stay_in_schl_Y &lt;dbl&gt;, dist_sped_Y &lt;dbl&gt;, ## # trgt_assist_fg_Y &lt;dbl&gt;, ayp_dist_partic_Y &lt;dbl&gt;, ## # ayp_schl_partic_Y &lt;dbl&gt;, ayp_dist_prfrm_Y &lt;dbl&gt;, ## # ayp_schl_prfrm_Y &lt;dbl&gt;, rc_dist_partic_Y &lt;dbl&gt;, ## # rc_schl_partic_Y &lt;dbl&gt;, rc_dist_prfrm_Y &lt;dbl&gt;, ## # rc_schl_prfrm_Y &lt;dbl&gt;, lang_cd_E &lt;dbl&gt;, ## # tst_atmpt_fg_Y &lt;dbl&gt;, grp_rpt_dist_partic_Y &lt;dbl&gt;, ## # grp_rpt_schl_partic_Y &lt;dbl&gt;, grp_rpt_dist_prfrm_Y &lt;dbl&gt;, ## # grp_rpt_schl_prfrm_Y &lt;dbl&gt; 3.2.1 Order matters It’s important to realize that the order of the steps matters. In our recipe, we first declare ID variables as having a different role than predictors or outcomes. We then modify two variables, remove zero-variance predictors, and finally dummy code all categorical (nominal) variables. What happens if we instead dummy code and then remove zero-variance predictors? rec &lt;- recipe(score ~ ., train) %&gt;% step_dummy(all_nominal()) %&gt;% step_zv(all_predictors()) prep(rec) ## Error: Only one factor level in lang_cd We end up with our original error. We don’t get this error if we remove zero variance predictors and then dummy code. rec &lt;- recipe(score ~ ., train) %&gt;% step_zv(all_predictors()) %&gt;% step_dummy(all_nominal()) prep(rec) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 38 ## ## Training data contained 2841 data points and 2841 incomplete rows. ## ## Operations: ## ## Zero variance filter removed calc_admn_cd, lang_cd [trained] ## Dummy variables from gndr, ethnic_cd, tst_bnch, ... [trained] The fact that order matters may occasionally require that you apply the same operation at multiple steps (e.g., a near zero variance filter could be applied before and after dummy-coding). All of the above serves as a basic introduction to developing a recipe, and what follows goes into more detail on specific feature engineering pieces. For complete information on all possible recipe steps, please see the documentation. "],["encoding-categorical-data.html", "3.3 Encoding categorical data", " 3.3 Encoding categorical data For many (but not all) modeling frameworks, categorical data must be transformed somehow. The most common strategy for this is dummy coding, which stats::model.matrix() will do for you automatically using the stats::contrasts() function. There are also other coding schemes you can use with base R, such as Helmert and polynomial coding (see ?contrasts and related functions). Dummy coding leaves one group out (the first level of the factor, by default) and creates new columns for all the other groups coded \\(0\\) or \\(1\\) depending on whether the original variable represented that value or not. For example: f &lt;- factor(c(&quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;green&quot;, &quot;green&quot;)) contrasts(f) ## green red ## blue 0 0 ## green 1 0 ## red 0 1 In the above, \"blue\" has been assigned as the reference category (note that factor levels are assigned in alphabetical order by default), and dummy variables have been created for \"green\" and \"red\". In a linear regression framework, \"blue\" would become the intercept. We can recreate this same coding scheme with {recipes}, but we need to first put it in a data frame. df &lt;- data.frame(f, score = rnorm(length(f))) df ## f score ## 1 red 1.0734291 ## 2 red -0.2675359 ## 3 blue 0.7512238 ## 4 green 0.5436071 ## 5 green 0.6940371 ## 6 green -0.6446104 recipe(score ~ f, data = df) %&gt;% step_dummy(f) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 6 x 3 ## score f_green f_red ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.07 0 1 ## 2 -0.268 0 1 ## 3 0.751 0 0 ## 4 0.544 1 0 ## 5 0.694 1 0 ## 6 -0.645 1 0 In the above, we’ve created the actual columns we need, while in the base example we only created the contrast matrix (although it’s relatively straightforward to then create the columns). The {recipes} version is, admittedly, a fair amount of additional code, but as we saw in the previous section, {recipes} is capable of making a wide range of transformation in a systematic way. 3.3.1 Transformations beyond dummy coding Although less used in inferential statistics, there are a number of additional transformations we can use to encode categorical data. The most straightforward is one-hot encoding. One-hot encoding is essentially equivalent to dummy coding except we create the variables for all levels in the categorical variable (i.e., we do not leave one out as a reference group). This generally makes them less useful in linear regression frameworks (unless the model intercept is dropped), but they can be highly useful in a number of other frameworks, such as tree-based methods (covered later in the book). To use one-hot encoding, we pass the additional one_hot argument to step_dummy(). recipe(score ~ f, data = df) %&gt;% step_dummy(f, one_hot = TRUE) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 6 x 4 ## score f_blue f_green f_red ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.07 0 0 1 ## 2 -0.268 0 0 1 ## 3 0.751 1 0 0 ## 4 0.544 0 1 0 ## 5 0.694 0 1 0 ## 6 -0.645 0 1 0 Another relatively common encoding scheme, particularly within natural language processing frameworks, is integer encoding, where each level is associated with a unique integer. For example recipe(score ~ f, data = df) %&gt;% step_integer(f) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 6 x 2 ## f score ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 1.07 ## 2 3 -0.268 ## 3 1 0.751 ## 4 2 0.544 ## 5 2 0.694 ## 6 2 -0.645 Notice that the syntax is essentially equivalent to the previous dummy-coding example, but we’ve just swapped out step_dummy() for step_integer(). Integer encoding can be useful in natural language processing in particular because words can be encoded as integers, and then the algorithm can search for patterns in the numbers. 3.3.2 Handling new levels One other very common problem with encoding categorical data is how to handle new, unseen levels. For example, let’s take a look at the recipe below: rec &lt;- recipe(score ~ f, data = df) %&gt;% step_dummy(f) We will have no problem creating dummy variables within this recipe as long as the levels of \\(f\\) are within those contained in df$f (or, more mathematically, where \\(f \\in F\\)). But what if, in a new sample, \\(f =\\) “purple” or \\(f =\\) “gray”? Let’s try and see what happens. df2 &lt;- data.frame(f = factor(c(&quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;gray&quot;)), score = rnorm(4)) rec %&gt;% prep() %&gt;% bake(new_data = df2) ## # A tibble: 4 x 3 ## score f_green f_red ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.39 0 0 ## 2 0.616 1 0 ## 3 -2.35 NA NA ## 4 -0.165 NA NA We end up propagating missing data, which is obviously less than ideal. Luckily, the solution is pretty straightforward. We just add a new step to our recipe to handle novel (or new) categories, lumping them all in their own level (labeled with the suffix _new). rec &lt;- recipe(score ~ f, data = df) %&gt;% step_novel(f) %&gt;% step_dummy(f) rec %&gt;% prep() %&gt;% bake(new_data = df2) ## # A tibble: 4 x 4 ## score f_green f_red f_new ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.39 0 0 0 ## 2 0.616 1 0 0 ## 3 -2.35 0 0 1 ## 4 -0.165 0 0 1 This is not perfect, because \"purple\" and \"orange\" may be highly different, and we’re modeling them as a single category. But at least we’re able to move forward with our model without introducing new missing data. As an aside, this is a small example of why having good training data is so important. If you don’t have all the levels of a categorical variable represented, you may end up essentially collapsing levels when there is meaningful variance that could be parsed out. You can also use a similar approach with step_other() if you have a categorical variable with lots of levels (and a small-ish \\(n\\) by comparison). Using step_other(), you specify a threshold below which levels should be collapsed into a single “other” category. The threshold can be passed as a proportion or a frequency. 3.3.3 Final thoughts on encoding categorical data There are, of course, many other ways you can encode categorical data. One important consideration is whether or not the variable is ordered (e.g., low, medium, high) in which case it may make sense to have a corresponding ordered numeric variable (e.g., \\(0\\), \\(1\\), \\(2\\)). Of course, the method of coding these ordered values will relate to assumptions of your modeling process. For example, in the previous example, we assumed that there is a linear, constant change across categories. In our experience, however, the combination of dummy coding (with potentially a one-hot alternative used), integer coding, or simply leaving the categorical variables as they are (for specific frameworks, like tree-based methods) is sufficient most (but not all) of the time. For a more complete discussion of encoding categorical data for predictive modeling frameworks, we recommend Chapter 5 of Kuhn &amp; Johnson (2019). "],["dealing-with-low-variance-predictors.html", "3.4 Dealing with low variance predictors", " 3.4 Dealing with low variance predictors Occasionally you have (or can create) variables that are highly imbalanced. A common example might include a gender variable that takes on the values “male”, “female”, “non-binary”, “other”, and “refused to answer”. Once you dummy-code a variable like this, it is possible that one or more of the categories may be so infrequent that it makes modeling that category difficult. This is not to say that these categories are not important, particularly when considering the representation of your training dataset to real-world applications (and any demographic variable is going to be associated with issues of ethics). Ignoring this variation may lead to systematic biases in model predictions. However, you also regularly have to make compromises to get models to work and be useful. One of those compromises often includes (with many types of variables, not just demographics) dropping highly imbalanced predictors. Let’s look back at our statewide testing data. Let’s bake the final recipe from our Creating a recipe section on the training data and look at the dummy variables that are created. rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), &quot;E&quot;, lang_cd)), tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% step_zv(all_predictors()) %&gt;% step_dummy(all_nominal()) baked &lt;- rec %&gt;% prep() %&gt;% bake(new_data = NULL) Below is a table of just the categorical variables and the frequency of each value. The relative frequency of many of these looks fine, but for some one category has very low frequency. For example, ayp_lep_M has 576 observations (from our random 2% sample) that were \\(0\\), and only 2 that were \\(1\\). This is the same for ayp_lep_S. We may therefore consider applying a near-zero variance filter to drop these columns. Let’s try this, and then we’ll talk a bit more about what the filter is actually doing. rec_nzv &lt;- rec %&gt;% step_nzv(all_predictors()) baked_rm_nzv &lt;- rec_nzv %&gt;% prep() %&gt;% bake(new_data = NULL) Let’s look at what columns are in baked that were removed from baked_rm_nzv. removed_columns &lt;- names(baked)[!(names(baked) %in% names(baked_rm_nzv))] removed_columns ## [1] &quot;ethnic_cd_B&quot; &quot;ethnic_cd_I&quot; ## [3] &quot;ethnic_cd_P&quot; &quot;migrant_ed_fg_Y&quot; ## [5] &quot;ind_ed_fg_Y&quot; &quot;ayp_lep_B&quot; ## [7] &quot;ayp_lep_M&quot; &quot;ayp_lep_S&quot; ## [9] &quot;ayp_lep_W&quot; &quot;stay_in_dist_Y&quot; ## [11] &quot;stay_in_schl_Y&quot; &quot;dist_sped_Y&quot; ## [13] &quot;trgt_assist_fg_Y&quot; &quot;ayp_dist_partic_Y&quot; ## [15] &quot;ayp_schl_partic_Y&quot; &quot;ayp_dist_prfrm_Y&quot; ## [17] &quot;ayp_schl_prfrm_Y&quot; &quot;rc_dist_partic_Y&quot; ## [19] &quot;rc_schl_partic_Y&quot; &quot;rc_dist_prfrm_Y&quot; ## [21] &quot;rc_schl_prfrm_Y&quot; &quot;lang_cd_E&quot; ## [23] &quot;tst_atmpt_fg_Y&quot; &quot;grp_rpt_dist_partic_Y&quot; ## [25] &quot;grp_rpt_schl_partic_Y&quot; &quot;grp_rpt_dist_prfrm_Y&quot; ## [27] &quot;grp_rpt_schl_prfrm_Y&quot; As you can see, the near-zero variance filter has been quite aggressive here, removing 27 columns. Looking back at our table of variables, we can see that, for example, there are 55 students coded Black out of 2841, and it could be reasonably argued that this column is worth keeping in the model. So how is step_nzv working and how can we adjust it to be not quite so aggressive? Variables are flagged for being near-zero variance if they Have very few unique values, and The frequency ratio for the most common value to the second most common value is large These criteria are implemented in step_nzv through the unique_cut and freq_cut arguments, respectively. unique_cut is estimated as the number of unique values divided by the total number of samples (length of the column) times 100 (i.e., it is a percent). freq_cut is estimated by dividing the most common level frequency by the second most common level frequency. The default for unique_cut is \\(10\\), while the default for freq_cut is \\(95/5 = 19\\). For a column to be “caught” by a near-zero variance filter, and removed from the training set, it must be below the specified unique_cut and above the specified freq_cut. In the case of ethnic_cd_B, we see that there are two unique values, \\(0\\) and \\(1\\) (because it’s a dummy-coded variable). There are 2841 rows, so the unique_cut value is \\((2 / 2841) \\times 100 = 0.07\\). The frequency ratio is \\(2786/55 = 50.65\\). It therefore meets both of the default criteria (below unique_cut and above freq_cut) and is removed. If you’re applying a near-zero variance filter on dummy variables, there will always be only 2 values, leading to a small unique_cut. This might encourage you to up the freq_cut to a higher value. Let’s try this approach rec_nzv2 &lt;- rec %&gt;% step_nzv(all_predictors(), freq_cut = 99/1) baked_rm_nzv2 &lt;- rec_nzv2 %&gt;% prep() %&gt;% bake(new_data = NULL) removed_columns2 &lt;- names(baked)[!(names(baked) %in% names(baked_rm_nzv2))] removed_columns2 ## [1] &quot;ethnic_cd_P&quot; &quot;ayp_lep_M&quot; ## [3] &quot;ayp_lep_S&quot; &quot;ayp_lep_W&quot; ## [5] &quot;dist_sped_Y&quot; &quot;ayp_dist_partic_Y&quot; ## [7] &quot;ayp_schl_partic_Y&quot; &quot;rc_dist_partic_Y&quot; ## [9] &quot;rc_schl_partic_Y&quot; &quot;tst_atmpt_fg_Y&quot; ## [11] &quot;grp_rpt_dist_partic_Y&quot; &quot;grp_rpt_schl_partic_Y&quot; ## [13] &quot;grp_rpt_dist_prfrm_Y&quot; Removing near-zero variance dummy variables can be a bit tricky because they will essentially always meet the unique_cut criteria. But it can be achieved by fiddling with the freq_cut variable and, actually, could be considered part of your model tuning process. In this case, we’ve set it so variables will be removed if greater than 99 out of every 100 cases is the same. This led to only 13 variables being flagged and removed. But we could continue on even further specifying, for example, that 499 out of every 500 must be the same for the variable to be flagged. At some point, however, you’ll end up with variables that have such low variance that model estimation becomes difficult, which is the purpose of applying the near-zero variance filter in the first place. "],["missing-data.html", "3.5 Missing data", " 3.5 Missing data If we look closely at our statewide testing data, we will see that there is a considerable amount of missingness. In fact, every row of the data frame has at least one value that is missing. The amount to which missing data impacts your work varies by field, but in most fields you’re likely to run into situations where you have to handle missing data in some way. The purpose of this section is to discuss a few approaches using {recipes} to handle missingness for predictive modeling purposes. Note that this is not a comprehensive discussion on the topic (for which, we recommend Little and Rubin (2002)), but is instead an applied discussion of what you can do. As with many aspects of data analysis, generally, there is no single approach that will always work best, and it’s worth trying a few different approaches in your model development to see how different choices impact your model performance. There are three basic ways of handling missing data: Omit rows of the data frame that include missing values Encode or Impute the missing data Ignore the missing data and estimate from the available data The last option is not always feasible and will depend the modeling framework you’re working within. Some estimation procedures can also lend themselves to efficient handling of missing data (for example, imputation via the posterior distribution with Bayesian estimation). In this section, we’ll mostly focus on the first two approaches. Additionally, we will only really be concerned with missingness on the predictor variables here, rather than the outcome. Generally, missing data in the predictors is a much more difficult problem than missingness in the outcome, because most models assume you have complete data across your predictors. This is not to say that missingness on your outcome is not challenging (it can be highly challenging, and can make model performance evaluation more difficult). However, without handling missing data on your predictors, you generally cannot even fit the model. So we’ll mostly focus there. 3.5.1 Omission We can omit missing data with step_naomit(). This will remove any row that has any missing data. Let’s see how this impacts our data, working with our same recipe we finished up with in the Creating a recipe section. I’ve placed the recipe here again so we don’t have to go back to remind ourselves what we did previously. rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), &quot;E&quot;, lang_cd)), tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% step_zv(all_predictors()) na_omit_data &lt;- rec %&gt;% step_naomit(all_predictors()) %&gt;% step_dummy(all_nominal()) %&gt;% prep() %&gt;% bake(new_data = NULL) nrow(na_omit_data) ## [1] 573 As can be seen above, when we omit any row with missing data we end up with only 573 rows out of the original 2841 rows in the training data (or approximately 20% of the original data). This level of data omission is highly likely to introduce systematic biases into your model prediction. Generally, step_naomit() should only be used when developing preliminary models, where you’re just trying to get code to run. When you get to the point where you’re actually trying to improve performance, you should consider alternative approaches. 3.5.2 Encoding and simple imputation Encoding missing data is similar to imputation. In imputation, we replace the missing value with something we think could have reasonably been the real value, if it were observed. When we encode missing data we are creating values that will be included in the modeling process. For example, with categorical variables, we could replace the missingess with a “missing” level, which would then get its own dummy code (if we were using dummy coding to encode the categorical variables). I mentioned in the Creating a recipe section that we were getting warnings but I was omitting them in the text. The reason for these warnings is that some of these columns have missing data. If we want to avoid this warning, we have to add an additional step to our recipe to encode the missing data in the categorical variables. This step is called step_unknown(), and it replaces missing values with \"unknown\". Let’s do this for all categorical variables and omit any rows that are missing on numeric columns. na_encode_data &lt;- rec %&gt;% step_unknown(all_nominal()) %&gt;% step_naomit(all_predictors()) %&gt;% step_dummy(all_nominal()) %&gt;% prep() %&gt;% bake(new_data = NULL) nrow(na_encode_data) ## [1] 2788 Notice in the above that when I call step_naomit() I state that it should be applied to all_predictors() because I’ve already encoded the nominal predictors in the previous step. This approach allows us to capture 98% of the original data. And as a bonus, we’ve removed the warnings. (Note: we might also want to apply step_novel() for any future data that had levels outside of our training data - see Handling new levels). Just a slight step up in complexity from omission of rows with missing data is to impute them with sample descriptive statistics, such as the mean or the median. Generally, I’ve found that median imputation works better than mean imputation, but that could be related to the types of data I work with most frequently. Let’s switch datasets so we can see what’s happening more directly. Let’s look at the airquality dataset, which ships with R. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 As we can see, Solar.R is missing for observations 5 and 6. Let’s compute the sample mean and median for this column. mean(airquality$Solar.R, na.rm = TRUE) ## [1] 185.9315 median(airquality$Solar.R, na.rm = TRUE) ## [1] 205 If we use mean or median imputation, we just replace the missing values with these sample statistics. Let’s do this in a new recipe, assuming we’ll be fitting a model where Ozone is the outcome, predicted by all other variables in the dataset. recipe(Ozone ~ ., data = airquality) %&gt;% step_meanimpute(all_predictors()) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 153 x 6 ## Solar.R Wind Temp Month Day Ozone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 190 7.4 67 5 1 41 ## 2 118 8 72 5 2 36 ## 3 149 12.6 74 5 3 12 ## 4 313 11.5 62 5 4 18 ## 5 186 14.3 56 5 5 NA ## 6 186 14.9 66 5 6 28 ## 7 299 8.6 65 5 7 23 ## 8 99 13.8 59 5 8 19 ## 9 19 20.1 61 5 9 8 ## 10 194 8.6 69 5 10 NA ## # … with 143 more rows As we can see, the value \\(186\\) has been imputed for rows 5 and 6, which is the integer version of the sample mean (an integer was imputed because the column was already an integer, and not a double). Let’s try the same thing with median imputation recipe(Ozone ~ ., data = airquality) %&gt;% step_medianimpute(all_predictors()) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 153 x 6 ## Solar.R Wind Temp Month Day Ozone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 190 7.4 67 5 1 41 ## 2 118 8 72 5 2 36 ## 3 149 12.6 74 5 3 12 ## 4 313 11.5 62 5 4 18 ## 5 205 14.3 56 5 5 NA ## 6 205 14.9 66 5 6 28 ## 7 299 8.6 65 5 7 23 ## 8 99 13.8 59 5 8 19 ## 9 19 20.1 61 5 9 8 ## 10 194 8.6 69 5 10 NA ## # … with 143 more rows And as we would expect, the missingness has now been replaced with values of \\(205\\). Sometimes you have time series data, or there is a date variable in the dataset that accounts for a meaningful proportion of the variance. In these cases, you might consider step_rollimpute(), which provides a conditional median imputation based on time, and you can set the size of the window from which to calculate the median. In still other cases it may make sense to just impute with the lowest observed value (i.e., assume a very small amount of the predictor), which can be accomplished with step_lowerimpute(). These simple imputation techniques are fine to use when developing models. However, it’s an area that may be worth returning to as you start to refine your model to see if you can improve performance. 3.5.3 Modeling the missingness Another alternative for imputation is to fit a statistical model with the column you want to impute modeled as the outcome, with all other columns (minus the actual outcome) predicting it. We then use that model for the imputation. Let’s first consider a linear regression model. We’ll fit the same model we specified in our recipe, using the airquality data. m &lt;- lm(Solar.R ~ ., data = airquality[ ,-1]) summary(m) ## ## Call: ## lm(formula = Solar.R ~ ., data = airquality[, -1]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -182.945 -67.348 5.295 73.781 170.068 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.5960 84.4424 -0.173 0.863016 ## Wind 2.1661 2.2633 0.957 0.340171 ## Temp 3.7023 0.9276 3.991 0.000105 *** ## Month -13.2640 5.4525 -2.433 0.016242 * ## Day -1.0631 0.8125 -1.308 0.192875 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 85.14 on 141 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.131, Adjusted R-squared: 0.1063 ## F-statistic: 5.313 on 4 and 141 DF, p-value: 0.0005145 Notice that I’ve dropped the first column here, which is Ozone, our actual outcome. The model above has been fit using the equivalent of step_naomit(), otherwise known as listwise deletion, where any row with any missing data is removed. We can now use the coefficients from this model to impute the missing values in Solar.R. For example, row 6 in the dataset had a missing value on Solar.R and the following values for all other variables: row6 &lt;- data.frame(Wind = 14.9, Temp = 66, Month = 5, Day = 6) Using our model, we would predict the following score for this missing value predict(m, newdata = row6) ## 1 ## 189.3325 Let’s try this using {recipes}. recipe(Ozone ~ ., data = airquality) %&gt;% step_impute_linear(all_predictors()) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 153 x 6 ## Solar.R Wind Temp Month Day Ozone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 190 7.4 67 5 1 41 ## 2 118 8 72 5 2 36 ## 3 149 12.6 74 5 3 12 ## 4 313 11.5 62 5 4 18 ## 5 152 14.3 56 5 5 NA ## 6 189 14.9 66 5 6 28 ## 7 299 8.6 65 5 7 23 ## 8 99 13.8 59 5 8 19 ## 9 19 20.1 61 5 9 8 ## 10 194 8.6 69 5 10 NA ## # … with 143 more rows And we see two important things here. First, row 6 for Solar.R is indeed as we expected it to be (albeit, in integer form). Second, the imputed values for rows 5 and 6 are now different, which is the first time we’ve seen this via imputation. The same basic approach can be used for essentially any statistical model. The {recipes} package has currently implemented linear imputation (as above), \\(k\\)-nearest neighbor imputation, and bagged imputation (via bagged trees). Let’s see how rows 5 and 6 differ with these approaches. # k-nearest neighbor imputation recipe(Ozone ~ ., data = airquality) %&gt;% step_knnimpute(all_predictors()) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 153 x 6 ## Solar.R Wind Temp Month Day Ozone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 190 7.4 67 5 1 41 ## 2 118 8 72 5 2 36 ## 3 149 12.6 74 5 3 12 ## 4 313 11.5 62 5 4 18 ## 5 159 14.3 56 5 5 NA ## 6 220 14.9 66 5 6 28 ## 7 299 8.6 65 5 7 23 ## 8 99 13.8 59 5 8 19 ## 9 19 20.1 61 5 9 8 ## 10 194 8.6 69 5 10 NA ## # … with 143 more rows # bagged imputation recipe(Ozone ~ ., data = airquality) %&gt;% step_bagimpute(all_predictors()) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 153 x 6 ## Solar.R Wind Temp Month Day Ozone ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 190 7.4 67 5 1 41 ## 2 118 8 72 5 2 36 ## 3 149 12.6 74 5 3 12 ## 4 313 11.5 62 5 4 18 ## 5 99 14.3 56 5 5 NA ## 6 252 14.9 66 5 6 28 ## 7 299 8.6 65 5 7 23 ## 8 99 13.8 59 5 8 19 ## 9 19 20.1 61 5 9 8 ## 10 194 8.6 69 5 10 NA ## # … with 143 more rows These models are quite a bit more flexible than linear regression, and can potentially overfit. You can, however, control some of the parameters to the models through additional arguments (e.g., \\(k\\) for \\(knn\\), which defaults to 5). The benefit of these models is that they may provide better estimates of what the imputed value would have been, were it not missing, which may then improve model performance. The downside is that they are quite a bit more computationally intensive. Generally, you use recipes within processes like \\(k\\)-fold cross-validation, with the recipe being applied to each fold. In this case, a computationally expensive approach may significantly bog down hyperparameter tuning. 3.5.4 A few words of caution Missing data is a highly complex topic. This section was meant to provide a basic overview of some of the options you can choose from when building a predictive model. None of these approaches, however, will “fix” data that are missing not at random (MNAR). Unfortunately, it is usually impossible to know if your data are MNAR, and we therefore assume that data are missing at random (MAR), or missing at random conditional on the observed data. For example, if boys were more likely to have missing data on the outcome than girls, we could account for this by including a gender variable in the model, and the resulting data would be MAR. If you have significant missing data, this section is surely incomplete. We recommended Little and Rubin (2002) previously, and there are a number of other good resources, including a chapter in Kuhn and Johnson (2019). "],["transformations.html", "3.6 Transformations", " 3.6 Transformations In standard inferential statistics, we are often concerned with the distribution of the outcome. Linear regression, for example, assumes the outcome is at least reasonably normally distributed. If this is not the case, the standard errors (in particular) can be misrepresented. We therefore generally inspect the outcome before modeling it, and, if it is not approximately normally distributed, we either transform it to make it more closely approximate a normal distribution, or we use an analysis technique that does not assume normality in the outcome. In predictive modeling, transformations of the predictors or the outcome(s) (or both) can sometimes help improve model performance. For example, let’s quickly simulate some data. set.seed(3) # parameters alpha &lt;- 10 b1 &lt;- 5 # simulate predictor variable x &lt;- 1:100 log_x &lt;- log(x) # residual SD e &lt;- rnorm(length(x), sd = 0.8) y &lt;- alpha + b1*log_x + e sim &lt;- data.frame(x, y) As you can see from the above, we have simulated the data according to \\(\\log x\\), but our data frame only has \\(x\\). This is a common situation where we don’t know the true functional form. But of course, if we fit a linear regression model to these data, we’ll end up with high bias, particularly in the lower tail (and issues with heteroscedasticity). ggplot(sim, aes(x, y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) But all we have to do, in this case, is use a log transformation to the \\(x\\) variable and our linear model fits great. sim %&gt;% mutate(log_x = log(x)) %&gt;% head() ## x y log_x ## 1 1 9.230453 0.0000000 ## 2 2 13.231715 0.6931472 ## 3 3 15.700092 1.0986123 ## 4 4 16.009766 1.3862944 ## 5 5 18.203816 1.6094379 ## 6 6 18.982897 1.7917595 sim %&gt;% mutate(log_x = log(x)) %&gt;% ggplot(aes(log_x, y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) Note that the model is linear in the transformed units, but curvilinear on the raw scale. sim %&gt;% mutate(log_x = log(x)) %&gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ log(x)) So in this case, a log transformation to the x variable works perfectly (as we would expect, given that we simulated the data to be this way). But how do we know how to transform variables? 3.6.1 Box-Cox and similar transformations A more general formula for transforming variables is given by the Box-Cox transformation, defined by \\[ \\begin{equation} x^* = \\begin{cases} \\frac{x^\\lambda-1}{\\lambda}, &amp; \\text{if}\\ \\lambda \\neq 0 \\\\ \\log\\left(x\\right), &amp; \\text{if}\\ \\lambda = 0 \\end{cases} \\end{equation} \\] where \\(x\\) represents the variable in its raw units, and \\(x^*\\) represents the transformed variable. The Box-Cox transformation is a power transformation, where the intent is to estimate \\(\\lambda\\). Note that if \\(\\lambda\\) is estimated as zero, the power transformation is the same as a log transformation; otherwise the top portion of the equation is used. Fortunately, specific values of \\(\\lambda\\) map to common transformations. \\(\\lambda = 1\\): No transformation \\(\\lambda = 0.5\\): square root transformation \\(\\lambda = 0\\): log transformation \\(\\lambda = -1\\): inverse Given the above, we would expect that \\(\\lambda\\) would be estimated close to zero with our simulated data. Let’s try using {recipes}. To access the actual \\(\\lambda\\) value, we’ll need to take a brief foray into tidying recipes. 3.6.1.1 Tidying recipes Let’s first specify the recipe with a Box-Cox transformation to our \\(x\\) variable. rec &lt;- recipe(y ~ x, data = sim) %&gt;% step_BoxCox(all_predictors()) Now we can tidy the recipe. tidy(rec) ## # A tibble: 1 x 6 ## number operation type trained skip id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 step BoxCox FALSE FALSE BoxCox_dj7Ww In this case, our recipe is incredibly simple. We have one step, which is a Box-Cox transformation. Let’s make the recipe a bit more complicated just for completeness. rec &lt;- recipe(y ~ x, data = sim) %&gt;% step_impute_linear(all_predictors()) %&gt;% step_nzv(all_predictors()) %&gt;% step_BoxCox(all_numeric(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) Most of these steps won’t do anything in this case, but let’s look at the tidied recipe now. tidy(rec) ## # A tibble: 4 x 6 ## number operation type trained skip id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 step impute_linear FALSE FALSE impute_linear_J8… ## 2 2 step nzv FALSE FALSE nzv_8d2bJ ## 3 3 step BoxCox FALSE FALSE BoxCox_NxJt6 ## 4 4 step dummy FALSE FALSE dummy_z6myS Now we have four steps. We can look at any one step by declaring the step number. Let’s look at the linear imputation tidy(rec, n = 1) ## # A tibble: 1 x 3 ## terms model id ## &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 all_predictors() NA impute_linear_J8I0L Notice there’s nothing there, because at this point the recipe is still just a blueprint. We have to prep() the recipe if we want it to actually do any work. Let’s prep the recipe and try again. lm_imputation &lt;- rec %&gt;% prep() %&gt;% tidy(n = 1) lm_imputation ## # A tibble: 1 x 3 ## terms model id ## &lt;chr&gt; &lt;named list&gt; &lt;chr&gt; ## 1 x &lt;lm&gt; impute_linear_J8I0L And now we can see a linear model has been fit. We can even access the model itself. lm_imputation$model ## $x ## ## Call: ## NULL ## ## Coefficients: ## (Intercept) ## 50.5 What we get is actually a list of models, one for each predictor. But in this case there’s only one predictor, so the list is only of length 1. 3.6.1.2 Estimating \\(\\lambda\\) We can do the same thing to find \\(\\lambda\\) by tidying the Box-Cox step rec %&gt;% prep() %&gt;% tidy(n = 3) ## # A tibble: 1 x 3 ## terms value id ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 x 0.717 BoxCox_NxJt6 And without any further work we can see that we estimated \\(\\lambda = 0.72\\), which is pretty much directly between a square-root transformation and no transformation. Why did it not estimate a \\(log\\) transformation as most appropriate? Because the log transformation is only ideal when viewing \\(x\\) relative to \\(y\\). Put differently, the Box-Cox transformation is an unsupervised approach that attempts to make each variable approximate a univariate normal distribution. As we’ll see in the next section, there are other methods that can be used to help with issues of non-linearity. For completeness, let’s see if the transformation helped us. We’ll use \\(\\lambda = 0.72\\) to manually transform \\(x\\), then plot the result. # transform x sim &lt;- sim %&gt;% mutate(x_bc = ((x^0.72) - 1) / 0.72) # fit the model using the transformed data m &lt;- lm(y ~ x_bc, sim) # add the model predictions to the data frame sim &lt;- sim %&gt;% mutate(pred = predict(m)) # plot the model fit using raw data on the x-axis ggplot(sim, aes(x, y)) + geom_point() + geom_line(aes(y = pred)) As we can see, it’s better than the raw data, but still insufficient. 3.6.2 An applied example Let’s look at an applied example. We’ll use the violence data (see the full data dictionary here) and see if we can predict the neighborhoods where the number of murders are greater than zero, using the percentage of people living in poverty and the percentage of people living in dense housing units (more than one person per room) as predictors. Let’s start with a basic plot. violence &lt;- read_csv(here::here(&quot;data&quot;, &quot;violence.csv&quot;)) violence &lt;- violence %&gt;% mutate(murder = ifelse(murders &gt; 0, &quot;Yes&quot;, &quot;No&quot;)) ggplot(violence, aes(pctPoverty, houseVacant)) + geom_point(aes(color = murder), alpha = 0.5, stroke = 0) As you can see, it’s pretty difficult to see much separation here. Let’s look at the univariate views of each predictor. ggplot(violence, aes(pctPoverty)) + geom_histogram() ggplot(violence, aes(pctPopDenseHous)) + geom_histogram() Both predictors are quite skewed. What do they look like after transformation? murder_rec &lt;- recipe(murder ~ ., violence) %&gt;% step_BoxCox(all_numeric(), -all_outcomes()) transformed_murder &lt;- murder_rec %&gt;% prep() %&gt;% bake(new_data = NULL) ggplot(transformed_murder, aes(pctPoverty)) + geom_histogram() ggplot(transformed_murder, aes(pctPopDenseHous)) + geom_histogram() Each of these look considerably better. What about the bivariate view? ggplot(transformed_murder, aes(pctPoverty, pctPopDenseHous)) + geom_point(aes(color = murder), alpha = 0.5, stroke = 0) We can much more clearly see the separation here. We could almost draw a diagonal line in the data separating the classes, as below There’s of course still some misclassification going on here, and that line was drawn by just eye-balling it, but even by hand we can do this much easier after the transformation. What were the lambda values estimated at for these variables? Let’s check. murder_rec %&gt;% prep() %&gt;% tidy(1) %&gt;% filter(terms %in% c(&quot;pctPoverty&quot;, &quot;pctPopDenseHous&quot;)) ## # A tibble: 2 x 3 ## terms value id ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 pctPoverty 0.177 BoxCox_bG8nP ## 2 pctPopDenseHous -0.0856 BoxCox_bG8nP Both are fairly close to zero, implying they are similar to log transformations. "],["nonlinearity.html", "3.7 Nonlinearity", " 3.7 Nonlinearity Very regularly, predictor variables will have a nonlinear relation with the outcome. In the previous section, we simulated data to follow a log-linear trend, which is linear on the log scale and curvilinear on the raw scale. As a reminder, the data looked like this ggplot(sim, aes(x, y)) + geom_point() If we just looked at a plot of these data, we’d probably think to ourselves “that looks a whole lot like a log function!”. But let’s pretend for a moment that we’re not quite as brilliant as we actually are and we, say, didn’t plot the data first (always plot the data first!). Or maybe we’re just not all that familiar with the log function, and we didn’t realize that a simple transformation could cure all our woes. How would we go about modeling this? Clearly a linear relation would be insufficient. There are many different options for modeling curvilinnear relations, but we’ll focus on two: basis expansion via polynomial transformations and natural splines. Basis expansion is defined, mathematically, as \\[ f(X) = \\sum_{m = 1}^M\\beta_mh_m(X) \\] where \\(h_m\\) is the \\(m\\)th transformation of \\(X\\). Once the transformations are defined, multiple coefficients (\\(\\beta\\)) are estimated to represent each transformation, which sum to form the non-linear trend. However, the model is itself still linear. We illustrate below. 3.7.1 Polynomial transformations Polynomial transformations include the variable in its raw units, plus additional (basis expansion) units raised to a given power. For example, a cubic trend could be fit by transforming \\(X\\) into \\(X\\), \\(X^2\\), and \\(X^3\\), and estimating coefficients for each. Let’s do this manually with the sim data from above. # remove variables from previous example # keep only `x` and `y` sim &lt;- sim %&gt;% select(x, y) sim &lt;- sim %&gt;% mutate(x2 = x^2, x3 = x^3) %&gt;% select(starts_with(&quot;x&quot;), y) head(sim) ## x x2 x3 y ## 1 1 1 1 9.230453 ## 2 2 4 8 13.231715 ## 3 3 9 27 15.700092 ## 4 4 16 64 16.009766 ## 5 5 25 125 18.203816 ## 6 6 36 216 18.982897 Now let’s fit a model to the data using the polynomial expansion, and plot the result. poly_m &lt;- lm(y ~ ., data = sim) sim &lt;- sim %&gt;% mutate(poly_pred = predict(poly_m)) head(sim) ## x x2 x3 y poly_pred ## 1 1 1 1 9.230453 14.99051 ## 2 2 4 8 13.231715 15.63371 ## 3 3 9 27 15.700092 16.25849 ## 4 4 16 64 16.009766 16.86514 ## 5 5 25 125 18.203816 17.45393 ## 6 6 36 216 18.982897 18.02516 ggplot(sim, aes(x, y)) + geom_point() + geom_line(aes(y = poly_pred)) Not bad! Now let’s replicate it using {recipes}. sim &lt;- sim %&gt;% select(x, y) poly_rec &lt;- recipe(y ~ ., sim) %&gt;% step_poly(all_predictors(), degree = 3) poly_d &lt;- poly_rec %&gt;% prep() %&gt;% bake(new_data = NULL) poly_d ## # A tibble: 100 x 4 ## y x_poly_1 x_poly_2 x_poly_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.23 -0.171 0.217 -0.249 ## 2 13.2 -0.168 0.204 -0.219 ## 3 15.7 -0.165 0.191 -0.190 ## 4 16.0 -0.161 0.178 -0.163 ## 5 18.2 -0.158 0.166 -0.137 ## 6 19.0 -0.154 0.154 -0.113 ## 7 19.8 -0.151 0.142 -0.0904 ## 8 21.3 -0.147 0.131 -0.0690 ## 9 20.0 -0.144 0.119 -0.0489 ## 10 22.5 -0.140 0.108 -0.0302 ## # … with 90 more rows Notice that these values look quite a bit different than the ones we had before because these are orthogonal polynomials. Orthogonal polynomials have some advantages over raw polynomials, including that, in a standard linear regression modeling framework, each higher-order term can be directly evaluated for its contribution to the model. This is mostly because non-orthogonal polynomials are highly correlated. However, the fitted values remain the same, as we can see below. poly_m2 &lt;- lm(y ~ ., data = poly_d) sim &lt;- sim %&gt;% mutate(poly_pred2 = predict(poly_m2)) ggplot(sim, aes(x, y)) + geom_point() + geom_line(aes(y = poly_pred2)) But let’s be even more precise and verify that they are, indeed, identical in their fitted values. all( round(predict(poly_m), 7) == round(predict(poly_m2), 7) ) ## [1] TRUE This shows that, to seven decimal places, the results are identical (note, the only reason I had to round at all was because of floating point hell). 3.7.2 Splines When thinking about splines, we find it most helpful to first think about discontinuous splines. Splines divide up the “x” region of the predictor into bins, and fits a model within each bin. Let’s first look at a very simple model, where we divide our sim dataset into lower and upper portions. # get only x and y back sim &lt;- sim %&gt;% select(x, y) # Fit a model to the lower and upper parts lower_part &lt;- lm(y ~ x, filter(sim, x &lt;= 50)) upper_part &lt;- lm(y ~ x, filter(sim, x &gt; 50)) # make predictions from each part lower_pred &lt;- predict(lower_part, newdata = data.frame(x = 0:50)) upper_pred &lt;- predict(upper_part, newdata = data.frame(x = 51:100)) # graph the results ggplot(sim, aes(x, y)) + annotate(&quot;rect&quot;, xmin = -Inf, ymin = -Inf, ymax = Inf, xmax = 50, fill = &quot;#7EC1E7&quot;, alpha = 0.3) + geom_point() + geom_line(data = data.frame(x = 0:50, y = lower_pred)) + geom_line(data = data.frame(x = 51:100, y = upper_pred)) As you can see, this gets at the same non-linearity, but with two models. We can better approximate the curve by increasings the number of “bins” in the x-axis. Let’s use 10 bins instead. ten_bins &lt;- sim %&gt;% mutate(tens = as.integer(x/10)) %&gt;% group_by(tens) %&gt;% nest() %&gt;% mutate(m = map(data, ~lm(y ~ x, .x)), pred_frame = map2(data, m, ~data.frame(x = .x$x, y = predict(.y)))) ggplot(sim, aes(x, y)) + geom_vline(xintercept = seq(10, 90, 10), color = &quot;gray30&quot;) + geom_point() + map(ten_bins$pred_frame, ~geom_line(data = .x)) This approximates the underlying curve quite well. We can try again by fitting polynomial models within each bin to see if that helps. Note we’ve place the point where x == 100 into the bin for 9, because there’s only one point in that bin and the model can’t be fit to a single point. ten_bins &lt;- sim %&gt;% mutate(tens = as.integer(x / 10), tens = ifelse(tens == 10, 9, tens)) %&gt;% group_by(tens) %&gt;% nest() %&gt;% mutate(m = map(data, ~lm(y ~ poly(x, 3), .x)), pred_frame = map2(data, m, ~data.frame(x = .x$x, y = predict(.y)))) ggplot(sim, aes(x, y)) + geom_vline(xintercept = seq(10, 90, 10), color = &quot;#7EC1E7&quot;) + geom_point() + map(ten_bins$pred_frame, ~geom_line(data = .x)) But now we have 10 different models! Wouldn’t it be better if we had a single curve? YES! Of course. And that’s exactly what a spline is. It forces each of these curves to connect in a smooth way. The points defining the bins are referred to as “knots”. Let’s fit a spline to these data, using 8 “interior” knots (for 10 knots total, including one on each boundary). library(splines) ns_mod &lt;- lm(y ~ ns(x, 8), sim) ggplot(sim, aes(x, y)) + geom_point() + geom_line(data = data.frame(x = 1:100, y = predict(ns_mod))) And now we have our single curve! The purpose of our discussion here is to build conceptual understandings of splines, and we will therefore not go into the mathematics behind the constraints leading to a single curve. However, for those interested, we recommend Hastie, Tibshirani, and Friedman. One of the drawbacks of polynomial regression is that they can be wild in the tails. Natural splines avoid this problem by constraining the tails of the curve to be linear. Sometimes, however, this property may not be desirable, and that’s when we instead use a B-spline. bs_mod &lt;- lm(y ~ bs(x, 8), sim) ggplot(sim, aes(x, y)) + geom_point() + geom_line(data = data.frame(x = 1:100, y = predict(bs_mod))) Let’s look at the true data-generating curve with each of the splines, omitting the points for clarity. ns_d &lt;- data.frame(x = 1:100, y = predict(ns_mod), method = &quot;Natural Spline&quot;) bs_d &lt;- data.frame(x = 1:100, y = predict(bs_mod), method = &quot;B-Spline&quot;) ggplot(rbind(ns_d, bs_d), aes(x, y)) + geom_smooth(data = sim, method = &quot;lm&quot;, formula = y ~ log(x), se = FALSE, color = &quot;#5E8192&quot;) + geom_line(aes(color = method)) + facet_wrap(~method) Both are quite close, but you can see the natural spline does not dip quite as close to the true value for zero as the B-spline. It’s also a tiny bit more wiggly in the upper tail (because it’s not constrained to be linear). The number of knots is also likely a bit too high here and was chosen fairly randomly. 3.7.2.1 Basis expansion with {recipes} Basis expansion via recipes proceeds in much the same way as we’ve seen before. We just specify a step identifying the the type of basis expansion we want to conduct, along with any additional arguments like the degrees of freedom (number of knots). Let’s try building a dataset using basis expansion with a B-Spline bs_d &lt;- recipe(y ~ x, data = sim) %&gt;% step_bs(x, deg_free = 8) %&gt;% prep() %&gt;% bake(new_data = NULL) bs_d ## # A tibble: 100 x 9 ## y x_bs_1 x_bs_2 x_bs_3 x_bs_4 x_bs_5 x_bs_6 x_bs_7 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.23 0 0 0. 0 0 0 0 ## 2 13.2 0.166 0.00531 3.71e-5 0 0 0 0 ## 3 15.7 0.301 0.0204 2.97e-4 0 0 0 0 ## 4 16.0 0.407 0.0441 1.00e-3 0 0 0 0 ## 5 18.2 0.488 0.0751 2.37e-3 0 0 0 0 ## 6 19.0 0.545 0.112 4.64e-3 0 0 0 0 ## 7 19.8 0.580 0.154 8.01e-3 0 0 0 0 ## 8 21.3 0.596 0.200 1.27e-2 0 0 0 0 ## 9 20.0 0.596 0.248 1.90e-2 0 0 0 0 ## 10 22.5 0.582 0.298 2.70e-2 0 0 0 0 ## # … with 90 more rows, and 1 more variable: x_bs_8 &lt;dbl&gt; And as you can see, we get similar output to polynomial regression, but we’re just using a spline for our basis expansion now instead of polynomials. Note that the degrees of freedom can also be set to tune() and trained withing tune::tune_grid(). In other words, the amount of basis expansion (degree of “wiggliness”) can be treated as a hyperparamter to be tuned. Once the basis expansion is conducted, we can model it just like any other linear model. bs_mod_2 &lt;- lm(y ~ x_bs_1 + x_bs_2 + x_bs_3 + x_bs_4 + x_bs_5 + x_bs_6 + x_bs_7 + x_bs_8, data = bs_d) And this will give us the same results we got before. all( round(predict(bs_mod), 7) == round(predict(bs_mod_2), 7) ) ## [1] TRUE To use polynomial transformations or natural splines instead, we would just use step_poly() or step_ns(), respectively. One rather important note on tuning splines, you will probably want to avoid using code like recipe(murders ~ ., data = violence) %&gt;% step_ns(all_predictors(), deg_free = tune()) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 147 ## ## Operations: ## ## Natural Splines on all_predictors() because this will constrain the smooth to be the same for all predictors. Instead, use exploratory analyses and plots to figure out which variables should be fit with splines, and then tune them separately. For example recipe(murders ~ ., data = violence) %&gt;% step_ns(pctPoverty, deg_free = tune()) %&gt;% step_ns(houseVacant, deg_free = tune()) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 147 ## ## Operations: ## ## Natural Splines on pctPoverty ## Natural Splines on houseVacant Of course, the more splines you are tuning, the more computationally expensive it will become, so you’ll need to balance this with your expected return in model improvement (i.e., you likely can’t tune every predictor in your dataset). "],["interactions.html", "3.8 Interactions", " 3.8 Interactions An interaction occurs when the slope, or relation, between one variable and the outcome depends upon a second variable. This is also often referred to as moderation or a moderating variable. For example, consider data that look like this: benchmarks &lt;- read_csv(here::here(&quot;data&quot;, &quot;benchmarks.csv&quot;)) ggplot(benchmarks, aes(rdg_fall, rdg_spr)) + geom_point(color = &quot;gray70&quot;) + geom_smooth(aes(color = ell), method = &quot;lm&quot;) where ell stands for “English language learner”. The plot above shows the relation between students scores on a reading assessment administered in the fall and the spring by ell status. Notice that the slope for Non-ELL is markedly steeper than the other two groups. ell status thus moderates the relation between the fall and spring assessment scores. If we’re fitting a model within a linear regression framework, we’ll want to make sure that we model the slopes separately for each of these groups. Otherwise, we would be leaving out important structure in the data, and our model performance would suffer. Interactions are powerful and important to consider, particularly if there is strong theory (or empirical evidence) suggesting that an interaction exists. However, they are not needed in many frameworks and can be derived from the data directly (e.g., tree-based methods, where the degree of possible interaction is determined by the depth of the tree). In this section, we therefore focus on interactions within a linear regression framework. 3.8.1 Creating interactions “by hand” Even with base R, modeling interactions is pretty straightforward. In the above, we would just specify something like lm(rdg_spr ~ rdg_fall*ell, data = benchmarks) or equivalently, lm(rdg_spr ~ rdg_fall + ell + rdg_fall:ell, data = benchmarks) However, this is doing some work under the hood for you which might go unnoticed. First, it’s dummy-coding ell, then it’s creating, in this case, two new variables that are equal to \\(ELL_{Monitor} \\times Rdg_{fall}\\) and \\(ELL_{Non-ELL} \\times Rdg_{fall}\\) (with \\(ELL_{Active}\\) set as the reference group by default). Let’s try doing this manually. First we need to dummy code ell. To make things a bit more clear, I’ll only select the variables here that we’re using in our modeling. dummies &lt;- benchmarks %&gt;% select(rdg_fall, ell, rdg_spr) %&gt;% mutate(ell_monitor = ifelse(ell == &quot;Monitor&quot;, 1, 0), ell_non = ifelse(ell == &quot;Non-ELL&quot;, 1, 0),) dummies ## # A tibble: 174 x 5 ## rdg_fall ell rdg_spr ell_monitor ell_non ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 181 Active 194 0 0 ## 2 166 Non-ELL 159 0 1 ## 3 216 Non-ELL 198 0 1 ## 4 203 Non-ELL 204 0 1 ## 5 198 Active 173 0 0 ## 6 188 Active 173 0 0 ## 7 202 Monitor 200 1 0 ## 8 182 Active 206 0 0 ## 9 194 Non-ELL 191 0 1 ## 10 170 Active 185 0 0 ## # … with 164 more rows Next, we’ll multiply each of these dummy variables by rdg_fall. interactions &lt;- dummies %&gt;% mutate(fall_monitor = rdg_fall * ell_monitor, fall_non = rdg_fall * ell_non) interactions ## # A tibble: 174 x 7 ## rdg_fall ell rdg_spr ell_monitor ell_non fall_monitor ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 181 Acti… 194 0 0 0 ## 2 166 Non-… 159 0 1 0 ## 3 216 Non-… 198 0 1 0 ## 4 203 Non-… 204 0 1 0 ## 5 198 Acti… 173 0 0 0 ## 6 188 Acti… 173 0 0 0 ## 7 202 Moni… 200 1 0 202 ## 8 182 Acti… 206 0 0 0 ## 9 194 Non-… 191 0 1 0 ## 10 170 Acti… 185 0 0 0 ## # … with 164 more rows, and 1 more variable: fall_non &lt;dbl&gt; As would be expected, these values are zero if they are not for the corresponding group, and are otherwise equal to rdg_fall. If we enter all these variables in our model, then our model intercept will represent the intercept for the active group, with the corresponding slope estimated by rdg_fall. The ell_monitor and ell_non terms represent the intercepts for the monitor and non-ELL groups, respectively, and each of these slopes are estimated by the corresponding interaction. Let’s try. m_byhand &lt;- lm(rdg_spr ~ rdg_fall + ell_monitor + ell_non + fall_monitor + fall_non, data = interactions) summary(m_byhand) ## ## Call: ## lm(formula = rdg_spr ~ rdg_fall + ell_monitor + ell_non + fall_monitor + ## fall_non, data = interactions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.812 -7.307 -0.100 8.616 26.693 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 180.29666 32.95228 5.471 1.6e-07 *** ## rdg_fall 0.03939 0.18459 0.213 0.8313 ## ell_monitor -0.47286 53.18886 -0.009 0.9929 ## ell_non -64.12123 36.80395 -1.742 0.0833 . ## fall_monitor 0.06262 0.28669 0.218 0.8274 ## fall_non 0.40801 0.20346 2.005 0.0465 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.37 on 168 degrees of freedom ## Multiple R-squared: 0.2652, Adjusted R-squared: 0.2433 ## F-statistic: 12.13 on 5 and 168 DF, p-value: 4.874e-10 And we can verify that this does, indeed, get us the same results that we would have obtained with the shortcut syntax with base R. summary( lm(rdg_spr ~ rdg_fall*ell, data = benchmarks) ) ## ## Call: ## lm(formula = rdg_spr ~ rdg_fall * ell, data = benchmarks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.812 -7.307 -0.100 8.616 26.693 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 180.29666 32.95228 5.471 1.6e-07 *** ## rdg_fall 0.03939 0.18459 0.213 0.8313 ## ellMonitor -0.47286 53.18886 -0.009 0.9929 ## ellNon-ELL -64.12123 36.80395 -1.742 0.0833 . ## rdg_fall:ellMonitor 0.06262 0.28669 0.218 0.8274 ## rdg_fall:ellNon-ELL 0.40801 0.20346 2.005 0.0465 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.37 on 168 degrees of freedom ## Multiple R-squared: 0.2652, Adjusted R-squared: 0.2433 ## F-statistic: 12.13 on 5 and 168 DF, p-value: 4.874e-10 3.8.2 Creating interactions with {recipes} Specifying interactions in a recipe is similar to other steps, with one small exception, which is that the use of the tilde, ~, is required. Note that, once again, order does matter, and you should always do your dummy coding before specifying interactions with categorical variables. In the example before, this would correspond to: recipe(rdg_spr ~ rdg_fall + ell, benchmarks) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~rdg_fall:starts_with(&quot;ell&quot;)) %&gt;% prep() %&gt;% bake(new_data = NULL) ## # A tibble: 174 x 6 ## rdg_fall rdg_spr ell_Monitor ell_Non.ELL rdg_fall_x_ell_… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 181 194 0 0 0 ## 2 166 159 0 1 0 ## 3 216 198 0 1 0 ## 4 203 204 0 1 0 ## 5 198 173 0 0 0 ## 6 188 173 0 0 0 ## 7 202 200 1 0 202 ## 8 182 206 0 0 0 ## 9 194 191 0 1 0 ## 10 170 185 0 0 0 ## # … with 164 more rows, and 1 more variable: ## # rdg_fall_x_ell_Non.ELL &lt;dbl&gt; There are a few things to mention here. First, the data look identical (minus the column names) to what we created by hand, so that’s good news. However, we also used starts_with() here, rather than just specifying something like step_interact(~rdg_fall:ell). This is because after dummy coding, the column names change (as you can see in the above). By specifying starts_with(), we are ensuring that we get all the interactions we need. If we forget this step, we end up with the wrong output, along with a warning. recipe(rdg_spr ~ rdg_fall + ell, benchmarks) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~rdg_fall:ell) %&gt;% prep() %&gt;% bake(new_data = NULL) ## Warning: Interaction specification failed for: ~rdg_fall:ell. No ## interactions will be created. ## # A tibble: 174 x 4 ## rdg_fall rdg_spr ell_Monitor ell_Non.ELL ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 181 194 0 0 ## 2 166 159 0 1 ## 3 216 198 0 1 ## 4 203 204 0 1 ## 5 198 173 0 0 ## 6 188 173 0 0 ## 7 202 200 1 0 ## 8 182 206 0 0 ## 9 194 191 0 1 ## 10 170 185 0 0 ## # … with 164 more rows This warning occurs because R is trying to find a column called ell to multiply with rdg_fall, but no such variable exists after dummy coding. "],["pca.html", "3.9 PCA", " 3.9 PCA For some sets of models (such as linear regression), highly correlated variables may cause model instability or, perhaps more importantly, lead to wider confidence intervals around model predictions (i.e., how confident we are about our prediction). In these cases, it may help to collapse these variables, while still accounting for the majority of the variance. This is the basic idea behind Principal Components Analysis (PCA). You use PCA to collapse sets of variables into principal components, reducing the dimensionality of your data while maintaining \\(X\\) percent of the original variation in the data. Reducing the dimensionality generally has an associated cost of higher model bias. However, it will also nearly always reduce model variability. The proportion of the original variability to maintain can thus be considered a tuning parameter, balancing bias with variance. Probably my favorite discussion of PCA comes from a discussion on CrossValidated on how to make sense of PCA. The opening poster asked how you would explain PCA to a layman and why it’s needed. The entire thread is worth reading through, and there’s a particularly nice example from JD Long comparing the first principal component to the line of best fit in linear regression. For our purposes, we’re primarily interested in what proportion of the total variability in the independent variables we should maintain. That is, can we improve our model performance by collapsing groups of (correlated) columns into a smaller set of principal components? 3.9.1 PCA with {recipes} Let’s move back to our full dataset. Our final recipe looked like this. rec &lt;- recipe(score ~ ., train) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), &quot;E&quot;, lang_cd)), tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% step_zv(all_predictors()) %&gt;% step_dummy(all_nominal()) To conduct PCA we need to make sure that All variables are numeric All variables are on the same scale There is no missing data Our recipe above ensures all variables are numeric, but it doesn’t handle missing data, and there is no standardization of variables. Let’s update this recipe to make sure it’s ready for PCA. rec &lt;- recipe(score ~ ., train) %&gt;% step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_zv(all_predictors()) %&gt;% step_unknown(all_nominal()) %&gt;% step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;% step_normalize(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;% step_dummy(all_nominal(), -has_role(&quot;id vars&quot;)) The above is not a whole lot more complex than our original recipe. It just assigns an \"unknown\" level to the nominal variables and imputes the remaining numeric variables with the median of that variable. It then normalizes all variables and dummy codes the nominal variables. This recipe gives us a data frame with many columns. Specifically: rec %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% ncol() ## [1] 80 Let’s create a recipe that retains 80% of the total variability with a smaller set of principal components. rec_80 &lt;- rec %&gt;% step_pca(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;), threshold = .80) Notice that we just have to specify the threshold of variance we want to maintain. How many columns do we have now? rec_80 %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% ncol() ## [1] 10 We’ve dramatically reduced the dimensionality in the data, while still retaining 80% of the total variability. An alternative method is to specify the number of components we want. For example, let’s extract only the first five components. rec_5 &lt;- rec %&gt;% step_pca(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;), num_comp = 5) I generally prefer the former because, if we’re thinking about PCA through a predictive modeling framework, we’re probably less concerned with the number of components and more concerned with the variation they represent. So how do we know how much of the variabilty we should retain? This is a difficult question, but sometimes plots of the principal components can help. Let’s look at the five components we just extracted. If we prep the recipe, you’ll see there’s some extra information we can access. prepped_pca &lt;- prep(rec_5) names(prepped_pca) ## [1] &quot;var_info&quot; &quot;term_info&quot; &quot;steps&quot; ## [4] &quot;template&quot; &quot;retained&quot; &quot;tr_info&quot; ## [7] &quot;orig_lvls&quot; &quot;last_term_info&quot; If we access the steps we can get additional information. The PCA step is the seventh, and it has the following elements stored in the list. names(prepped_pca$steps[[7]]) ## [1] &quot;terms&quot; &quot;role&quot; &quot;trained&quot; &quot;num_comp&quot; &quot;threshold&quot; ## [6] &quot;options&quot; &quot;res&quot; &quot;prefix&quot; &quot;skip&quot; &quot;id&quot; Let’s look into res. names(prepped_pca$steps[[7]]$res) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; And now we get what we’ve been (perhaps unknowingly to you, dear reader) looking for, the sdev object, which lists the standard deviation of each principal component. We can look at how much variance each component accounts for, as follows: vars &lt;- prepped_pca$steps[[7]]$res$sdev^2 pvar &lt;- vars / sum(vars) pvar ## [1] 7.503092e-01 5.138784e-02 5.055445e-02 4.135567e-02 ## [5] 2.388100e-02 1.154347e-02 8.222078e-03 8.051600e-03 ## [9] 7.564274e-03 7.346244e-03 5.926695e-03 5.187219e-03 ## [13] 4.420626e-03 3.803443e-03 3.418581e-03 2.536214e-03 ## [17] 2.344631e-03 1.533003e-03 1.516079e-03 1.397730e-03 ## [21] 1.170231e-03 1.071354e-03 1.001754e-03 8.622161e-04 ## [25] 7.596821e-04 7.042076e-04 4.803264e-04 3.837786e-04 ## [29] 3.359652e-04 3.219790e-04 1.786945e-04 1.426193e-04 ## [33] 7.831676e-05 6.866070e-05 3.886009e-05 3.175614e-05 ## [37] 2.647517e-05 2.034393e-05 1.580665e-05 6.911480e-06 ## [41] 1.361666e-30 5.047925e-31 1.214792e-32 9.199957e-33 ## [45] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 ## [49] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 ## [53] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 ## [57] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 ## [61] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 ## [65] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 ## [69] 6.414887e-33 6.414887e-33 6.414887e-33 6.414887e-33 That’s difficult to read. Let’s plot it instead pcs &lt;- data.frame(pc = paste0(&quot;PC&quot;, 1:length(pvar)), percent_var = pvar) %&gt;% mutate(pc = reorder(factor(pc), percent_var)) ggplot(pcs, aes(pc, percent_var)) + geom_col() + coord_flip() + scale_y_continuous(expand = c(0, 0)) # move this to a book theme for plots A few things are of note here. First, it appears the majority of the variance is accounted for by our first few components, which is not altogether surprising given that we already new the first 10 components accounted for approximately 85% of the total variability. However, you might also wonder why there are so many freaking components shown?! I thought we only asked for five!? That is true. But the model actually estimates all the components, and then just pulls out whatever we ask, based on either the threshold or the num_comp arguments. For example, let’s bake the five component recipe on the full training data and print the resulting variables. rec_5 %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% names() ## [1] &quot;id&quot; &quot;attnd_dist_inst_id&quot; ## [3] &quot;attnd_schl_inst_id&quot; &quot;tst_dt&quot; ## [5] &quot;partic_dist_inst_id&quot; &quot;partic_schl_inst_id&quot; ## [7] &quot;ncessch&quot; &quot;score&quot; ## [9] &quot;PC1&quot; &quot;PC2&quot; ## [11] &quot;PC3&quot; &quot;PC4&quot; ## [13] &quot;PC5&quot; And as expected, we get only the five components we requested. PCA reduces the dimensionality in the data, which can help guard against overfitting. However, it also may introduce a small amount of bias. Plots like the above can help determine how many components we should retain because we can see that, for example, after about 20 components or so we’re getting almost nothing in return. If you are using PCA and your model still seems to be overfitting, try reducing the dimensionality more (extracting fewer components). On the other hand, if you do not appear to be in danger of overfitting, you might try extracting more components to help reduce bias. "],["wrapping-up.html", "3.10 Wrapping up", " 3.10 Wrapping up Feature engineering is a gigantic topic. The purpose of this chapter was to provide a basic introduction. We illustrated creating a recipe or blueprint for your feature engineering, which can be applied iteratively to slices of the data (as in a k-fold cross-validation approach) or to new data with the same structure. This process helps guard against data leakage, where information from the test set leaks into the training set. Let’s look at a quick example of this by using PCA to retain 95% of the total variation in the data. We’ll use \\(k\\)-fold cross-validation and see how many components we retain in each fold. First, we’ll create the recipe, which is essentially equivalent to what we saw in the previous chapter. rec &lt;- recipe(score ~ ., train) %&gt;% step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_zv(all_predictors()) %&gt;% step_unknown(all_nominal()) %&gt;% step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;% step_normalize(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;% step_dummy(all_nominal(), -has_role(&quot;id vars&quot;)) %&gt;% step_pca(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;), threshold = .95) Now let’s create our \\(k\\)-fold cross-validation object. set.seed(42) cv &lt;- vfold_cv(train) cv ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2.6K/285]&gt; Fold01 ## 2 &lt;split [2.6K/284]&gt; Fold02 ## 3 &lt;split [2.6K/284]&gt; Fold03 ## 4 &lt;split [2.6K/284]&gt; Fold04 ## 5 &lt;split [2.6K/284]&gt; Fold05 ## 6 &lt;split [2.6K/284]&gt; Fold06 ## 7 &lt;split [2.6K/284]&gt; Fold07 ## 8 &lt;split [2.6K/284]&gt; Fold08 ## 9 &lt;split [2.6K/284]&gt; Fold09 ## 10 &lt;split [2.6K/284]&gt; Fold10 Now we’ll loop through each split and pull out the data we will build our model on, for that fold. cv %&gt;% mutate(assessment_data = map(splits, assessment)) ## # 10-fold cross-validation ## # A tibble: 10 x 3 ## splits id assessment_data ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; ## 1 &lt;split [2.6K/285]&gt; Fold01 &lt;tibble [285 × 39]&gt; ## 2 &lt;split [2.6K/284]&gt; Fold02 &lt;tibble [284 × 39]&gt; ## 3 &lt;split [2.6K/284]&gt; Fold03 &lt;tibble [284 × 39]&gt; ## 4 &lt;split [2.6K/284]&gt; Fold04 &lt;tibble [284 × 39]&gt; ## 5 &lt;split [2.6K/284]&gt; Fold05 &lt;tibble [284 × 39]&gt; ## 6 &lt;split [2.6K/284]&gt; Fold06 &lt;tibble [284 × 39]&gt; ## 7 &lt;split [2.6K/284]&gt; Fold07 &lt;tibble [284 × 39]&gt; ## 8 &lt;split [2.6K/284]&gt; Fold08 &lt;tibble [284 × 39]&gt; ## 9 &lt;split [2.6K/284]&gt; Fold09 &lt;tibble [284 × 39]&gt; ## 10 &lt;split [2.6K/284]&gt; Fold10 &lt;tibble [284 × 39]&gt; And finally, we’ll apply the recipe to each dataset. cv %&gt;% mutate(assessment_data = map(splits, assessment), baked_data = map(assessment_data, ~prep(rec) %&gt;% bake(.x))) ## # 10-fold cross-validation ## # A tibble: 10 x 4 ## splits id assessment_data baked_data ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2.6K/285… Fold01 &lt;tibble [285 × 39… &lt;tibble [285 × 1… ## 2 &lt;split [2.6K/284… Fold02 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 3 &lt;split [2.6K/284… Fold03 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 4 &lt;split [2.6K/284… Fold04 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 5 &lt;split [2.6K/284… Fold05 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 6 &lt;split [2.6K/284… Fold06 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 7 &lt;split [2.6K/284… Fold07 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 8 &lt;split [2.6K/284… Fold08 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 9 &lt;split [2.6K/284… Fold09 &lt;tibble [284 × 39… &lt;tibble [284 × 1… ## 10 &lt;split [2.6K/284… Fold10 &lt;tibble [284 × 39… &lt;tibble [284 × 1… Let’s see how many principal components are in each data fold. cv %&gt;% mutate(assessment_data = map(splits, assessment), baked_data = map(assessment_data, ~prep(rec) %&gt;% bake(.x)), n_components = map_int(baked_data, ~sum(grepl(&quot;^PC&quot;, names(.x))))) ## # 10-fold cross-validation ## # A tibble: 10 x 5 ## splits id assessment_data baked_data n_components ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;int&gt; ## 1 &lt;split [2.… Fold… &lt;tibble [285 × … &lt;tibble [285… 9 ## 2 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 3 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 4 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 5 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 6 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 7 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 8 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 9 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 ## 10 &lt;split [2.… Fold… &lt;tibble [284 × … &lt;tibble [284… 9 In this case, the number of components is the same across all folds, but the representation of those components is likely slightly different. Why? Because the recipe has been applied to each fold, independent of the other folds. Feature engineering can also regularly feel overwhelming. There are so many possible options that you could do to potentially increase model performance. This chapter just scratched the surface. The sheer number of decisions is why many believe that feature engineering is the most “art” part of machine learning. Strong domain knowledge will absolutely help, as will practice. For those looking to go further with feature engineering, we suggest Kuhn and Johnson, who wrote an entire book on the topic. It provides practical advice in an accessible way without skimping on important details. "],["decision-trees.html", "4 Decision Trees", " 4 Decision Trees One of our favorite machine learning algorithms is decision trees. They are not the most complex models, but they are quite intuitive. A single decision tree generally doesn’t have great “out of the box” model performance, and even with considerable model tuning they are unlikely to perform as well as other approaches. They do, however, form the building blocks for more complicated models that do have high out-of-the-box model performance and can produce state-of-the-art level predictions. Decision trees are non-parametric, meaning they do not make any assumptions about the underlying data generating process. By contrast, models like linear regression assume that the underlying data were generated by a standard normal distribution (and, if this is not the case, the model will result is systematic biases - although we can also use transformations and other strategies to help; see Feature Engineering). Note that assuming an underlying data generating distribution is not a weakness of linear regression - often it’s a tenable assumption and can regularly lead to better model performance, if the assumption holds. But decision trees do not require that you make any such assumptions, which is particularly helpful when it’s difficult to assume a specific underlying data generating distribution. At their core, decision trees work by splitting the features into a series of yes/no decisions. These splits divide the feature space into a series of non-overlapping regions, where the cases are similar in each region. To understand how a given prediction is made, one simply “follows” the splits of the tree (a branch) to the terminal node (the final prediction). This splitting continues until a specified criterion is met. 4.0.1 A simple decision tree Initially, we think it’s easiest to think about decision trees through a classification lens. One of the most common example datasets for decision trees is the titanic dataset, which includes information on passengers aboard the titanic. The data look like this library(tidyverse) titanic &lt;- read_csv(here::here(&quot;data&quot;, &quot;titanic.csv&quot;)) Imagine we wanted to create a model that would predict if passengers aboard the titanic survived. A decision tree model might look something like this. where Sibsp indicates the number of siblings/spouses onboard with the passenger. Let’s talk about vocabulary here for a bit (see Definitions table below). In the above, there is a node at the top that provides us some descriptive statistics - namely that when there have been no splits on any features (we have 100% of the sample), approximately 38% of passengers survived. But the root node, the first feature we split on, is the sex of the passenger, which in this case is a binary indicator with two levels: male and female. Approximately 65% of all passengers were coded as male, while the remaining 35% were coded as female. Of those that were coded male, about 19% survived, while approximately 74% of those coded female survived. These are different branches of the tree. Each node is then split again on an internal node, but the feature that optimally splits these nodes is different. For passengers coded female, we use the number of siblings/spouses, with an optimal cut being three. Those with three or fewer would be predicted to survive, while those with two or less would be not be predicted to survive. Note that these are the final predictions, or the terminal nodes for this branch. Females who with three or fewer siblings/spouses represent 33% of the total sample, of which 77% survived (as predicted by the terminal node), while females with three or more siblings/spouses represent 2% of the total sample, of which 29% survived (these passengers would not be predicted to survive). For male passengers, note that the first internal node splits first on age, because this is the more important feature for these passengers. Those who were six and a half or older are immediately predicted to not survive. This group represents 62% of the total sample, with only a 17% survival rate. However, for passengers coded male who were younger than 6.5, there was a small amount of additional information in the siblings/spouses feature. Passengers with three or more siblings/spouses would not be predicted to survive (representing 1% of the total sample, and an 11% survival rate) while those with fewer than three sibling/spouses would be predicted to survive (and all such passengers did actually survive, representing 2% of the total sample). Note that in this example, the optimal split for siblings/spouses happened to be the same in both branches, but this is a bit of coincidence. It’s possible there’s something important about this number, but the split value does not have to be the same, and in fact the same feature can be used multiple times for multiple splits, each with a different value, while splitting on internal nodes. For classification problems, like the above, the predicted class in the terminal node is determined by the most frequent class (the mode). For regression problems, the prediction is determined by the mean of the outcome for all cases in the given terminal node. Term Description Root node The top feature in a decision tree. The column that has the first split Internal node A grouping within the tree between the root node and the terminal node, e.g., all passengers coded female. Terminal node or Terminal leaf The final node/leaf of a decision tree. The prediction grouping. Branch The prediction “flow” of the tree. One often “follows” a branch to a terminal node. Split A threshold value for numeric features, or a classification separation rule for categorical features, that optimally separates the sample according to the outcome "],["determining-optimal-splits.html", "4.1 Determining optimal splits", " 4.1 Determining optimal splits Regression trees work by optimally splitting each node until some criterion is met. But how do we determine what’s optimal? We use an objective function. For regression problems, this usually the sum of the squared errors, defined by \\[ \\Sigma_{R1}(y_i - c_1)^2 + \\Sigma_{R2}(y_i - c_2)^2 \\] where \\(c\\) is the prediction (mean of cases in the region), which starts as a constant and is updated, and \\(R\\) is the region. The algorithm searches through every possible split for every possible feature to identify the split that minimizes the sum of the squared errors. For classification problems, the Gini impurity index is most common. Note that this is not the same index that is regularly used in spatial demography and similar fields to estimate inequality. Confusingly, both terms are referred to as the Gini index (with the latter also being referred to as the Gini coefficient or Gini ratio), but they are entirely separate. For a two-class situation, the Gini impurity index, used in decision trees, is defined by \\[ D_i = 1 - P(c_1)^2 - P(c_2)^2 \\] where where \\(P(c_1)\\) and \\(P(c_2)\\) are the probabilities of being in Class 1 or 2, respectively, for node \\(i\\). This formula can be generalized to a multiclass situation by \\[ D_i = 1 - \\Sigma(p_i)^2 \\] In either case, when \\(D = 0\\), the node is “pure” and the classification is perfect. When \\(D = 0.5\\), the node is random (flip a coin). As an example, consider a terminal node with 75% of cases in one class. The Gini impurity index would be estimated as \\[ \\begin{aligned} D &amp;= 1 - (0.75^2 + 0.25^2) \\\\\\ D &amp;= 1 - (0.5625 + 0.0625) \\\\\\ D &amp;= 1 - 0.625 \\\\\\ D &amp;= 0.375 \\end{aligned} \\] As the proportion in one class goes up, the value of \\(D\\) goes down. Similar to regression problems, the searches through all possible features to find the optimal split that minimizes \\(D\\). Regardless of whether the decision tree is built to solve a regression or classification problem, the algorithm is built recursively, with new optimal splits determined from previous splits. This “top down” approach is one example of a greedy algorithm, in which a series of localized optimal decisions are made. In other words, the optimal split is determined for each node. There is no going backwards to check if a different combination of features and splits would lead to better performance. "],["visualizing-decision-trees.html", "4.2 Visualizing decision trees", " 4.2 Visualizing decision trees The decision tree itself is often helpful for understanding how predictions are made. Scatterplots can also be helpful in terms of viewing how the predictor space is being partitioned. For example, imagine we are fitting a model to the following data ggplot(mpg, aes(displ, cty)) + geom_point() In this case, we only have a single predictor variable, displ, but we can split on that variable multiple times. Let’s start with a single split, and we’ll build from there. The decision tree for a single split model (also called a “stump”) looks like this So we are just splitting based on whether displ is greater than or equal to 2.6. How does this look in the scatterplot? Well, if the displ &gt;= 2.6, it’s a horizontal line at 14, and if displ &lt;= 2.6, it’s a horizontal line at 21. What if we add one additional split? Well then the decision tree looks like this and the scatterplot looks like this As the number of splits increases, the fit to the data increases. However, the model can also quickly overfit and not generalize to new data well, which is why a single decision tree is often not the most performant model. We can visualize classification problems with decision trees using scatterplots as well, as long as there are two continuous predictor variables. Sticking with our mpg dataset, let’s say we wanted to predict the drivetrain: front-wheel drive, rear-wheel drive, or four-wheel drive. The scatterplot would then look like this (assuming we’re now using displ and cty as predictor variables). ggplot(mpg, aes(displ, cty)) + geom_point(aes(color = drv)) How does the predictor space get divided up? Let’s try first with a stump model. The tree looks like this drv_stump &lt;- rpart(drv ~ displ + cty, mpg, control = list(maxdepth = 1)) rpart.plot(drv_stump, type = 4) This model isn’t doing too well. It looks like we’re not capturing any of the rear-wheel drive cases. That of course makes sense, because we only have one split, so we can’t predict three different classes. Here’s what it looks like with the scatterplot NOTE: There’s a bug (I think) in the package that currently means I have to flip the axes here. We should come back and change it when it’s fixed. See here: https://github.com/grantmcdermott/parttree/issues/5 library(parttree) ggplot(mpg, aes(cty, displ)) + geom_parttree(aes(fill = drv), data = drv_stump, alpha = 0.4, flipaxes = TRUE) + geom_point(aes(color = drv)) And what if we go with a more complicated model? Say, 3 splits? Then the tree looks like this. drv_threesplits &lt;- rpart(drv ~ displ + cty, mpg, control = list(maxdepth = 3)) rpart.plot(drv_threesplits, type = 4) And our predictor space can now be divided up better. ggplot(mpg, aes(displ, cty)) + geom_parttree(aes(fill = drv), data = drv_threesplits, alpha = 0.4, flipaxes = TRUE) + geom_point(aes(color = drv)) This looks considerably better and, although we still have some misclassification, that’s ultimately probably a good thing because we don’t want to start to overfit to our training data. Decision trees, like all models, should balance the bias-variance tradeoff. They are flexible and make few assumptions about the data, but that can also quickly lead to overfitting and poor generalizations to new data. "],["fitting-a-decision-tree.html", "4.3 Fitting a decision tree", " 4.3 Fitting a decision tree As an applied example, let’s fit a decision tree model to data from the 2019 Data Science Bowl from Kaggle. These data come from PBS KIDS on data from an educational gaming app called Measure Up!. You can actually still submit to this competition (as of the time of this writing) if you’d like to see how performant your model is. The objective is to fit a model to predict kiddo’s (ages 3-5) scores on an in-game assessment. The outcome is accuracy_group, which is an ordered categorical variable ranging from 0-4. See the Kaggle description of the data for more information. 4.3.1 Load the data First, we need to read in the train.csv and train_labels.csv data. Note that I’ve removed the event_data column, which includes JSON data on event_count, event_code, and game_time, which are already represented as columns in train.csv. I’ve also sampled only read in the first 10,000 rows of the data so that things will run more quickly. k_train &lt;- read_csv(here::here(&quot;data&quot;, &quot;ds-bowl-2019.csv&quot;), col_types = cols(.default = col_guess(), accuracy_group = col_character())) After joining the training data with the labels, our full training dataset look like this Next we’ll create our initial split, pull our training data, and create our \\(k\\)-fold cross-validation data object. library(tidymodels) splt &lt;- initial_split(k_train) train &lt;- training(splt) cv &lt;- vfold_cv(train) Let’s now create a basic recipe for this dataset. In this case, we won’t spend much time on the feature engineering (because we’re focusing on the modeling). So we’ll just use the recipe to specify the outcome as a factor with four levels. rec &lt;- recipe(accuracy_group ~ ., data = train) %&gt;% step_mutate(accuracy_group = as.factor(accuracy_group)) And finally, we’ll specify the type of model we want to fit. Because we’re using {tidymodels} metapackage, and the {parsnip} package in particular for specifying our model, the code setup is essentially equivalent to what we’ve seen in the past. Rather than specifying, say, lineary_regression(), however, we just use decision_tree(). We then specify the engine to estimate the model, which will be {rpart}, and set our mode (classification). dt_mod &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) And finally, we can estimate the model! Let’s use fit_resamples() so we can get an idea of how the model performs using the default settings. Notice I’ve added timings here so we have a sense of how long each takes (which is a while). library(tictoc) tic() dt_default &lt;- fit_resamples(dt_mod, preprocessor = rec, resamples = cv) toc() ## 7.087 sec elapsed As we’ve seen before, the code above fits the decision tree model with the default parameters to each of the 10 folds, and evaluates the predictions from that model against the left out set. Let’s look at our model performance: collect_metrics(dt_default) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.647 10 0.00919 ## 2 roc_auc hand_till 0.784 10 0.00712 Not too bad, but given that we haven’t tuned the model at all, it’s likely we can improve on this. To learn more about these models and how much they differ across folds, we probably want to actually inspect the splits (e.g., how many) and (at least) the root nodes. Unfortunatley we can’t do that with the dt_default object because, by default, the model object is not saved (which generally makes sense from an efficiency and storage perspective). Let’s re-run this, but this time ask it to also save the model object. tic() dt_default2 &lt;- fit_resamples( dt_mod, preprocessor = rec, resamples = cv, control = control_resamples(extract = function(x) extract_model(x)) ) toc() ## 6.753 sec elapsed We can visualize the model using the {rpart.plot} package. Actually pulling out the model is a bit difficult, because it’s pretty deeply nested, but it’s not horrible. Let’s write a function to do so. pull_model &lt;- function(extract_fold) { extract_fold$.extracts[[1]] } And now we’ll loop through the full list of extract to get a list of just the models. all_mods &lt;- map(dt_default2$.extracts, pull_model) And now we can easily look at any of the models. Here’s the tree for the first. library(rpart.plot) rpart.plot(all_mods[[1]], type = 4) If you’re running this with me, you’ll notice that you get a warning here about not being able to retrieve the data used to build the model. This is generally not a big deal for our purposes. I’ve also included the optional type argument, which just changes the way the tree is drawn a bit. By comparison, let’s look at the tree for the third fold. rpart.plot(all_mods[[3]], type = 4) Notice that the models are quite different. This show the potential for high variability with decision trees (while also potentially having low bias). "],["tuning-decision-trees.html", "4.4 Tuning decision trees", " 4.4 Tuning decision trees In the previous section, we fit a model with the default settings. Can we improve performance by changing these? Let’s find out! But first, what might we change? 4.4.1 Decision tree hyperparamters Decision trees have three hyperparamters as shown below. These are standard hyperparameters and are implemented in {rpart}, the engine we used for fitting decision tree models in the previous section. Alternative implementations may have slightly different hyperparameters (see the documentation for parsnip::decision_tree() details on other engines). Hyperparameter Function Description Cost Complexity cost_complexity() A regularization term that introduces a penalty to the objective function and controls the amount of pruning. Tree depth tree_depth() The maximum depth the tree should be grown Minimum \\(n\\) min_n() The minimum number of observations that must be present in a terminal node. Perhaps the most important hyperparameter is the cost complexity parameter, which is a regularization parameter that penalizes the objective function by model complexity. In other words, the deeper the tree, the higher the penalty. The cost complexity parameter is typically denoted \\(\\alpha\\), and penalizes the sum of squared errors by \\[ SSE + \\alpha |T| \\] where \\(T\\) is the number of terminal nodes. Any value can be use for alpha, but typical values are less that 0.1. The cost complexity helps control model complexity through a process called pruning, in which a decision tree is first grown very deep, and then pruned back to a smaller subtree. The tree is initially grown just like any standard decision tree, but it is pruned to the subtree that optimizes the penalized objective function above. Different values of \\(\\alpha\\) will, of course, lead to different subtrees. The best values are typically determined via grid search via cross validation. Larger cost complexity values will result in smaller trees, while smaller values will result in more complex trees. Note that, similar to penalized regression, if you are using cost complexity to prune a tree it is important that all features are placed on the same scale (normalized) so the scale of the feature doesn’t influence the penalty. The tree depth and minimum \\(n\\) are a more straightforward methods to control model complexity. The tree depth is just the maximum depth to which a tree can be grown (maximum number of splits). The minimum \\(n\\) controls the splitting criteria. A node cannot be split further once the \\(n\\) within that node is below the minimum specified. 4.4.2 Conducting the grid search Let’s tune our model using cost_complexity() and min_n() and let the depth be controlled by these parameters. First, we’ll modify our model from before to set the parameters to tune. dt_tune &lt;- dt_mod %&gt;% set_args(cost_complexity = tune(), min_n = tune()) Next, we’ll set up our grid. We can use helper functions from the {dials} package (part of tidymodels) to help us come up with reasonable values. Let’s use a regular grid with 10 possible values for cost complexity and 5 possible values for the minimum \\(n\\). dt_grid &lt;- grid_regular( cost_complexity(), min_n(), levels = c(10, 5) ) Let’s see what the space we’rd evaluating actually looks like for our hyperparamters. ggplot(dt_grid, aes(cost_complexity, min_n)) + geom_point() As we can see, there’s a big gap in cost complexity, so we’ll want to be careful when we investigate the optimal values there. Now let’s actually conduct the search. I have again included timing here so you can see how long it took for me to run on my local computer (which is a decent amount of time). tic() dt_tune_fit &lt;- tune_grid( dt_tune, preprocessor = rec, resamples = cv, grid = dt_grid ) toc() ## 946.207 sec elapsed First let’s look at our results by hyperparameter. We can use collect_metrics to get a summary (mean) of the metrics we set (which we didn’t, so we’ll get the defaults, which in this case are roc_auc and accuracy) across folds. You can optionally get te results by fold (no summarizing) by specifying summarize = FALSE. dt_tune_metrics &lt;- collect_metrics(dt_tune_fit) dt_tune_metrics ## # A tibble: 100 x 8 ## cost_complexity min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0000000001 2 accura… multiclass 0.631 10 0.0104 ## 2 0.0000000001 2 roc_auc hand_till 0.718 10 0.00915 ## 3 0.000000001 2 accura… multiclass 0.631 10 0.0104 ## 4 0.000000001 2 roc_auc hand_till 0.718 10 0.00915 ## 5 0.00000001 2 accura… multiclass 0.631 10 0.0104 ## 6 0.00000001 2 roc_auc hand_till 0.718 10 0.00915 ## 7 0.0000001 2 accura… multiclass 0.631 10 0.0104 ## 8 0.0000001 2 roc_auc hand_till 0.718 10 0.00915 ## 9 0.000001 2 accura… multiclass 0.631 10 0.0104 ## 10 0.000001 2 roc_auc hand_till 0.718 10 0.00915 ## # … with 90 more rows, and 1 more variable: .config &lt;chr&gt; To get an idea of how things are performing, let’s look at our two hyperparameters with roc_auc as our metric. This is a metric we want to maximize, with a value of 1.0 indicating perfect predictions. dt_tune_metrics %&gt;% filter(.metric == &quot;roc_auc&quot;) %&gt;% ggplot(aes(cost_complexity, mean)) + geom_point() + facet_wrap(~min_n) So generally it’s looking like lower values of of cost complexity are leading to better performing models, and the minimum sample size for a terminal node is looking best at 21 or 30. Let’s look at our best models in a more tabular form. This time we’ll use show_best to show our best hyperparameter combinations. show_best(dt_tune_fit, metric = &quot;roc_auc&quot;) ## # A tibble: 5 x 8 ## cost_complexity min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.001 21 roc_auc hand_till 0.831 10 0.00485 ## 2 0.001 2 roc_auc hand_till 0.830 10 0.00905 ## 3 0.001 11 roc_auc hand_till 0.829 10 0.00714 ## 4 0.0001 30 roc_auc hand_till 0.828 10 0.00361 ## 5 0.0000000001 30 roc_auc hand_till 0.827 10 0.00365 ## # … with 1 more variable: .config &lt;chr&gt; And we see a little bit different picture here. Our best model has a minimum \\(n\\) of 11 with a relatively higher cost complexity. But the amount this model is “better” is trivial and could easily be due to sampling variability. All of the rest of the best models have the same minimum \\(n\\), with the cost complexity playing essentially no role. This may lead us to consider not pruning by the cost complexity parameter at all. Let’s use all the results again to look at the minimum \\(n\\) a little closer. We’ll filter for a very low cost complexity. dt_tune_metrics %&gt;% filter(.metric == &quot;roc_auc&quot; &amp; cost_complexity == 0.0000000001) %&gt;% arrange(desc(mean)) ## # A tibble: 5 x 8 ## cost_complexity min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0000000001 30 roc_auc hand_till 0.827 10 0.00365 ## 2 0.0000000001 21 roc_auc hand_till 0.825 10 0.00459 ## 3 0.0000000001 40 roc_auc hand_till 0.822 10 0.00469 ## 4 0.0000000001 11 roc_auc hand_till 0.812 10 0.00682 ## 5 0.0000000001 2 roc_auc hand_till 0.718 10 0.00915 ## # … with 1 more variable: .config &lt;chr&gt; Unsurprisingly, 30 is looking best for our minimum \\(n\\). To be sure we’ve got this right though, let’s set cost_complexity and tune just on the minimum \\(n\\). We know that values of 21 and 40 are both worst than 30, but let’s see if there’s any more room for optimization around there. This shouldn’t take quite as long as our previous grid search. grid_min_n &lt;- tibble(min_n = 23:37) dt_tune2 &lt;- dt_tune %&gt;% set_args(cost_complexity = 0.0000000001) tic() dt_tune_fit2 &lt;- tune_grid( dt_tune2, preprocessor = rec, resamples = cv, grid = grid_min_n ) toc() ## 92.516 sec elapsed Let’s look at our best metrics now and see if we’ve made any improvements. show_best(dt_tune_fit2, metric = &quot;roc_auc&quot;) ## # A tibble: 5 x 7 ## min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 27 roc_auc hand_till 0.833 10 0.00453 Model05 ## 2 24 roc_auc hand_till 0.833 10 0.00345 Model02 ## 3 25 roc_auc hand_till 0.833 10 0.00356 Model03 ## 4 26 roc_auc hand_till 0.833 10 0.00448 Model04 ## 5 28 roc_auc hand_till 0.832 10 0.00484 Model06 And look at that! It’s a (very) marginal improvement, but we have optimized our model a bit more. 4.4.3 Finalizing our model fit Generally before moving to the our final fit we’d probably want to do a bit more work with the model to make sure we were confident it was really the best model we could produce. I’d be particularly interested at looking at minimum \\(n\\) around the 0.001 cost complexity parameter (given that the overall optimum in our original gridsearch had this value with a minimum \\(n\\) of 11). But for illustration purposes, let’s assume we’re ready to go (and really, decision trees don’t have a lot more tuning we can do with them, at least using the rpart engine). First, let’s finalize our model using the best min_n we found from our grid search. We’ll use finalize_model along with select_best (rather than show_best) to set the final model parameters. best_params &lt;- select_best(dt_tune_fit2, metric = &quot;roc_auc&quot;) final_mod &lt;- finalize_model(dt_tune2, best_params) final_mod ## Decision Tree Model Specification (classification) ## ## Main Arguments: ## cost_complexity = 1e-10 ## min_n = 27 ## ## Computational engine: rpart Note that the min_n is now set. If we had done any tuning with our recipe we could follow a similar process. Next, we’re going to use our original initial_split() object to, with a single function fit our model to our full training data (rather than by fold) and make predictions on the test set, and evalute the performance of the model. We do this all throught he last_fit function. dt_finalized &lt;- last_fit(final_mod, preprocessor = rec, split = splt) dt_finalized ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [3… train/t… &lt;tibble … &lt;tibble… &lt;tibble [12,… &lt;workflo… What we get output doesn’t look terrifically helpful, but it is. It’s basically everything we need. For example, let’s look at our metrics. dt_finalized$.metrics[[1]] ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.614 ## 2 roc_auc hand_till 0.823 unsurprisingly, our AUC is a bit lower for our test set. What if we want our predictions? predictions &lt;- dt_finalized$.predictions[[1]] predictions ## # A tibble: 12,500 x 7 ## .pred_0 .pred_1 .pred_2 .pred_3 .row .pred_class ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 0.235 0 0.0294 0.735 1 3 ## 2 0.235 0 0.0294 0.735 4 3 ## 3 0.357 0 0.143 0.5 6 3 ## 4 0.357 0 0.143 0.5 9 3 ## 5 0.154 0 0.385 0.462 22 3 ## 6 0.542 0 0.375 0.0833 28 0 ## 7 0.542 0 0.375 0.0833 32 0 ## 8 0.0588 0 0.294 0.647 33 3 ## 9 0.111 0 0.222 0.667 36 3 ## 10 0.111 0 0.222 0.667 37 3 ## # … with 12,490 more rows, and 1 more variable: ## # accuracy_group &lt;fct&gt; This shows us the predicted probability that each case would be in each class, along with a “hard” prediction into a class, and their observed class (accuracy_group). We can use this for further visualizations and to better understand how our model makes predictions and where it is wrong. For example, let’s look at a quick heat map of the predicted class versus the observed. counts &lt;- predictions %&gt;% count(.pred_class, accuracy_group) %&gt;% drop_na() %&gt;% group_by(accuracy_group) %&gt;% mutate(prop = n/sum(n)) ggplot(counts, aes(.pred_class, accuracy_group)) + geom_tile(aes(fill = prop)) + geom_label(aes(label = round(prop, 2))) + colorspace::scale_fill_continuous_diverging( palette = &quot;Blue-Red2&quot;, mid = .25, rev = TRUE) Notice that I’ve omitted NA’s here, which is less than ideal, because we have a lot of them. This is mostly because the original data themselves have so much missing data on the outcome, so it’s hard to know how well we’re actually doing with those cases. Instead, we’re just evaluating our model with the cases for which we actually have an observed outcome. The plot above shows the proportion by row. In other words, each row sums to 1.0. We can fairly quickly see that our model has some fairly significant issues. We are doing okay predicting classes for 0 and 3 (about 75% correct, in each case) but we’re not a whole lot better than random chance leve (which would be 0.25-ish in each cell) when predicting Classes 1 and 2. It’s fairly concerning that 32% of cases that were actually Class 2 were predicted to be Class 0. We would likely want to conduct a post-mortem with these cases to see if we could understand why our model was failing in this particular direction. Decision trees, generally, are easily interpretable and easy to communicate with stakeholders. They also make no assumptions about the data, and can be applied in a large number of situations. Unfortunately, they often suffer from rapid overfitting to the data leading to poor generalizations to unseen data. In the next chapter, we’ll build on decision trees to talk about ensemble methods, where we use multiple trees to make a single prediction. "],["bagging-and-random-forests.html", "5 Bagging and Random Forests", " 5 Bagging and Random Forests In the last chapter, we talked about how decision trees are highly flexible, but are often not the most performant model on their own because they can easily overfit, leading to poor generalizability to new/unseen data. One way to avoid this problem is to build an ensemble of trees from random bootstrap samples of the data, and aggregate the predictions across the entire ensemble. This is a general approach known as bagging, or bootstrap aggregation, which can be applied to any modeling framework, but generally will only provide large gains in improvement if, like decision trees, the model has high variability. Bagging: A general process to improve the performance of highly variable models, regularly applied to decision trees. Create \\(b\\) bootstrap resamples of the original data Fit the model (base learner) to each \\(b\\) resample Aggregate predictions across all models For regression problems, the final prediction is the average of all model predictions For classification problems, either (a) average model classification probabilities or (b) take the mode of model classifications. Benefits: Leads to more stable model predictions (reduces model variability) As mentioned previously, bagging does not tend to help much (and increases computational burdens) when the model already has low variance. Models like linear regression will generally not change much in their model predictions when using bagging. Decision trees, however, can be highly variable. Bagging can help reduce the variance of these models and lead to more stable model predictions. How many bags? There is no hard and fast rule. The important thing is to have enough bags to reach a stable model. Noisier data will require more bags. Data with strongly predictive features will require fewer bags. Start with somewhere between between 50-500 bags, evaluate the learning curve, and adjust the bags from there. There is no rule for the number of “bags”, or bootstrap resamples, that one should use to create a stable model. Further, the number of bags just needs to be sufficiently high that the model becomes stable—after stability is achieved, additional bags will not help model performance. In other words, there is no upper bound for the number of bags (the only burden is computational), but it is critical that there are enough bags to create a stable model. Datasets with highly predictive features will generally need fewer bags to reach stability. A good rule of thumb is to start with somewhere between 50-500 bags, depending on how variable you think your data are, and then adjust up or down from there accordingly. 5.0.1 Bagging “by hand” To understand bagging we first have to be clear about what bootstrap resampling implies. When we take bootstrap resamples from our dataset, we sample \\(n\\) rows of our dataset with replacement, where \\(n\\) represents the total number of rows in our data. For example, suppose we had a dataset that had the first five letters of the alphabet, each with an associated score. lets &lt;- data.frame(letters = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;), score = c(5, 7, 2, 4, 9)) lets ## letters score ## 1 a 5 ## 2 b 7 ## 3 c 2 ## 4 d 4 ## 5 e 9 Bootstrap resampling would imply sampling five rows from the above dataset with replacement. This means some rows may be represented multiple times, and others not at all. Let’s do this and see what the first three datasets look like. # set seed for reproducibility set.seed(42) # specify the number of bootstrap resamples b &lt;- 3 resamples &lt;- replicate(b, lets[sample(1:5, 5, replace = TRUE), ], simplify = FALSE) resamples ## [[1]] ## letters score ## 1 a 5 ## 5 e 9 ## 1.1 a 5 ## 1.2 a 5 ## 2 b 7 ## ## [[2]] ## letters score ## 4 d 4 ## 2 b 7 ## 2.1 b 7 ## 1 a 5 ## 4.1 d 4 ## ## [[3]] ## letters score ## 1 a 5 ## 5 e 9 ## 4 d 4 ## 2 b 7 ## 2.1 b 7 Notice that in the first bootstrap resample, a is represented three times, b once, c and d not at all, and e once. Similar patterns, with different distributional frequencies, are represented in the second and third datasets. Why is this useful? It turns out that if we do this enough times, we develop a sampling distribution. Fitting the model to all of these different samples then gives us an idea of the variability of the model, which we can reduce by averaging across all samples. Bootstrap resampling is useful in all sorts of different ways in statistics. In the above, our observed mean across the letters is 5.4. We can compute the standard error of this mean analytically by \\(\\sigma/\\sqrt{n}\\), or sd(lets$score)/sqrt(5), which is equal to 1.2083046. We can also approximate this same standard error by computing the mean of many bootstrap resamples, and estimating the standard deviation among these means. For example library(tidyverse) b &lt;- 5000 resamples &lt;- replicate(b, lets[sample(1:5, 5, replace = TRUE), ], simplify = FALSE) means &lt;- map_dbl(resamples, ~mean(.x$score)) sd(means) ## [1] 1.108987 In this case, the difference between the analytic standard error and the bootstrap estimate is greater than typical because the sample size is so small. The process of bagging is essentially equivalent to the above, except instead of computing the mean with each bootstrap resample, we fit a full model. We then compute the predictions from all of these models and either average the resulting predictions, or take the mode of the classifications. "],["bagged-trees.html", "5.1 Bagged trees", " 5.1 Bagged trees The {baguette} package, part of the {tidymodels} metapackage, provides an interface for bagging in R. It is not part of the core set of packages, implying it is installed with {tidymodels} but not loaded. You must load {baguette} outside of your call to {tidymodels} (i.e., similar to the {lubridate} package in the {tidyverse}). Recall our best model when fitting a decision tree in the previous chapter had an average AUC across folds of \\(0.825\\). This included a very low cost complexity parameter of \\(0.0000000001\\) and a minimum \\(n\\) for our terminal nodes of 35. Can we improve performance from this model when using bagging? Let’s try! First, we need to load the data, create a split training/test set, pull the training data, and create a \\(k\\)-fold cross-validation dataset. Some of the models we’ll be working with cannot handle missingness on the outcome, so we’ll remove these rows upon reading the data into R. library(tidyverse) library(tidymodels) k_train &lt;- read_csv( here::here(&quot;data&quot;, &quot;ds-bowl-2019.csv&quot;), col_types = cols(.default = col_guess(), accuracy_group = readr::col_factor(levels = as.character(0:3), ordered = TRUE)) ) %&gt;% drop_na(accuracy_group) splt &lt;- initial_split(k_train) train &lt;- training(splt) cv &lt;- vfold_cv(train) Next, we’ll specify a basic recipe that just specifies the model formula. rec &lt;- recipe(accuracy_group ~ ., data = train) And now we’re ready to specify our model. This is pretty much the same as before, except now we are going to load the {baguette} package in addition to {tidymodels} and use bag_tree() instead of decision_tree(). Additionally, we’ll specify a times argument when we set the engine. Let’s start by fitting a model to 50 bootstrap resamples and aggregating the results across all 50 trees. The rest is the same as before. Let’s start by by building a very deep tree, with no pruning and a minimum sample size for the terminal nodes of 2. We’ll also use parallel processing here via the {future} package to help the model run faster. The code below also includes timings via teh {tictoc} package. library(baguette) library(tictoc) # set model bt_mod &lt;- bag_tree() %&gt;% set_engine(&quot;rpart&quot;, times = 50) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_args(cost_complexity = 0, min_n = 2) # fit to $k$ folds tic() bt_fit1 &lt;- fit_resamples(bt_mod, preprocessor = rec, resamples = cv) toc(log = TRUE) # `log = TRUE` so I can refer to this timing later ## 151.234 sec elapsed collect_metrics(bt_fit1) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.656 10 0.00906 ## 2 roc_auc hand_till 0.840 10 0.00622 That’s a pretty decent gain! But how do we know that 50 bags is enough? We can create a learning curve by fitting our model to many different bootstrap resample values, and evaluate the objective function for each of these values. To do that, let’s write a function that specifies a model with any bootstrap value, \\(b\\), fits the model, and then extracts the AUC. Remember, we only need to find the value where our objective function stablizes. Adding additional bootstrap resamples won’t hurt in terms of model performance, but it will cost us in terms of computational time. So we want to use a value of \\(b\\) that is around the lowest possible value once stability has been reached (so we don’t waste computational time). When we fit the model above, we used fit_resamples() using 10-fold cross validation. This time, we only want to get a rough estimate of model stability. So, to save on computation time, let’s create a small cv object with just two folds, then use this to fit all the \\(b\\) candidate models. # specify a small cv small_cv &lt;- vfold_cv(train, v = 2) pull_auc &lt;- function(b) { # specify model mod &lt;- bag_tree() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_args(cost_complexity = 0, min_n = 2) %&gt;% set_engine(&quot;rpart&quot;, times = b) # fit model to full training dataset m &lt;- fit_resamples(mod, rec, small_cv) # extract the AUC &amp; add the $b$ value auc &lt;- show_best(m, &quot;roc_auc&quot;) auc$b &lt;- b # return the AUC data frame auc } Now we just need to specify a vector of candidate bags, \\(b\\), and loop our function through this vector. We’ll look at values between \\(5\\) and \\(305\\) by increments of 25. Note that we have used parallel processing here again to help speed things along. This is still a pretty time-intensive operation. As the timings below indicate, it took a pretty decent amount of time to run for us (we’ll talk about more efficient ways to do this in the next section). # specify candidate b models b &lt;- seq(5, 305, 25) # Fit models library(future) plan(multisession) tic() aucs &lt;- map_df(b, pull_auc) toc() ## 287.645 sec elapsed plan(sequential) Let’s plot these samples now to see when we reach stability. Note ggplot(aucs, aes(b, mean)) + geom_line() + geom_point() And it looks like after about 150 bags the model becomes stable. Moving forward, we could proceed with model tuning just as we did before, using \\(k\\)-fold cross validation, and using a bagged tree model with \\(b = 150\\). However, as the process above illustrates, this can be a highly computationally intensive process. We would need to fit decision trees to each of 200 bootstrap resamples, for each of the \\(k\\) folds for every hyperparameter we evaluated. In the Decision Trees chapter, we evaluated 50 hyperparamters in our initial model tuning (10 for cost complexity and 5 for the minimum \\(n\\) size for a terminal node). Assuming 10-fold cross-validation, this would result in \\(50 \\times 10 \\times 150 = 75000\\) decision trees! That’s going to take a long time. Luckily, there are alternative options. 5.1.1 Working with out of bag samples Recall from our chapter on cross-validation procedures that there are multiple approaches to cross-validation, including bootstrap resampling. When using boostrap resampling for cross-validation, we fit a candidate model on the boostrapped data, and evaluate it against the cases that were not included in the bootstrap. For example, if our data looked like this: lets ## letters score ## 1 a 5 ## 2 b 7 ## 3 c 2 ## 4 d 4 ## 5 e 9 and our bootstrap resample looked like this resamples[[1]] ## letters score ## 3 c 2 ## 1 a 5 ## 1.1 a 5 ## 3.1 c 2 ## 4 d 4 Then we would fit our model to letters a, b, and e, and evaluate our model by making predictions for letters c and d. If you’re using bagging to develop a model, you already have bootstrap resamples. The out-of-bag (OOB) samples are then “free”, computationally. If your sample size is reasonably large (\\(n &gt; 1000\\)) the OOB estimates of model performance will be similar to those obtained from \\(k\\)-fold CV, but take only a fraction of the time. Unfortunately, as of the time of this writing, there is no way to easily access the OOB samples with {baguette}. Luckily, we can fit the model in a slightly different way, using the {ranger} package, and this will allow us to access the OOB samples. In what follows, we’ll use {ranger} within a {tidymodels} framework to fit and tune a bagged tree model using the OOB samples. The {ranger} package is designed to fit random forests, which we’ll talk about next. Bagged trees, however, are just a special case of random forests where there is no sampling of columns when each tree is built (more on this soon). To fit a bagged tree with {ranger}, we just have to set the mtry argument equal to the number of predictors in our data frame. Let’s start by re-fitting our bt_mod1 model with {ranger}, using the OOB samples for our model performance. To do this, we’re going to use the fit() function, instead of fit_resamples() (because we’re only going to be fitting the model once). We will therefore need to prep() and bake() our recipe to get our processed training data. processed_train &lt;- rec %&gt;% prep() %&gt;% bake(new_data = NULL) processed_train ## # A tibble: 2,767 x 6 ## event_count event_code game_time title world accuracy_group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;ord&gt; ## 1 1 2000 0 Mushro… TREE… 3 ## 2 3 3010 37 Mushro… TREE… 3 ## 3 5 3010 3901 Mushro… TREE… 3 ## 4 10 4025 8400 Mushro… TREE… 3 ## 5 12 3121 8926 Mushro… TREE… 3 ## 6 13 4025 9502 Mushro… TREE… 3 ## 7 16 3121 10210 Mushro… TREE… 3 ## 8 17 2035 10210 Mushro… TREE… 3 ## 9 18 2020 10210 Mushro… TREE… 3 ## 10 20 4070 10543 Mushro… TREE… 3 ## # … with 2,757 more rows Next, it’s helpful to determine the number of predictors we have with code. In this case, it’s fairly straightforward, but occassionally things like dummy-coding can lead to many new columns, and zero or near-zero variance filters may remove columns, so it’s worth double-checking our assumptions. ncol(processed_train) - 1 ## [1] 5 Note that we subtract one from the number of columns because we are only counting predictors (not the outcome). Next, we specify the model. Notice we use rand_forest() here for our model, even though we’re actually fitting a bagged tree, and we set mtry = 5. The number of bags is set by the number of trees. Note that, while we found a higher value is likely better, we’ve set the number of trees below to be 50 so the model is comparable to bt_mod. There is no pruning hyperparameter with {ranger}, but we can set the min_n to 2 as we had it before. The below code includes one additional argument that is passed directly to {ranger}, probability = FALSE, which will make the predictions from the model be the actual classes, instead of the probabilities in each class. bt_mod2 &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_args(mtry = 5, trees = 50, min_n = 2, probability = FALSE) Now we just fit the model to our processed data. tic() bt_fit2 &lt;- fit(bt_mod2, accuracy_group ~ ., processed_train) toc(log = TRUE) # `log = TRUE` so I can refer to this timing later ## 0.54 sec elapsed As you can see, we have substantially cut the fitting time down because we’ve only fit the model once. We went from 2.52 minutes to only 0.54 seconds! But do we get the same estimates for our metrics if we use the OOB samples? Let’s look at the model object bt_fit2 ## parsnip model object ## ## Fit time: 177ms ## Ranger result ## ## Call: ## ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~5, x), num.trees = ~50, min.node.size = min_rows(~2, x), probability = ~FALSE, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Classification ## Number of trees: 50 ## Sample size: 2767 ## Number of independent variables: 5 ## Mtry: 5 ## Target node size: 2 ## Variable importance mode: none ## Splitrule: gini ## OOB prediction error: 33.25 % This says that our OOB prediction error is 33.25. Our accuracy is one minus this value, or 66.75. Using \\(k\\)-fold cross validation we estimated our accuracy at 65.63. So we’re getting essentially the exact same results, but in this case using the OOB samples is approximately 280 times faster! What if we want other metrics? We can access the OOB predictions from our model using bt_fit2$fit$predictions. We can then use these predictions to calculate OOB metrics via the {yardstick} package, which is used internally for functions like collect_metrics(). For example, assume we wanted to estimate the OOB AUC. In this case, we would need to re-estimate our model to get the predicted probabilites for each class. bt_mod3 &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_args(mtry = 5, trees = 50, min_n = 2, probability = TRUE) # this is the default bt_fit3 &lt;- fit(bt_mod3, accuracy_group ~ ., processed_train) Now we just pull the OOB predicted probabilities for each class, and add in the observed class. preds &lt;- bt_fit3$fit$predictions %&gt;% as_tibble() %&gt;% mutate(observed = processed_train$accuracy_group) preds ## # A tibble: 2,767 x 5 ## `0` `1` `2` `3` observed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; ## 1 0.354 0 0.260 0.386 3 ## 2 0.824 0 0 0.176 3 ## 3 0 0 0.929 0.0714 3 ## 4 0.406 0 0.344 0.25 3 ## 5 0.176 0 0.765 0.0588 3 ## 6 0.111 0 0.111 0.778 3 ## 7 0.227 0 0 0.773 3 ## 8 0.269 0 0.154 0.577 3 ## 9 0.312 0 0.125 0.562 3 ## 10 0.289 0 0.158 0.553 3 ## # … with 2,757 more rows And now we can use this data frame to estimate our AUC, using yardstick::roc_auc(). roc_auc(data = preds, truth = observed, `0`:`3`) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc hand_till 0.843 How does this compare to our estimate with 10-fold CV? collect_metrics(bt_fit1) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.656 10 0.00906 ## 2 roc_auc hand_till 0.840 10 0.00622 It’s very close and, again, took a fraction of the time. 5.1.2 Tuning with OOB samples If we want to conduct hyperparameter tuning with a bagged tree model, we have to go to a bit more work, but it’s not too terrible. Let’s train on minimum \\(n\\) and set the number of trees to be large—say, 200. Much like we did before, we’ll write a function that fits a model for any min_n value. We’ll optimize our model by trying to maximize AUC, so we’ll have our function return the OOB AUC estimate, along with the \\(n\\) size that was used for the terminal nodes. tune_min_n &lt;- function(n) { mod &lt;- rand_forest() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = 5, min_n = n, trees = 200) # fit model to full training dataset m &lt;- fit(mod, accuracy_group ~ ., processed_train) # create probabilities dataset pred_frame &lt;- m$fit$predictions %&gt;% as_tibble() %&gt;% mutate(observed = processed_train$accuracy_group) # calculate auc auc &lt;- roc_auc(pred_frame, observed, `0`:`3`) %&gt;% pull(.estimate) # pull just the estimate # return as a tibble tibble(auc = auc, min_n = n) } Now we can loop through a bunch of \\(n\\) sizes for the terminal nodes and see which provides us the best OOB AUC values for a bagged tree with 200 bags. We’ll use map_df() so the results are bound into a single data frame. Let’s search through values from 2 to 50 and see how the OOB AUC changes. tic() min_n_aucs &lt;- map_df(2:50, tune_min_n) toc() ## 37.125 sec elapsed Let’s plot the learning curve. ggplot(min_n_aucs, aes(min_n, auc)) + geom_line(color = &quot;gray40&quot;) + geom_smooth(se = FALSE) Because we are trying to maximize AUC, the ideal value appears to be somewhere around 15. Let’s extract the maximum AUC \\(n\\). max_auc &lt;- min_n_aucs %&gt;% filter(auc == max(auc)) max_auc ## # A tibble: 1 x 2 ## auc min_n ## &lt;dbl&gt; &lt;int&gt; ## 1 0.859 6 And now we’re likely ready to finalize our model. Let’s evaluate it against our test set. final_mod &lt;- rand_forest() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = 5, min_n = 13, trees = 200) final_fit &lt;- last_fit(final_mod, rec, splt) final_fit$.metrics ## [[1]] ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.680 ## 2 roc_auc hand_till 0.871 And our final AUC estimate on our test set is essentially equivalent to what we found during training using our OOB samples "],["random-forests.html", "5.2 Random Forests", " 5.2 Random Forests One potential problem with bagged trees is that the predictions across trees often correlate strongly. This is because, even if a new bootstrap sample is used to create the tree, all of the features are used in every tree. Thus, if a given feature is strongly predictive of the outcome, it is likely to be used as the root node in nearly every tree. Because decision trees are built recursively, with optimal splits only determined after previous splits have already been determined, this means that we may be missing out on potential predictive accuracy by not allowing other features to “start” the tree (be the root node). This does not mean that bagging is not useful. As we just saw, we were able to have fairly substantial gains in model performance using bagging when compared to what we observed with a single tree. Yet, if we could decorrelate the trees, we might be able to improve performance even more. Random forests extend bagged trees by introducing a stochastic component to the features. Specifically, rather than building each tree with all of the features, each tree is built using a random sample of features at each split. The predictive performance of any individual tree is then unlikely to be as performant as a tree using all the features at each split, but the forest of trees (aggregate prediction) can regularly provide better predictions than any single tree. Assume we have a features with one very strong predictor and a few features that are moderately strong predictors. If we used bagging, the moderately strong predictors would likely be internal nodes for nearly every tree–meaning their importance would be conditional on the strong predictor. Yet, these variables might provide important information for specific samples on their own. If we remove the very strong predictor for the root node split, that tree we be built with one of the moderately strong predictors at the root node. The very strong predictor may then be selected in one of the internal nodes and, while this model may not be as predictive overall, it may result in better predictions for specific cases. If we average over all these diverse models we can often get better predictions for the entire dataset. In essence, a random forest is just a bagged tree with a stochastic component introduced to decorrelate the trees. This means each individual model is more different than a bagged tree. When we average across all the trees we reduce this variance and may be able to get overall improved performance when compared to a single tree or a bagged tree. Random forests work like bagged trees, but include a random selection of features at each split. This helps decorrelate the trees and can help get at the unique features of the data. Random forest models tend to provide very good “out of the box” model performance. Because bagged trees are just a special case of random forests, and the number of predictors to select for each tree is a hyperparameter that can be tuned, it’s generally worth starting with a random forest and only moving to a bagged tree if, for your specific situation, using all the predictors for each tree works better than using a sample. 5.2.1 Fitting random forests In the previous section we used the rand_forest() function to fit bagged classification trees and obtain performance metrics on the OOB samples. We did this by setting mtry = p, where p is equal to the number of predictor variables. When fitting a random forest, we just change mtry to a smaller value, which represents the number of features to randomly select from at each split. Everything else is the same: trees represents the number of bags, or the number of tree in the forest, while min_n represents the minimum sample size for a terminal node (i.e., limiting the depth to which each tree is grown). A good place to start is mtry = p/3 for regression problems and mtry = sqrt(p) for classification problems. Generally, higher values of mtry will work better when the data are fairly noisy and the predictors are not overly strong. Lower values of mtry will work better when there are a few very strong predictors. Just like bagged trees, we need to have a sufficient number of trees that the performance estimate stabilizes. Including more trees will not hurt your model performance, but it will increase computation time. Generally, random forests will need more trees to stabilize than bagged trees, because each tree is “noisier” than those in a bagged tree. A good place to start is at about 1,000 trees, but if your learning curve suggests the model is still trending toward better performance (rather than stabilizing/flat lining) then you should add more trees. Note that the number of trees you need will also depend on other hyperparameters. Lower values of mtry and min_n will likely lead to needing a greater number of trees. Because we’re using bagging in a random forest we can again choose whether to use \\(k\\)-fold cross validation or just evaluate performance based on the OOB samples. If we were in a situation where we wanted to be extra sure we had conducted the hyperparameter tuning correctly, we might start by building a model on the OOB metrics, then evaluate a small candidate of hyperparameters with \\(k\\)-fold CV to finalize them. In this case we will only use the OOB metrics to save on computation time. 5.2.1.1 A classification example Let’s continue with our classification example, using the same training data and recipe we used in the previous section. As a reminder, the recipe looked like this: rec &lt;- recipe(accuracy_group ~ ., data = train) And we prepared the data for analysis so it looked like this: processed_train &lt;- rec %&gt;% prep() %&gt;% bake(new_data = NULL) processed_train ## # A tibble: 2,767 x 6 ## event_count event_code game_time title world accuracy_group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;ord&gt; ## 1 1 2000 0 Mushro… TREE… 3 ## 2 3 3010 37 Mushro… TREE… 3 ## 3 5 3010 3901 Mushro… TREE… 3 ## 4 10 4025 8400 Mushro… TREE… 3 ## 5 12 3121 8926 Mushro… TREE… 3 ## 6 13 4025 9502 Mushro… TREE… 3 ## 7 16 3121 10210 Mushro… TREE… 3 ## 8 17 2035 10210 Mushro… TREE… 3 ## 9 18 2020 10210 Mushro… TREE… 3 ## 10 20 4070 10543 Mushro… TREE… 3 ## # … with 2,757 more rows We will again optimize on the model AUC. Recall that we had 5 predictor variables. Let’s start by fitting a random forest with 1,000 trees, randomly sampling two predictors at each split (\\(\\sqrt{5}=2.24\\)). We’ll set min_n to 2 again so we are starting our modeling process by growing very deep trees rf_mod1 &lt;- rand_forest() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = 2, min_n = 2, trees = 1000) tic() rf_fit1 &lt;- fit(rf_mod1, formula = accuracy_group ~ ., data = processed_train) toc() ## 1.861 sec elapsed Note that even with 1,000 trees, the model fits very quickly. What does our AUC look like for this model? pred_frame &lt;- rf_fit1$fit$predictions %&gt;% as_tibble() %&gt;% mutate(observed = processed_train$accuracy_group) roc_auc(pred_frame, observed, `0`:`3`) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc hand_till 0.848 Looks pretty good! Note that this is quite similar to the best value we estimated using a bagged tree. But did our model stabilize? Did we use enough trees? Let’s investigate. First, we’ll write a function so we can easily refit our model with any number of trees, and return the OOB AUC estimate. fit_rf &lt;- function(tree_n, mtry = 2, min_n = 2) { # specify the model rf_mod &lt;- rand_forest() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = mtry, min_n = min_n, trees = tree_n) # fit the model rf_fit &lt;- fit(rf_mod, formula = accuracy_group ~ ., data = processed_train) # Create a data frame from which to make predictions pred_frame &lt;- rf_fit$fit$predictions %&gt;% as_tibble() %&gt;% mutate(observed = processed_train$accuracy_group) # Make the predictions, and output other relevant information roc_auc(pred_frame, observed, `0`:`3`) %&gt;% mutate(trees = tree_n, mtry = mtry, min_n = min_n, model = list(rf_fit)) } Notice in the above we’ve made it a bit more general so we can come back to checking the number of trees with different values of mtry and min_n. Let’s look at values from 100 (which is almost certainly too low) to 1201 by increments of 50 trees. First, we loop through these values using map_df() so they are all bound in a single data frame. test_n_trees &lt;- map_df(seq(1, 1201, 50), fit_rf) Next, we plot the result! ggplot(test_n_trees, aes(trees, .estimate)) + geom_line() And as we can see, we’re actually pretty safe with a much lower value, perhaps even as low as 100. However, because the models run very fast, we won’t worry too about this too much. When we conduct our model tuning, we’ll drop to 500 trees instead of 1000 (still well above what it appears is needed). Let’s see if we can improve performance by changing the mtry or min_n. Because we only have five predictor variables in this case, we don’t have a huge range to evaluate with mtry. But let’s setup a regular grid looking at values of 2, 3, 4 and 5 for mtry, and min_n values of 2, 5, 10, 15, 20, and 25. We can then use our fit_rf() function again, but this time setting the number of trees and looping through each of our mtry and min_n values. grid &lt;- expand.grid(mtry = 2:5, min_n = c(2, seq(5, 25, 5))) rf_tune &lt;- map2_df(grid$mtry, grid$min_n, ~fit_rf(500, .x, .y)) rf_tune %&gt;% arrange(desc(.estimate)) ## # A tibble: 24 x 7 ## .metric .estimator .estimate trees mtry min_n model ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;list&gt; ## 1 roc_auc hand_till 0.859 500 5 5 &lt;fit[+]&gt; ## 2 roc_auc hand_till 0.859 500 5 10 &lt;fit[+]&gt; ## 3 roc_auc hand_till 0.857 500 5 20 &lt;fit[+]&gt; ## 4 roc_auc hand_till 0.857 500 5 15 &lt;fit[+]&gt; ## 5 roc_auc hand_till 0.856 500 4 10 &lt;fit[+]&gt; ## 6 roc_auc hand_till 0.856 500 4 15 &lt;fit[+]&gt; ## 7 roc_auc hand_till 0.855 500 5 2 &lt;fit[+]&gt; ## 8 roc_auc hand_till 0.855 500 4 5 &lt;fit[+]&gt; ## 9 roc_auc hand_till 0.855 500 5 25 &lt;fit[+]&gt; ## 10 roc_auc hand_till 0.854 500 4 20 &lt;fit[+]&gt; ## # … with 14 more rows And notice that all of our top models here include our maximum number of predictors. So in this case, bagged trees are actually looking like our best option (rather than a random forest). 5.2.1.2 A regression example For completeness, let’s run through another example using our statewide testing data. We didn’t use these data when fitting a bagged tree model, but we can start with a random forest anyway and see if it simplifies to a bagged tree. First, let’s read in the data and create our testing set. We’ll only sample 5% of the data so things run more quickly. state_tests &lt;- read_csv(&quot;https://github.com/uo-datasci-specialization/c4-ml-fall-2020/raw/master/data/train.csv&quot;) %&gt;% slice_sample(prop = 0.05) %&gt;% select(-classification) splt_reg &lt;- initial_split(state_tests) train_reg &lt;- training(splt_reg) Now we’ll create a recipe for these data. It is a bit more complicated than the last example because the data are a fair amount more complicated. The recipe looks like this: rec_reg &lt;- recipe(score ~ ., data = train_reg) %&gt;% step_mutate(tst_dt = lubridate::mdy_hms(tst_dt), time_index = as.numeric(tst_dt)) %&gt;% update_role(tst_dt, new_role = &quot;time_index&quot;) %&gt;% update_role(contains(&quot;id&quot;), ncessch, new_role = &quot;id vars&quot;) %&gt;% step_novel(all_nominal()) %&gt;% step_unknown(all_nominal()) %&gt;% step_rollimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;% step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;% # neccessary when date is NA step_zv(all_predictors()) The recipe above does the following Defines score as the outcome and all other variables as predictors Changes the tst_dt column (date the assessment was taken) to be a date (rather than character) and creates a new column that is a numeric version of the date Changes the role of tst_dt from a predictor to a time_index, which is a special role that can be used to calculate date windows Changes the role of all ID variables to an arbitrary \"id vars\" role, just so they are not used as predictors in the model Recodes nominal columns such that previously unencountered levels of the variable will be recoded to a \"new\" level Imputes and \"unknown\" category for all nominal data Uses a rolling time window imputation (median for the closes 5 data points) to impute all numeric columns In cases where the time window is missing, imputes with the median of the column for all numeric columns Removes zero variance predictors Next, we want to process the data using this recipe. Note that after we bake() the data, we remove those variables that are not predictors. Note that it might be worth considering keeping a categorical version of the school ID in the model (given that the school in which a student is enrolled is likely related to their score), but to keep things simple we’ll just remove all ID variables for now. processed_reg &lt;- rec_reg %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% select(-contains(&quot;id&quot;), -ncessch, -tst_dt) processed_reg ## # A tibble: 7,104 x 32 ## gndr ethnic_cd enrl_grd tst_bnch migrant_ed_fg ind_ed_fg ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 M W 3 1B N N ## 2 M W 8 3B N N ## 3 M W 3 1B N N ## 4 F H 3 1B N N ## 5 F H 5 2B N N ## 6 F W 4 G4 N N ## 7 F W 7 G7 N N ## 8 M W 3 1B N N ## 9 M H 8 3B N N ## 10 M W 4 G4 N N ## # … with 7,094 more rows, and 26 more variables: ## # sp_ed_fg &lt;fct&gt;, tag_ed_fg &lt;fct&gt;, econ_dsvntg &lt;fct&gt;, ## # ayp_lep &lt;fct&gt;, stay_in_dist &lt;fct&gt;, stay_in_schl &lt;fct&gt;, ## # dist_sped &lt;fct&gt;, trgt_assist_fg &lt;fct&gt;, ## # ayp_dist_partic &lt;fct&gt;, ayp_schl_partic &lt;fct&gt;, ## # ayp_dist_prfrm &lt;fct&gt;, ayp_schl_prfrm &lt;fct&gt;, ## # rc_dist_partic &lt;fct&gt;, rc_schl_partic &lt;fct&gt;, ## # rc_dist_prfrm &lt;fct&gt;, rc_schl_prfrm &lt;fct&gt;, lang_cd &lt;fct&gt;, ## # tst_atmpt_fg &lt;fct&gt;, grp_rpt_dist_partic &lt;fct&gt;, ## # grp_rpt_schl_partic &lt;fct&gt;, grp_rpt_dist_prfrm &lt;fct&gt;, ## # grp_rpt_schl_prfrm &lt;fct&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, ## # score &lt;dbl&gt;, time_index &lt;dbl&gt; We’ll now use the processed data to fit a random forest, evaluating our model using the OOB samples. Let’s start by writing a function that fits the model for a given set of hyperparameters and returns a data frame with the hyperparameter values, our performance metric (RMSE) and the model object. rf_fit_reg &lt;- function(tree_n, mtry, min_n) { rf_mod_reg &lt;- rand_forest() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = mtry, min_n = min_n, trees = tree_n) rf_fit &lt;- fit(rf_mod_reg, formula = score ~ ., data = processed_reg) # output RMSE tibble(rmse = sqrt(rf_fit$fit$prediction.error)) %&gt;% mutate(trees = tree_n, mtry = mtry, min_n = min_n, model = list(rf_fit)) } Notice in the above that I’ve used sqrt(rf_fit$fit$prediction.error) for the root mean square error, rather than using the model predictions with a {yardstick} function. This is because the default OOB error for a regression models with {ranger} is the mean square error, so we don’t have to do any predictions on our own - we just need to take the square root of this value. Most of the rest of this function is the same as before. Let’s now conduct our tuning. First, let’s check how many predictors we have: ncol(processed_reg) - 1 ## [1] 31 Remember, for regression problems, I good place to start is around \\(m/3\\), or 10.33. Let’s make a grid of mtry values, centered around 10.33. For min_n, we’ll use the same values we did before: 2, 5, 10, 15, 20, and 25. We’ll then evaluate the OOB RMSE across these different values and see if we need to conduct any more hyperparameter tuning. grid_reg &lt;- expand.grid(mtry = 5:15, min_n = c(2, seq(5, 25, 5))) And now we’ll use our function from above to fit each of these models, evaluating each with the OOB RMSE. We’ll start out with 1000 trees, then inspect that with our finalized model to make sure it is sufficient. Note that, even with using the OOB samples for our tuning, the following code takes a decent amount of time to run. tic() rf_reg_fits &lt;- map2_df(grid_reg$mtry, grid_reg$min_n, ~rf_fit_reg(1000, .x, .y)) toc() ## 671.409 sec elapsed rf_reg_fits %&gt;% arrange(rmse) ## # A tibble: 66 x 5 ## rmse trees mtry min_n model ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;list&gt; ## 1 87.8 1000 7 20 &lt;fit[+]&gt; ## 2 87.9 1000 6 15 &lt;fit[+]&gt; ## 3 87.9 1000 7 25 &lt;fit[+]&gt; ## 4 87.9 1000 6 25 &lt;fit[+]&gt; ## 5 87.9 1000 7 15 &lt;fit[+]&gt; ## 6 87.9 1000 8 25 &lt;fit[+]&gt; ## 7 87.9 1000 6 20 &lt;fit[+]&gt; ## 8 87.9 1000 6 10 &lt;fit[+]&gt; ## 9 88.0 1000 7 10 &lt;fit[+]&gt; ## 10 88.0 1000 6 5 &lt;fit[+]&gt; ## # … with 56 more rows Let’s evaluate the hyperparameters we search by plotting the mtry values against the RMSE, faceting by min_n. ggplot(rf_reg_fits, aes(mtry, rmse)) + geom_line() + facet_wrap(~min_n) It looks like values of 7 or 8 are optimal for mtry, and our min_n is likely somewhere north of 20. Let’s see if we can improve performance more by tuning a bit more on min_n. We’ll evaluate values from 21 to 31 in increments of 2, while using both mtry values of 7 and 8. grid_reg2 &lt;- expand.grid(mtry = c(7, 8), min_n = seq(21, 31, 2)) tic() rf_reg_fits2 &lt;- map2_df(grid_reg2$mtry, grid_reg2$min_n, ~rf_fit_reg(1000, .x, .y)) toc() ## 88.753 sec elapsed rf_reg_fits2 &lt;- rf_reg_fits2 %&gt;% arrange(rmse) rf_reg_fits2 ## # A tibble: 12 x 5 ## rmse trees mtry min_n model ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 87.8 1000 7 23 &lt;fit[+]&gt; ## 2 87.9 1000 7 31 &lt;fit[+]&gt; ## 3 87.9 1000 7 21 &lt;fit[+]&gt; ## 4 87.9 1000 7 27 &lt;fit[+]&gt; ## 5 87.9 1000 7 29 &lt;fit[+]&gt; ## 6 87.9 1000 7 25 &lt;fit[+]&gt; ## 7 87.9 1000 8 29 &lt;fit[+]&gt; ## 8 87.9 1000 8 25 &lt;fit[+]&gt; ## 9 87.9 1000 8 23 &lt;fit[+]&gt; ## 10 87.9 1000 8 31 &lt;fit[+]&gt; ## 11 88.0 1000 8 21 &lt;fit[+]&gt; ## 12 88.0 1000 8 27 &lt;fit[+]&gt; Notice the RMSE is essentially equivalent for all models listed above. We’ll go forward with the first model, but any of these combinations of hyperparameters likely provide similar out-of-sample predictive accuracy (according to RMSE). Finally, before moving on to evaluating our model against the test set, we would likely want to make sure that we fit the model with a sufficiently large number of trees. Let’s investigate with a model that had the lowest RMSE. We’ll use trees from 500 to 1500 by increments of 100. tic() rf_reg_ntrees &lt;- map_df(seq(500, 1500, 100), ~rf_fit_reg(.x, mtry = rf_reg_fits2$mtry[1], min_n = rf_reg_fits2$min_n[1]) ) toc() ## 77.671 sec elapsed And now we’ll plot the number of trees against the RMSE, looking for a point of stability. ggplot(rf_reg_ntrees, aes(trees, rmse)) + geom_line() Hmm… that doesn’t look terrifically stable. BUT WAIT! Notice the bottom to the top of the y-axis is only about one-tenth of a point difference. So really, this is not a problem with instability, but that it is actually stable across all these values. Just to prove this to ourselves, let’s look at the same plot, but making the y-axis span one point (which is still not very much), going from 87 to 88. ggplot(rf_reg_ntrees, aes(trees, rmse)) + geom_line() + ylim(87, 88) And now it looks more stable. So even with 500 trees we likely would have been fine. From here, we could continue on to evaluate our final model against the test set using the last_fit() function, but we will leave that as an exercise for the reader. Note that we also only worked with 5% of the total data. The hyperparameters that we settled on may be different if we used more of the data. We could also likely improve performance more by including additional information in the model - e.g., information on staffing and funding, the size of the school or district, indicators of the economic and demographic makeup of the surrounding area, or practices related to school functioning (e.g., use of suspension/expulsion). "],["feature-and-model-interpretation.html", "5.3 Feature and model interpretation", " 5.3 Feature and model interpretation A considerable benefit of decision tree model is that they are relatively easy to understand, in terms of how predictions are made. It’s perhaps less clear why specific splits have been made, but it’s relatively straightforward to communicate with stakeholders how predictions are made. This is because the tree itself can be followed, like a road map, to the terminal node. The tree is just a series of if/then statements. Unfortunately, this ease of interpretation all goes out the window with bagged trees and random forests. Instead of just building one decision tree, we are building hundreds or even thousands, with each tree (or at least most trees) being at least slightly different. So how do we communicate the results of our model with stakeholders? How can we effectively convey how our model makes predictions? Perhaps the most straightforward way to communicate complex models with stakeholders is to focus on feature importance. That is, which features in the model are most important in establishing the prediction. We can do this with {ranger} models using the {vip} package (vip stands for variable importance plots). In a standard decision, a variable is selected at a given node if it improves the objective function score. The relative importance of a given variable is then determined by the sum of the squared improvements (see {vip} documentation). This basic idea is extended to ensembles of trees, like bagged trees and random forest, by computing the mean of these importance scores across all trees in the ensemble. To obtain variable importance scores, we first have to re-run our {ranger} model, requesting it compute variable importance metrics. We do this by specifying importance = \"impurity\" (which is the Gini index for classification problems). # specify the model and request a variable importance metric rf_mod_reg_final &lt;- rand_forest() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = rf_reg_fits2$mtry[1], min_n = rf_reg_fits2$min_n[1], trees = 1000, importance = &quot;impurity&quot;) # fit the model rf_fit_reg_final &lt;- fit(rf_mod_reg_final, score ~ ., processed_reg) And now we can request variable importance scores with vip::vi(), or a variable importance plot with vip::vip(). Let’s first look at the scores. library(vip) vi(rf_fit_reg_final$fit) ## # A tibble: 31 x 2 ## Variable Importance ## &lt;chr&gt; &lt;dbl&gt; ## 1 enrl_grd 7574484. ## 2 tag_ed_fg 6912953. ## 3 econ_dsvntg 6686036. ## 4 sp_ed_fg 5728096. ## 5 lon 4258608. ## 6 lat 4057123. ## 7 time_index 3781738. ## 8 tst_bnch 3715267. ## 9 ayp_lep 3672420. ## 10 ethnic_cd 2556395. ## # … with 21 more rows The results of this investigation are not entirely surprising. The enrl_grd the student is in is the most important predictor of their score. Following grade level, the students’ economic disadvantaged status is the most predictive feature in the model, following a long history of evidence documenting differential achievement by socioeconomic status. Students classified as talented and gifted (TAG), and those who received special education services are the next two most predictive variables, followed by the physical location of the school (lat = latitude and lon = longitude). Each of these variables would be among those that we might have guessed, a priori, would be the most predictive. Let’s look at a plot showing the relative importance of the features in our model. vip(rf_fit_reg_final$fit) Our inferences here are similar, but this view helps us see more clearly that the first three variables are similarly important, then there is a bit of a dip for with special education status, followed by a sizeable dip for latitude and longitude. The tst_bnch feature is essentially a categorical version of grade level (so we are modeling it as both a continuous and categorical feature; the merits of such an approach could be argued, but we see that we are getting improvements from both versions). With bagged trees and random forests, we can also look at variable importance in a slightly different way. Rather than summing the contributions for each tree, and taking the average across trees, we can compute the OOB error for a given tree, then shuffle the values for a given variable and re-compute the OOB error. If the variable is important, the OOB error will increase as we perturb the given variable, but otherwise the error will stay (approximately) the same (see Wright, Ziegler, and König, 2016). To get this purmutation-based importance measure, we just change importance to \"purmutation\". Let’s do this and see if the resulting plot differs. rf_mod_reg_final2 &lt;- rand_forest() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_args(mtry = rf_reg_fits2$mtry[1], min_n = rf_reg_fits2$min_n[1], trees = 1000, importance = &quot;purmutation&quot;) rf_fit_reg_final2 &lt;- fit(rf_mod_reg_final, score ~ ., processed_reg) vip(rf_fit_reg_final2$fit) As we can see, there are a few small differences, but generally the results agree, providing us with greater confidence in the ordering variable importance. If the results did not generally align that may be evidence that our model is unstable and we would likely want to probe our model a bit more. Inspecting variable importance helps us understand which variables contribute to our model predictions, but not necessarily how they relate to the outcome. For that, we can look at partial dependency plots via the {pdp} package (which is developed by the same people who created {vip}). Partial dependency plots show the marginal effect of a feature on the outcome. This can help us to understand the directionality of the effect and whether it is generally linear or not. For example, we would probably expect that students’ scores would increase roughly linearly and monotonically with enrl_grd. To see if that’s the case we case we use the pdp::partial() function. library(pdp) partial(rf_fit_reg_final$fit, pred.var = &quot;enrl_grd&quot;, train = processed_reg, plot = TRUE, plot.engine = &quot;ggplot2&quot;) And we can see that, yes, the relation is roughly linear and monotonically increasing. What if we want to explore more than one variable? Let’s look at the geographic location of the school. partial(rf_fit_reg_final$fit, pred.var = c(&quot;lon&quot;, &quot;lat&quot;), train = processed_reg, plot = TRUE, plot.engine = &quot;ggplot2&quot;) Because this is geographic data, and we know the data come from Oregon (although they are simulated) we can interpret this as if we were looking at a physical map. One notable aspect is that there is a fairly clear band running north/south in the western part of the state. This is roughly where I-5 runs and where many of the more populous towns/cities are located, including Ashland/Medford in the south, Eugene around the center, and Portland in the north. We could even overlay an image of Oregon here, which actually helps us with interpretation a bit more. Note that, when the PDP is created, it just create a grid for prediction. So when we overlay the map of Oregon, we see some areas where the prediction extends outside of the state, which are not particularly trustworthy. Finally, we might also be interested in the individual variability of a feature. Let’s again look at the enrolled grade of students. In this case, we’ll create individual conditional expectation plots, or ICE curves, which are the equivalent to PDP’s but for individual observations. The {pdp} package can again create these for us. The process for creating them is essentially equivalent, but we provide one additional argument, ice = TRUE. Note that we can use plot = TRUE here too but I’ve just output the values to have a bit more control of how the plot renders. ice_grade &lt;- partial(rf_fit_reg_final$fit, pred.var = &quot;enrl_grd&quot;, train = processed_reg, ice = TRUE) ggplot(ice_grade, aes(enrl_grd, yhat)) + geom_line(aes(group = yhat.id), color = &quot;gray40&quot;, size = 0.1, alpha = 0.1) + stat_summary(geom = &quot;line&quot;, fun = &quot;mean&quot;) And as we would likely expect, there is a lot of variation here in expected scores across grades. Bagged trees can often lead to better predictions than a single decision tree by creating ensembles of trees based on bootstrap resamples of the data and aggregating the results across all trees. Random forests extend this framework by randomly sample \\(m\\) columns at each split of each tree that is grown in the ensemble (or forest) which can help decorrelate the trees and, often, lead to better overall predictions. Unfortunately creating an ensemble of trees also makes feature and model interpretation a bit more difficult. Tools like variable importance and partial dependence plots can be an efficient means of communicating how and why a model is making the predictions it is, while still maintaining strong overall model performance. For more information on these and other methods, see Molnar, 2020. "],["introduction-to-r.html", "6 Introduction to R", " 6 Introduction to R ##Transitioning to R Moving to R from other statistical software generally requires a fundamental shift in the way we think about and interact with data. Aside from this shift in thinking, there is also a substantial amount of code to learn, which can be both frustrating and intimidating. The primary goal for this chapter is to make this shift less intimidating and the learning curve less steep. We will focus primarily on three components: data processing (munging/manipulating/wrangling), data visualization, and reproducible workflows. We will be oriented around the philosophy of tidy data and, as such, primarily rely on tools within the tidyverse for manipulating and visualizing data. The tidyverse is a suite of packages developed by RStudio, generally led by Hadley Wickham, which are all optimized for tidy data. The focus of this course is on working with R, as opposed to the specifics of any given analysis. Statistical models will be used for illustrative examples throughout the chapter, but high-level statistical knowledge (e.g., multilevel modeling or structural equation modeling) is not a prerequisite. ##Installing R The installation process is straight forward. Download R and RStudio (Windows, Linux and Mac OS X), run the files and follow the instructions to install them (install R first and RStudio second). Once both installed, open RStudio, go to File &gt; New File &gt; R Markdown to create a new RMarkdown. There are many other types of files but we will mainly be using RMarkdown in this book. I will give you few minutes to complete these steps. If you get any errors or warnings, please look them up. Looking up errors is 90% of coding especially in the early stages of learning, the percentage starts decreasing as you become more fluent in the coding language. In looking up errors, I mostly find the solution in Stackoverflow. All done? Perfect! Let’s talk about the R Markdown interface. It is divided into 4 sections: (1) upper left: the editor where you write your code, (2) upper right: the environment history where you will see you variables, and other details about your data and history of your edits, (3) lower left: the console where you see the execution of your code, and (4) lower right: the window that shows your files (databases and code files), the plots you generated, the packages you are using, a Help tab if needed (we will discuss later how to use help), and a Viewer tab. The editor section is the one you will use the most. ##Programing in R OK, let’s get started! First you need to import your data into R using the import function from the rio package, so make sure you install rio and call the library rio. Wait! What does all this mean? Great question. Before we get to how you import data, let’s clarify some key concepts in order to speak the same language. Functions are a simple If Then code. When you set your alarm, you are basically using the function: If DD/MM/YYYY HH:MM:SS = X Then Play Song Y. Packages are primarily a collections of R functions. In addition to functions, packages can also contain data and some other details, all compiled in a well-defined format, created to add specific functionality. There are 10,000+ user contributed packages and growing. Library is the directory where the packages are installed. Does that mean installing R is nothing without its libraries and packages? Not really, there are a set of standard (or base) packages which are considered part of the R source code and automatically available as part of your R installation. Think of R as a scientific calculator, the base functions as the +, - operations, the base packages as the mean or standard deviation calculations, the additional packages as installed programs to conduct specific regressions like fixed effects or a Poisson model. One other important about libraries is the difference between installing and loading libraries. Think of a library as book. Installing it is like buying a book and putting it on your shelf. Each time you want to reach the book you still need to get the book off the shelf, that’s what calling a library is. Let’s install then call our first library rio. This is often the library used for data import and export. install.packages(&quot;rio&quot;) #Installing rio. You only need to do this step once ## Installing rio [0.5.16] ... ## OK [linked cache] library(rio) #Calling rio. You need to do this each time you create a new R Markdown file. In writing code, you would probably have your first code chunk that calls all the libraries you will be using and install the ones that are not installed yet. I realize we did not talk about code chunks. It is basically the part of editor section that contains your code. All code chunks start and end with ``` – three backticks or graves. But you can also just insert it clicking on the Insert button you see in the middle of your editor. Finally, we will now import data. There are many ways you can this, but I personally prefer importing from the same folder what my R Markdown is saved. This helps me avoid any confusion with working directories. I usually never run into problems using this method. Basically, for every project create a folder, put all your data in that folder, and save your .rmd file in it (.rmd is the extension of an R Markdown file). If you this you should be able to see your datasets if you click on Files on the lower right section of the interface. Using this method, all you need to do when importing or reading your data files is to write the data name. Here, I am importing an spss dataset (.sav) called ‘sesame13’. I then store the data imported in an object called sesame13. sesame13 &lt;- import(&#39;data/sesame13.sav&#39;) The next thing we want to do is take a look at the data but without loading all of it. We use the function head to look at the first 6 lines. You can also use tail to look at the bottom 6 lines. head(sesame13) ## id site sex age viewing setting treatmen prebody prelet ## 1 1 1 1 66 1 1 1 16 23 ## 2 2 1 2 67 3 1 1 30 26 ## 3 3 1 1 56 3 1 0 22 14 ## 4 4 1 1 49 1 1 0 23 11 ## 5 5 1 1 69 4 1 0 32 47 ## 6 6 1 2 54 3 1 0 29 26 ## preform prenumb prerelat preclass postbody postlet postform ## 1 12 40 14 20 18 30 14 ## 2 9 39 16 22 30 37 17 ## 3 9 9 9 8 21 46 15 ## 4 10 14 9 13 21 14 13 ## 5 15 51 17 22 32 53 18 ## 6 10 33 14 14 27 36 14 ## postnumb postrel postclas ppvt ## 1 44 14 23 62 ## 2 39 14 22 80 ## 3 40 9 19 32 ## 4 19 8 15 27 ## 5 54 14 21 71 ## 6 39 16 24 32 #tail (sesame13) #You can use the symbol # to comment your code to avoid executing it. You can also use it to add comments and notes for you and your collaborators. ##Data cleaning and processing We will now move to cleaning our data starting with variable names. Often variable names have capital letters or spaces which are not ideal for our limited cognitive ability so it is easier to make all variables lower capital and no spaces. That’s what clean_names() does. This function is part of the janitor library so we will need to call it. library(janitor) sesame13 &lt;- clean_names(sesame13) "]]
