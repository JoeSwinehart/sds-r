<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 Random Forests | Social Data Science with R</title>
  <meta name="description" content="This is basically course notes corresponding to a series of courses in educational data science, which are generally applicable to a wide range of social data science problems, taught through R." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 Random Forests | Social Data Science with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is basically course notes corresponding to a series of courses in educational data science, which are generally applicable to a wide range of social data science problems, taught through R." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 Random Forests | Social Data Science with R" />
  
  <meta name="twitter:description" content="This is basically course notes corresponding to a series of courses in educational data science, which are generally applicable to a wide range of social data science problems, taught through R." />
  

<meta name="author" content="Daniel Anderson" />
<meta name="author" content="Brendan Cullen" />
<meta name="author" content="Ouafaa Hmaddi" />


<meta name="date" content="2020-11-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bagged-trees.html"/>
<link rel="next" href="feature-and-model-interpretation.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="assets/core-js-2.5.3/shim.min.js"></script>
<script src="assets/react-16.12.0/react.min.js"></script>
<script src="assets/react-16.12.0/react-dom.min.js"></script>
<script src="assets/reactwidget-1.0.0/react-tools.js"></script>
<script src="assets/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="assets/reactable-binding-0.2.0/reactable.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/iframe-resizer/3.5.16/iframeResizer.min.js" type="text/javascript"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="welcome.html"><a href="welcome.html"><i class="fa fa-check"></i><b>2</b> Welcome</a></li>
<li class="chapter" data-level="3" data-path="feature-engineering.html"><a href="feature-engineering.html"><i class="fa fa-check"></i><b>3</b> Feature Engineering</a><ul>
<li class="chapter" data-level="3.1" data-path="basics-of-recipes.html"><a href="basics-of-recipes.html"><i class="fa fa-check"></i><b>3.1</b> Basics of {recipes}</a></li>
<li class="chapter" data-level="3.2" data-path="creating-a-recipe.html"><a href="creating-a-recipe.html"><i class="fa fa-check"></i><b>3.2</b> Creating a recipe</a><ul>
<li class="chapter" data-level="3.2.1" data-path="creating-a-recipe.html"><a href="creating-a-recipe.html#order-matters"><i class="fa fa-check"></i><b>3.2.1</b> Order matters</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="encoding-categorical-data.html"><a href="encoding-categorical-data.html"><i class="fa fa-check"></i><b>3.3</b> Encoding categorical data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="encoding-categorical-data.html"><a href="encoding-categorical-data.html#transformations-beyond-dummy-coding"><i class="fa fa-check"></i><b>3.3.1</b> Transformations beyond dummy coding</a></li>
<li class="chapter" data-level="3.3.2" data-path="encoding-categorical-data.html"><a href="encoding-categorical-data.html#handling-new-levels"><i class="fa fa-check"></i><b>3.3.2</b> Handling new levels</a></li>
<li class="chapter" data-level="3.3.3" data-path="encoding-categorical-data.html"><a href="encoding-categorical-data.html#final-thoughts-on-encoding-categorical-data"><i class="fa fa-check"></i><b>3.3.3</b> Final thoughts on encoding categorical data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="dealing-with-low-variance-predictors.html"><a href="dealing-with-low-variance-predictors.html"><i class="fa fa-check"></i><b>3.4</b> Dealing with low variance predictors</a></li>
<li class="chapter" data-level="3.5" data-path="missing-data.html"><a href="missing-data.html"><i class="fa fa-check"></i><b>3.5</b> Missing data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="missing-data.html"><a href="missing-data.html#missing-data-via-recipes"><i class="fa fa-check"></i><b>3.5.1</b> Missing data via {recipes}</a></li>
<li class="chapter" data-level="3.5.2" data-path="missing-data.html"><a href="missing-data.html#a-few-words-of-caution"><i class="fa fa-check"></i><b>3.5.2</b> A few words of caution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>3.6</b> Transformations</a><ul>
<li class="chapter" data-level="3.6.1" data-path="transformations.html"><a href="transformations.html#box-cox-and-similar-transformations"><i class="fa fa-check"></i><b>3.6.1</b> Box-Cox and similar transformations</a></li>
<li class="chapter" data-level="3.6.2" data-path="transformations.html"><a href="transformations.html#an-applied-example"><i class="fa fa-check"></i><b>3.6.2</b> An applied example</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="nonlinearity.html"><a href="nonlinearity.html"><i class="fa fa-check"></i><b>3.7</b> Nonlinearity</a><ul>
<li class="chapter" data-level="3.7.1" data-path="nonlinearity.html"><a href="nonlinearity.html#polynomial-transformations"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial transformations</a></li>
<li class="chapter" data-level="3.7.2" data-path="nonlinearity.html"><a href="nonlinearity.html#splines"><i class="fa fa-check"></i><b>3.7.2</b> Splines</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="interactions.html"><a href="interactions.html"><i class="fa fa-check"></i><b>3.8</b> Interactions</a><ul>
<li class="chapter" data-level="3.8.1" data-path="interactions.html"><a href="interactions.html#creating-interactions-by-hand"><i class="fa fa-check"></i><b>3.8.1</b> Creating interactions “by hand”</a></li>
<li class="chapter" data-level="3.8.2" data-path="interactions.html"><a href="interactions.html#creating-interactions-with-recipes"><i class="fa fa-check"></i><b>3.8.2</b> Creating interactions with {recipes}</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>3.9</b> PCA</a><ul>
<li class="chapter" data-level="3.9.1" data-path="pca.html"><a href="pca.html#pca-with-recipes"><i class="fa fa-check"></i><b>3.9.1</b> PCA with {recipes}</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="wrapping-up.html"><a href="wrapping-up.html"><i class="fa fa-check"></i><b>3.10</b> Wrapping up</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.0.1" data-path="decision-trees.html"><a href="decision-trees.html#a-simple-decision-tree"><i class="fa fa-check"></i><b>4.0.1</b> A simple decision tree</a></li>
<li class="chapter" data-level="4.1" data-path="determining-optimal-splits.html"><a href="determining-optimal-splits.html"><i class="fa fa-check"></i><b>4.1</b> Determining optimal splits</a></li>
<li class="chapter" data-level="4.2" data-path="visualizing-decision-trees.html"><a href="visualizing-decision-trees.html"><i class="fa fa-check"></i><b>4.2</b> Visualizing decision trees</a></li>
<li class="chapter" data-level="4.3" data-path="fitting-a-decision-tree.html"><a href="fitting-a-decision-tree.html"><i class="fa fa-check"></i><b>4.3</b> Fitting a decision tree</a><ul>
<li class="chapter" data-level="4.3.1" data-path="fitting-a-decision-tree.html"><a href="fitting-a-decision-tree.html#load-the-data"><i class="fa fa-check"></i><b>4.3.1</b> Load the data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tuning-decision-trees.html"><a href="tuning-decision-trees.html"><i class="fa fa-check"></i><b>4.4</b> Tuning decision trees</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tuning-decision-trees.html"><a href="tuning-decision-trees.html#decision-tree-hyperparamters"><i class="fa fa-check"></i><b>4.4.1</b> Decision tree hyperparamters</a></li>
<li class="chapter" data-level="4.4.2" data-path="tuning-decision-trees.html"><a href="tuning-decision-trees.html#conducting-the-grid-search"><i class="fa fa-check"></i><b>4.4.2</b> Conducting the grid search</a></li>
<li class="chapter" data-level="4.4.3" data-path="tuning-decision-trees.html"><a href="tuning-decision-trees.html#finalizing-our-model-fit"><i class="fa fa-check"></i><b>4.4.3</b> Finalizing our model fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html"><i class="fa fa-check"></i><b>5</b> Bagging and Random Forests</a><ul>
<li class="chapter" data-level="5.0.1" data-path="bagging-and-random-forests.html"><a href="bagging-and-random-forests.html#bagging-by-hand"><i class="fa fa-check"></i><b>5.0.1</b> Bagging “by hand”</a></li>
<li class="chapter" data-level="5.1" data-path="bagged-trees.html"><a href="bagged-trees.html"><i class="fa fa-check"></i><b>5.1</b> Bagged trees</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bagged-trees.html"><a href="bagged-trees.html#working-with-out-of-bag-samples"><i class="fa fa-check"></i><b>5.1.1</b> Working with out of bag samples</a></li>
<li class="chapter" data-level="5.1.2" data-path="bagged-trees.html"><a href="bagged-trees.html#tuning-with-oob-samples"><i class="fa fa-check"></i><b>5.1.2</b> Tuning with OOB samples</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>5.2</b> Random Forests</a><ul>
<li class="chapter" data-level="5.2.1" data-path="random-forests.html"><a href="random-forests.html#fitting-random-forests"><i class="fa fa-check"></i><b>5.2.1</b> Fitting random forests</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="feature-and-model-interpretation.html"><a href="feature-and-model-interpretation.html"><i class="fa fa-check"></i><b>5.3</b> Feature and model interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>6</b> Introduction to R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Social Data Science with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">5.2</span> Random Forests</h2>
<p>One potential problem with bagged trees is that the predictions across trees often correlate strongly. This is because, even if a new bootstrap sample is used to create the tree, all of the features are used in every tree. Thus, if a given feature is strongly predictive of the outcome, it is likely to be used as the root node in nearly every tree. Because decision trees are built recursively, with optimal splits only determined after previous splits have already been determined, this means that we may be missing out on potential predictive accuracy by not allowing other features to “start” the tree (be the root node). This does not mean that bagging is not useful. As we just saw, we were able to have fairly substantial gains in model performance using bagging when compared to what we observed with a single tree. Yet, if we could decorrelate the trees, we might be able to improve performance even more.</p>
<p>Random forests extend bagged trees by introducing a stochastic component to the features. Specifically, rather than building each tree with all of the features, each tree is built using a random sample of features <em>at each split</em>. The predictive performance of any individual tree is then unlikely to be as performant as a tree using all the features at each split, but the forest of trees (aggregate prediction) can regularly provide better predictions than any single tree.</p>
<p>Assume we have a features with one <em>very</em> strong predictor and a few features that are moderately strong predictors. If we used bagging, the moderately strong predictors would likely be internal nodes for nearly every tree–meaning their importance would be conditional on the strong predictor. Yet, these variables might provide important information for specific samples on their own. If we remove the very strong predictor for the root node split, that tree we be built with one of the moderately strong predictors at the root node. The very strong predictor may then be selected in one of the internal nodes and, while this model may not be as predictive <em>overall</em>, it may result in better predictions for specific cases. If we average over all these diverse models we can often get better predictions for the entire dataset.</p>
<p>In essence, a random forest is just a bagged tree with a stochastic component introduced to decorrelate the trees. This means each individual model is <em>more</em> different than a bagged tree. When we average across all the trees we reduce this variance and <em>may</em> be able to get overall improved performance when compared to a single tree or a bagged tree.</p>
<div class="lightbulb">
<p>Random forests work like bagged trees, but include a random selection of features at each split. This helps decorrelate the trees and can help get at the unique features of the data.</p>
</div>
<p>Random forest models tend to provide very good “out of the box” model performance. Because bagged trees are just a special case of random forests, and the number of predictors to select for each tree is a hyperparameter that can be tuned, it’s generally worth starting with a random forest and only moving to a bagged tree if, for your specific situation, using all the predictors for each tree works better than using a sample.</p>
<div id="fitting-random-forests" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Fitting random forests</h3>
<p>In the previous section we used the <code>rand_forest()</code> function to fit bagged classification trees and obtain performance metrics on the OOB samples. We did this by setting <code>mtry = p</code>, where <code>p</code> is equal to the number of predictor variables. When fitting a random forest, we just change <code>mtry</code> to a smaller value, which represents the number of features to randomly select from at each split. Everything else is the same: <code>trees</code> represents the number of bags, or the number of tree in the forest, while <code>min_n</code> represents the minimum sample size for a terminal node (i.e., limiting the depth to which each tree is grown).</p>
<!-- Might need cites for the below  -->
<p>A good place to start is <code>mtry = p/3</code> for regression problems and <code>mtry = sqrt(p)</code> for classification problems. Generally, higher values of <code>mtry</code> will work better when the data are fairly noisy and the predictors are not overly strong. Lower values of <code>mtry</code> will work better when there are a few very strong predictors.</p>
<p>Just like bagged trees, we need to have a sufficient number of trees that the performance estimate stabilizes. Including more trees will not hurt your model performance, but it will increase computation time. Generally, random forests will need <em>more</em> trees to stabilize than bagged trees, because each tree is “noisier” than those in a bagged tree. A good place to start is at about 1,000 trees, but if your learning curve suggests the model is still trending toward better performance (rather than stabilizing/flat lining) then you should add more trees. Note that the number of trees you need will also depend on other hyperparameters. Lower values of <code>mtry</code> and <code>min_n</code> will likely lead to needing a greater number of trees.</p>
<p>Because we’re using bagging in a random forest we can again choose whether to use <span class="math inline">\(k\)</span>-fold cross validation or just evaluate performance based on the OOB samples. If we were in a situation where we wanted to be <em>extra sure</em> we had conducted the hyperparameter tuning correctly, we might start by building a model on the OOB metrics, then evaluate a small candidate of hyperparameters with <span class="math inline">\(k\)</span>-fold CV to finalize them. In this case we will <em>only</em> use the OOB metrics to save on computation time.</p>
<div id="a-classification-example" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> A classification example</h4>
<p>Let’s continue with our classification example, using the same training data and recipe we used in the previous section. As a reminder, the recipe looked like this:</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="random-forests.html#cb273-1"></a>rec &lt;-<span class="st"> </span><span class="kw">recipe</span>(accuracy_group <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train) </span></code></pre></div>
<p>And we prepared the data for analysis so it looked like this:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="random-forests.html#cb274-1"></a>processed_train &lt;-<span class="st"> </span>rec <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb274-2"><a href="random-forests.html#cb274-2"></a><span class="st">  </span><span class="kw">prep</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb274-3"><a href="random-forests.html#cb274-3"></a><span class="st">  </span><span class="kw">bake</span>(<span class="dt">new_data =</span> <span class="ot">NULL</span>)</span>
<span id="cb274-4"><a href="random-forests.html#cb274-4"></a></span>
<span id="cb274-5"><a href="random-forests.html#cb274-5"></a>processed_train</span></code></pre></div>
<pre><code>## # A tibble: 2,767 x 6
##    event_count event_code game_time title     world accuracy_group
##          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;ord&gt;         
##  1           1       2000         0 Mushroom… TREE… 3             
##  2           3       3010        37 Mushroom… TREE… 3             
##  3           5       3010      3901 Mushroom… TREE… 3             
##  4          10       4025      8400 Mushroom… TREE… 3             
##  5          12       3121      8926 Mushroom… TREE… 3             
##  6          13       4025      9502 Mushroom… TREE… 3             
##  7          16       3121     10210 Mushroom… TREE… 3             
##  8          17       2035     10210 Mushroom… TREE… 3             
##  9          18       2020     10210 Mushroom… TREE… 3             
## 10          20       4070     10543 Mushroom… TREE… 3             
## # … with 2,757 more rows</code></pre>
<p>We will again optimize on the model AUC. Recall that we had 5 predictor variables. Let’s start by fitting a random forest with 1,000 trees, randomly sampling two predictors at each split (<span class="math inline">\(\sqrt{5}=2.24\)</span>). We’ll set <code>min_n</code> to 2 again so we are starting our modeling process by growing very deep trees</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="random-forests.html#cb276-1"></a>rf_mod1 &lt;-<span class="st"> </span><span class="kw">rand_forest</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb276-2"><a href="random-forests.html#cb276-2"></a><span class="st">    </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb276-3"><a href="random-forests.html#cb276-3"></a><span class="st">    </span><span class="kw">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb276-4"><a href="random-forests.html#cb276-4"></a><span class="st">    </span><span class="kw">set_args</span>(<span class="dt">mtry =</span> <span class="dv">2</span>,</span>
<span id="cb276-5"><a href="random-forests.html#cb276-5"></a>             <span class="dt">min_n =</span> <span class="dv">2</span>,</span>
<span id="cb276-6"><a href="random-forests.html#cb276-6"></a>             <span class="dt">trees =</span> <span class="dv">1000</span>)</span>
<span id="cb276-7"><a href="random-forests.html#cb276-7"></a><span class="kw">tic</span>()</span>
<span id="cb276-8"><a href="random-forests.html#cb276-8"></a>rf_fit1 &lt;-<span class="st"> </span><span class="kw">fit</span>(rf_mod1, </span>
<span id="cb276-9"><a href="random-forests.html#cb276-9"></a>               <span class="dt">formula =</span> accuracy_group <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb276-10"><a href="random-forests.html#cb276-10"></a>               <span class="dt">data =</span> processed_train)</span>
<span id="cb276-11"><a href="random-forests.html#cb276-11"></a><span class="kw">toc</span>()</span></code></pre></div>
<pre><code>## 2.527 sec elapsed</code></pre>
<p>Note that even with 1,000 trees, the model fits very quickly. What does our AUC look like for this model?</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="random-forests.html#cb278-1"></a>pred_frame &lt;-<span class="st"> </span>rf_fit1<span class="op">$</span>fit<span class="op">$</span>predictions <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb278-2"><a href="random-forests.html#cb278-2"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb278-3"><a href="random-forests.html#cb278-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">observed =</span> processed_train<span class="op">$</span>accuracy_group)</span>
<span id="cb278-4"><a href="random-forests.html#cb278-4"></a></span>
<span id="cb278-5"><a href="random-forests.html#cb278-5"></a><span class="kw">roc_auc</span>(pred_frame, observed, <span class="st">`</span><span class="dt">0</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">3</span><span class="st">`</span>)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc hand_till      0.848</code></pre>
<p>Looks pretty good! Note that this is quite similar to the best value we estimated using a bagged tree.</p>
<p>But did our model stabilize? Did we use enough trees? Let’s investigate. First, we’ll write a function so we can easily refit our model with any number of trees, and return the OOB AUC estimate.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="random-forests.html#cb280-1"></a>fit_rf &lt;-<span class="st"> </span><span class="cf">function</span>(tree_n, <span class="dt">mtry =</span> <span class="dv">2</span>, <span class="dt">min_n =</span> <span class="dv">2</span>) {</span>
<span id="cb280-2"><a href="random-forests.html#cb280-2"></a>  </span>
<span id="cb280-3"><a href="random-forests.html#cb280-3"></a>  <span class="co"># specify the model</span></span>
<span id="cb280-4"><a href="random-forests.html#cb280-4"></a>  rf_mod &lt;-<span class="st"> </span><span class="kw">rand_forest</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb280-5"><a href="random-forests.html#cb280-5"></a><span class="st">    </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb280-6"><a href="random-forests.html#cb280-6"></a><span class="st">    </span><span class="kw">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb280-7"><a href="random-forests.html#cb280-7"></a><span class="st">    </span><span class="kw">set_args</span>(<span class="dt">mtry =</span> mtry,</span>
<span id="cb280-8"><a href="random-forests.html#cb280-8"></a>             <span class="dt">min_n =</span> min_n,</span>
<span id="cb280-9"><a href="random-forests.html#cb280-9"></a>             <span class="dt">trees =</span> tree_n)</span>
<span id="cb280-10"><a href="random-forests.html#cb280-10"></a>  </span>
<span id="cb280-11"><a href="random-forests.html#cb280-11"></a>  <span class="co"># fit the model</span></span>
<span id="cb280-12"><a href="random-forests.html#cb280-12"></a>  rf_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(rf_mod, </span>
<span id="cb280-13"><a href="random-forests.html#cb280-13"></a>                <span class="dt">formula =</span> accuracy_group <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb280-14"><a href="random-forests.html#cb280-14"></a>                <span class="dt">data =</span> processed_train)</span>
<span id="cb280-15"><a href="random-forests.html#cb280-15"></a></span>
<span id="cb280-16"><a href="random-forests.html#cb280-16"></a>  <span class="co"># Create a data frame from which to make predictions</span></span>
<span id="cb280-17"><a href="random-forests.html#cb280-17"></a>  pred_frame &lt;-<span class="st"> </span>rf_fit<span class="op">$</span>fit<span class="op">$</span>predictions <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb280-18"><a href="random-forests.html#cb280-18"></a><span class="st">    </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb280-19"><a href="random-forests.html#cb280-19"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">observed =</span> processed_train<span class="op">$</span>accuracy_group)</span>
<span id="cb280-20"><a href="random-forests.html#cb280-20"></a></span>
<span id="cb280-21"><a href="random-forests.html#cb280-21"></a>  <span class="co"># Make the predictions, and output other relevant information</span></span>
<span id="cb280-22"><a href="random-forests.html#cb280-22"></a>  <span class="kw">roc_auc</span>(pred_frame, observed, <span class="st">`</span><span class="dt">0</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">3</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb280-23"><a href="random-forests.html#cb280-23"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">trees =</span> tree_n, </span>
<span id="cb280-24"><a href="random-forests.html#cb280-24"></a>           <span class="dt">mtry =</span> mtry, </span>
<span id="cb280-25"><a href="random-forests.html#cb280-25"></a>           <span class="dt">min_n =</span> min_n,</span>
<span id="cb280-26"><a href="random-forests.html#cb280-26"></a>           <span class="dt">model =</span> <span class="kw">list</span>(rf_fit))</span>
<span id="cb280-27"><a href="random-forests.html#cb280-27"></a>}</span></code></pre></div>
<p>Notice in the above we’ve made it a bit more general so we can come back to checking the number of trees with different values of <code>mtry</code> and <code>min_n</code>. Let’s look at values from 100 (which is almost certainly too low) to 1201 by increments of 50 trees. First, we loop through these values using <code>map_df()</code> so they are all bound in a single data frame.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="random-forests.html#cb281-1"></a>test_n_trees &lt;-<span class="st"> </span><span class="kw">map_df</span>(<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">1201</span>, <span class="dv">50</span>), fit_rf)</span></code></pre></div>
<p>Next, we plot the result!</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="random-forests.html#cb282-1"></a><span class="kw">ggplot</span>(test_n_trees, <span class="kw">aes</span>(trees, .estimate)) <span class="op">+</span></span>
<span id="cb282-2"><a href="random-forests.html#cb282-2"></a><span class="st">  </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-186-1.png" width="672" /></p>
<p>And as we can see, we’re actually pretty safe with a much lower value, perhaps even as low as 100. However, because the models run very fast, we won’t worry too about this too much. When we conduct our model tuning, we’ll drop to 500 trees instead of 1000 (still well above what it appears is needed).</p>
<p>Let’s see if we can improve performance by changing the <code>mtry</code> or <code>min_n</code>. Because we only have five predictor variables in this case, we don’t have a huge range to evaluate with <code>mtry</code>. But let’s setup a regular grid looking at values of 2, 3, 4 and 5 for <code>mtry</code>, and <code>min_n</code> values of 2, 5, 10, 15, 20, and 25. We can then use our <code>fit_rf()</code> function again, but this time setting the number of trees and looping through each of our <code>mtry</code> and <code>min_n</code> values.</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="random-forests.html#cb283-1"></a>grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">mtry =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>,</span>
<span id="cb283-2"><a href="random-forests.html#cb283-2"></a>                    <span class="dt">min_n =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">25</span>, <span class="dv">5</span>)))</span>
<span id="cb283-3"><a href="random-forests.html#cb283-3"></a></span>
<span id="cb283-4"><a href="random-forests.html#cb283-4"></a>rf_tune &lt;-<span class="st"> </span><span class="kw">map2_df</span>(grid<span class="op">$</span>mtry, grid<span class="op">$</span>min_n, <span class="op">~</span><span class="kw">fit_rf</span>(<span class="dv">500</span>, .x, .y))</span>
<span id="cb283-5"><a href="random-forests.html#cb283-5"></a></span>
<span id="cb283-6"><a href="random-forests.html#cb283-6"></a>rf_tune <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb283-7"><a href="random-forests.html#cb283-7"></a><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(.estimate))</span></code></pre></div>
<pre><code>## # A tibble: 24 x 7
##    .metric .estimator .estimate trees  mtry min_n model   
##    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;list&gt;  
##  1 roc_auc hand_till      0.859   500     5     5 &lt;fit[+]&gt;
##  2 roc_auc hand_till      0.859   500     5    10 &lt;fit[+]&gt;
##  3 roc_auc hand_till      0.857   500     5    20 &lt;fit[+]&gt;
##  4 roc_auc hand_till      0.857   500     5    15 &lt;fit[+]&gt;
##  5 roc_auc hand_till      0.856   500     4    10 &lt;fit[+]&gt;
##  6 roc_auc hand_till      0.856   500     4    15 &lt;fit[+]&gt;
##  7 roc_auc hand_till      0.855   500     5     2 &lt;fit[+]&gt;
##  8 roc_auc hand_till      0.855   500     4     5 &lt;fit[+]&gt;
##  9 roc_auc hand_till      0.855   500     5    25 &lt;fit[+]&gt;
## 10 roc_auc hand_till      0.854   500     4    20 &lt;fit[+]&gt;
## # … with 14 more rows</code></pre>
<p>And notice that <em>all</em> of our top models here include our maximum number of predictors. So in this case, bagged trees are actually looking like our best option (rather than a random forest).</p>
</div>
<div id="a-regression-example" class="section level4">
<h4><span class="header-section-number">5.2.1.2</span> A regression example</h4>
<p>For completeness, let’s run through another example using our statewide testing data. We didn’t use these data when fitting a bagged tree model, but we can start with a random forest anyway and see if it simplifies to a bagged tree.</p>
<p>First, let’s read in the data and create our testing set. We’ll only sample 5% of the data so things run more quickly.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="random-forests.html#cb285-1"></a>state_tests &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://github.com/uo-datasci-specialization/c4-ml-fall-2020/raw/master/data/train.csv&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb285-2"><a href="random-forests.html#cb285-2"></a><span class="st">  </span><span class="kw">slice_sample</span>(<span class="dt">prop =</span> <span class="fl">0.05</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb285-3"><a href="random-forests.html#cb285-3"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>classification)</span>
<span id="cb285-4"><a href="random-forests.html#cb285-4"></a></span>
<span id="cb285-5"><a href="random-forests.html#cb285-5"></a>splt_reg &lt;-<span class="st"> </span><span class="kw">initial_split</span>(state_tests)</span>
<span id="cb285-6"><a href="random-forests.html#cb285-6"></a>train_reg &lt;-<span class="st"> </span><span class="kw">training</span>(splt_reg)</span></code></pre></div>
<p>Now we’ll create a recipe for these data. It is a bit more complicated than the last example because the data are a fair amount more complicated. The recipe looks like this:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="random-forests.html#cb286-1"></a>rec_reg &lt;-<span class="st"> </span><span class="kw">recipe</span>(score <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_reg)  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb286-2"><a href="random-forests.html#cb286-2"></a><span class="st">  </span><span class="kw">step_mutate</span>(<span class="dt">tst_dt =</span> lubridate<span class="op">::</span><span class="kw">mdy_hms</span>(tst_dt),</span>
<span id="cb286-3"><a href="random-forests.html#cb286-3"></a>              <span class="dt">time_index =</span> <span class="kw">as.numeric</span>(tst_dt)) <span class="op">%&gt;%</span></span>
<span id="cb286-4"><a href="random-forests.html#cb286-4"></a><span class="st">  </span><span class="kw">update_role</span>(tst_dt, <span class="dt">new_role =</span> <span class="st">&quot;time_index&quot;</span>)  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb286-5"><a href="random-forests.html#cb286-5"></a><span class="st">  </span><span class="kw">update_role</span>(<span class="kw">contains</span>(<span class="st">&quot;id&quot;</span>), ncessch, <span class="dt">new_role =</span> <span class="st">&quot;id vars&quot;</span>)  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb286-6"><a href="random-forests.html#cb286-6"></a><span class="st">  </span><span class="kw">step_novel</span>(<span class="kw">all_nominal</span>())  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb286-7"><a href="random-forests.html#cb286-7"></a><span class="st">  </span><span class="kw">step_unknown</span>(<span class="kw">all_nominal</span>())  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb286-8"><a href="random-forests.html#cb286-8"></a><span class="st">  </span><span class="kw">step_rollimpute</span>(<span class="kw">all_numeric</span>(), <span class="op">-</span><span class="kw">all_outcomes</span>(), <span class="op">-</span><span class="kw">has_role</span>(<span class="st">&quot;id vars&quot;</span>))  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb286-9"><a href="random-forests.html#cb286-9"></a><span class="st">  </span><span class="kw">step_medianimpute</span>(<span class="kw">all_numeric</span>(), <span class="op">-</span><span class="kw">all_outcomes</span>(), <span class="op">-</span><span class="kw">has_role</span>(<span class="st">&quot;id vars&quot;</span>))  <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># neccessary when date is NA</span></span>
<span id="cb286-10"><a href="random-forests.html#cb286-10"></a><span class="st">  </span><span class="kw">step_zv</span>(<span class="kw">all_predictors</span>()) </span></code></pre></div>
<p>The recipe above does the following</p>
<ul>
<li>Defines <code>score</code> as the outcome and all other variables as predictors</li>
<li>Changes the <code>tst_dt</code> column (date the assessment was taken) to be a date (rather than character) and creates a new column that is a numeric version of the date</li>
<li>Changes the role of <code>tst_dt</code> from a predictor to a <code>time_index</code>, which is a special role that can be used to calculate date windows</li>
<li>Changes the role of all ID variables to an arbitrary <code>"id vars"</code> role, just so they are not used as predictors in the model</li>
<li>Recodes nominal columns such that previously unencountered levels of the variable will be recoded to a <code>"new"</code> level</li>
<li>Imputes and <code>"unknown"</code> category for all nominal data</li>
<li>Uses a rolling time window imputation (median for the closes 5 data points) to impute all numeric columns</li>
<li>In cases where the time window is missing, imputes with the median of the column for all numeric columns</li>
<li>Removes zero variance predictors</li>
</ul>
<p>Next, we want to process the data using this recipe. Note that after we <code>bake()</code> the data, we remove those variables that are not predictors. Note that it might be worth considering keeping a categorical version of the school ID in the model (given that the school in which a student is enrolled is likely related to their score), but to keep things simple we’ll just remove all ID variables for now.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="random-forests.html#cb287-1"></a>processed_reg &lt;-<span class="st"> </span>rec_reg <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb287-2"><a href="random-forests.html#cb287-2"></a><span class="st">  </span><span class="kw">prep</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb287-3"><a href="random-forests.html#cb287-3"></a><span class="st">  </span><span class="kw">bake</span>(<span class="dt">new_data =</span> <span class="ot">NULL</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb287-4"><a href="random-forests.html#cb287-4"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="kw">contains</span>(<span class="st">&quot;id&quot;</span>), <span class="op">-</span>ncessch, <span class="op">-</span>tst_dt)</span>
<span id="cb287-5"><a href="random-forests.html#cb287-5"></a></span>
<span id="cb287-6"><a href="random-forests.html#cb287-6"></a>processed_reg</span></code></pre></div>
<pre><code>## # A tibble: 7,104 x 32
##    gndr  ethnic_cd enrl_grd tst_bnch migrant_ed_fg ind_ed_fg
##    &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;         &lt;fct&gt;    
##  1 M     W                3 1B       N             N        
##  2 M     W                8 3B       N             N        
##  3 M     W                3 1B       N             N        
##  4 F     H                3 1B       N             N        
##  5 F     H                5 2B       N             N        
##  6 F     W                4 G4       N             N        
##  7 F     W                7 G7       N             N        
##  8 M     W                3 1B       N             N        
##  9 M     H                8 3B       N             N        
## 10 M     W                4 G4       N             N        
## # … with 7,094 more rows, and 26 more variables: sp_ed_fg &lt;fct&gt;,
## #   tag_ed_fg &lt;fct&gt;, econ_dsvntg &lt;fct&gt;, ayp_lep &lt;fct&gt;,
## #   stay_in_dist &lt;fct&gt;, stay_in_schl &lt;fct&gt;, dist_sped &lt;fct&gt;,
## #   trgt_assist_fg &lt;fct&gt;, ayp_dist_partic &lt;fct&gt;,
## #   ayp_schl_partic &lt;fct&gt;, ayp_dist_prfrm &lt;fct&gt;,
## #   ayp_schl_prfrm &lt;fct&gt;, rc_dist_partic &lt;fct&gt;,
## #   rc_schl_partic &lt;fct&gt;, rc_dist_prfrm &lt;fct&gt;,
## #   rc_schl_prfrm &lt;fct&gt;, lang_cd &lt;fct&gt;, tst_atmpt_fg &lt;fct&gt;,
## #   grp_rpt_dist_partic &lt;fct&gt;, grp_rpt_schl_partic &lt;fct&gt;,
## #   grp_rpt_dist_prfrm &lt;fct&gt;, grp_rpt_schl_prfrm &lt;fct&gt;,
## #   lat &lt;dbl&gt;, lon &lt;dbl&gt;, score &lt;dbl&gt;, time_index &lt;dbl&gt;</code></pre>
<p>We’ll now use the processed data to fit a random forest, evaluating our model using the OOB samples. Let’s start by writing a function that fits the model for a given set of hyperparameters and returns a data frame with the hyperparameter values, our performance metric (RMSE) and the model object.</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="random-forests.html#cb289-1"></a>rf_fit_reg &lt;-<span class="st"> </span><span class="cf">function</span>(tree_n, mtry, min_n) {</span>
<span id="cb289-2"><a href="random-forests.html#cb289-2"></a>  rf_mod_reg &lt;-<span class="st"> </span><span class="kw">rand_forest</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb289-3"><a href="random-forests.html#cb289-3"></a><span class="st">    </span><span class="kw">set_mode</span>(<span class="st">&quot;regression&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb289-4"><a href="random-forests.html#cb289-4"></a><span class="st">    </span><span class="kw">set_engine</span>(<span class="st">&quot;ranger&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb289-5"><a href="random-forests.html#cb289-5"></a><span class="st">    </span><span class="kw">set_args</span>(<span class="dt">mtry =</span> mtry,</span>
<span id="cb289-6"><a href="random-forests.html#cb289-6"></a>             <span class="dt">min_n =</span> min_n,</span>
<span id="cb289-7"><a href="random-forests.html#cb289-7"></a>             <span class="dt">trees =</span> tree_n)</span>
<span id="cb289-8"><a href="random-forests.html#cb289-8"></a></span>
<span id="cb289-9"><a href="random-forests.html#cb289-9"></a>  rf_fit &lt;-<span class="st"> </span><span class="kw">fit</span>(rf_mod_reg, </span>
<span id="cb289-10"><a href="random-forests.html#cb289-10"></a>                <span class="dt">formula =</span> score <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb289-11"><a href="random-forests.html#cb289-11"></a>                <span class="dt">data =</span> processed_reg)</span>
<span id="cb289-12"><a href="random-forests.html#cb289-12"></a>  </span>
<span id="cb289-13"><a href="random-forests.html#cb289-13"></a>  <span class="co"># output RMSE</span></span>
<span id="cb289-14"><a href="random-forests.html#cb289-14"></a>  <span class="kw">tibble</span>(<span class="dt">rmse =</span> <span class="kw">sqrt</span>(rf_fit<span class="op">$</span>fit<span class="op">$</span>prediction.error)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb289-15"><a href="random-forests.html#cb289-15"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">trees =</span> tree_n, </span>
<span id="cb289-16"><a href="random-forests.html#cb289-16"></a>           <span class="dt">mtry =</span> mtry, </span>
<span id="cb289-17"><a href="random-forests.html#cb289-17"></a>           <span class="dt">min_n =</span> min_n,</span>
<span id="cb289-18"><a href="random-forests.html#cb289-18"></a>           <span class="dt">model =</span> <span class="kw">list</span>(rf_fit))</span>
<span id="cb289-19"><a href="random-forests.html#cb289-19"></a>}</span></code></pre></div>
<p>Notice in the above that I’ve used <code>sqrt(rf_fit$fit$prediction.error)</code> for the root mean square error, rather than using the model predictions with a <strong>{yardstick}</strong> function. This is because the default OOB error for a regression models with <strong>{ranger}</strong> is the mean square error, so we don’t have to do any predictions on our own - we just need to take the square root of this value. Most of the rest of this function is the same as before.</p>
<p>Let’s now conduct our tuning. First, let’s check how many predictors we have:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="random-forests.html#cb290-1"></a><span class="kw">ncol</span>(processed_reg) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 31</code></pre>
<p>Remember, for regression problems, I good place to start is around <span class="math inline">\(m/3\)</span>, or 10.33. Let’s make a grid of <code>mtry</code> values, centered around 10.33. For <code>min_n</code>, we’ll use the same values we did before: 2, 5, 10, 15, 20, and 25. We’ll then evaluate the OOB RMSE across these different values and see if we need to conduct any more hyperparameter tuning.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="random-forests.html#cb292-1"></a>grid_reg &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">mtry =</span> <span class="dv">5</span><span class="op">:</span><span class="dv">15</span>,</span>
<span id="cb292-2"><a href="random-forests.html#cb292-2"></a>                        <span class="dt">min_n =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">25</span>, <span class="dv">5</span>)))</span></code></pre></div>
<p>And now we’ll use our function from above to fit each of these models, evaluating each with the OOB RMSE. We’ll start out with 1000 trees, then inspect that with our finalized model to make sure it is sufficient. Note that, even with using the OOB samples for our tuning, the following code takes a decent amount of time to run.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="random-forests.html#cb293-1"></a><span class="kw">tic</span>()</span>
<span id="cb293-2"><a href="random-forests.html#cb293-2"></a>rf_reg_fits &lt;-<span class="st"> </span><span class="kw">map2_df</span>(grid_reg<span class="op">$</span>mtry, grid_reg<span class="op">$</span>min_n, </span>
<span id="cb293-3"><a href="random-forests.html#cb293-3"></a>                       <span class="op">~</span><span class="kw">rf_fit_reg</span>(<span class="dv">1000</span>, .x, .y))</span>
<span id="cb293-4"><a href="random-forests.html#cb293-4"></a><span class="kw">toc</span>()</span></code></pre></div>
<pre><code>## 774.04 sec elapsed</code></pre>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="random-forests.html#cb295-1"></a>rf_reg_fits <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb295-2"><a href="random-forests.html#cb295-2"></a><span class="st">  </span><span class="kw">arrange</span>(rmse)</span></code></pre></div>
<pre><code>## # A tibble: 66 x 5
##     rmse trees  mtry min_n model   
##    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;list&gt;  
##  1  87.8  1000     7    20 &lt;fit[+]&gt;
##  2  87.9  1000     6    15 &lt;fit[+]&gt;
##  3  87.9  1000     7    25 &lt;fit[+]&gt;
##  4  87.9  1000     6    25 &lt;fit[+]&gt;
##  5  87.9  1000     7    15 &lt;fit[+]&gt;
##  6  87.9  1000     8    25 &lt;fit[+]&gt;
##  7  87.9  1000     6    20 &lt;fit[+]&gt;
##  8  87.9  1000     6    10 &lt;fit[+]&gt;
##  9  88.0  1000     7    10 &lt;fit[+]&gt;
## 10  88.0  1000     6     5 &lt;fit[+]&gt;
## # … with 56 more rows</code></pre>
<p>Let’s evaluate the hyperparameters we search by plotting the <code>mtry</code> values against the RMSE, faceting by <code>min_n</code>.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="random-forests.html#cb297-1"></a><span class="kw">ggplot</span>(rf_reg_fits, <span class="kw">aes</span>(mtry, rmse)) <span class="op">+</span></span>
<span id="cb297-2"><a href="random-forests.html#cb297-2"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb297-3"><a href="random-forests.html#cb297-3"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>min_n)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-195-1.png" width="672" />
It looks like values of 7 or 8 are optimal for <code>mtry</code>, and our <code>min_n</code> is likely somewhere north of 20. Let’s see if we can improve performance more by tuning a bit more on <code>min_n</code>. We’ll evaluate values from 21 to 31 in increments of 2, while using both <code>mtry</code> values of 7 and 8.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="random-forests.html#cb298-1"></a>grid_reg2 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">mtry =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">8</span>),</span>
<span id="cb298-2"><a href="random-forests.html#cb298-2"></a>                         <span class="dt">min_n =</span> <span class="kw">seq</span>(<span class="dv">21</span>, <span class="dv">31</span>, <span class="dv">2</span>))</span>
<span id="cb298-3"><a href="random-forests.html#cb298-3"></a></span>
<span id="cb298-4"><a href="random-forests.html#cb298-4"></a><span class="kw">tic</span>()</span>
<span id="cb298-5"><a href="random-forests.html#cb298-5"></a>rf_reg_fits2 &lt;-<span class="st"> </span><span class="kw">map2_df</span>(grid_reg2<span class="op">$</span>mtry, grid_reg2<span class="op">$</span>min_n, </span>
<span id="cb298-6"><a href="random-forests.html#cb298-6"></a>                       <span class="op">~</span><span class="kw">rf_fit_reg</span>(<span class="dv">1000</span>, .x, .y))</span>
<span id="cb298-7"><a href="random-forests.html#cb298-7"></a><span class="kw">toc</span>()</span></code></pre></div>
<pre><code>## 104.774 sec elapsed</code></pre>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="random-forests.html#cb300-1"></a>rf_reg_fits2 &lt;-<span class="st"> </span>rf_reg_fits2 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb300-2"><a href="random-forests.html#cb300-2"></a><span class="st">  </span><span class="kw">arrange</span>(rmse)</span>
<span id="cb300-3"><a href="random-forests.html#cb300-3"></a>rf_reg_fits2</span></code></pre></div>
<pre><code>## # A tibble: 12 x 5
##     rmse trees  mtry min_n model   
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;  
##  1  87.8  1000     7    23 &lt;fit[+]&gt;
##  2  87.9  1000     7    31 &lt;fit[+]&gt;
##  3  87.9  1000     7    21 &lt;fit[+]&gt;
##  4  87.9  1000     7    27 &lt;fit[+]&gt;
##  5  87.9  1000     7    29 &lt;fit[+]&gt;
##  6  87.9  1000     7    25 &lt;fit[+]&gt;
##  7  87.9  1000     8    29 &lt;fit[+]&gt;
##  8  87.9  1000     8    25 &lt;fit[+]&gt;
##  9  87.9  1000     8    23 &lt;fit[+]&gt;
## 10  87.9  1000     8    31 &lt;fit[+]&gt;
## 11  88.0  1000     8    21 &lt;fit[+]&gt;
## 12  88.0  1000     8    27 &lt;fit[+]&gt;</code></pre>
<p>Notice the RMSE is essentially equivalent for all models listed above. We’ll go forward with the first model, but any of these combinations of hyperparameters likely provide similar out-of-sample predictive accuracy (according to RMSE).</p>
<p>Finally, before moving on to evaluating our model against the test set, we would likely want to make sure that we fit the model with a sufficiently large number of trees. Let’s investigate with a model that had the lowest RMSE. We’ll use trees from 500 to 1500 by increments of 100.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="random-forests.html#cb302-1"></a><span class="kw">tic</span>()</span>
<span id="cb302-2"><a href="random-forests.html#cb302-2"></a>rf_reg_ntrees &lt;-<span class="st"> </span><span class="kw">map_df</span>(<span class="kw">seq</span>(<span class="dv">500</span>, <span class="dv">1500</span>, <span class="dv">100</span>), </span>
<span id="cb302-3"><a href="random-forests.html#cb302-3"></a>                        <span class="op">~</span><span class="kw">rf_fit_reg</span>(.x, </span>
<span id="cb302-4"><a href="random-forests.html#cb302-4"></a>                                    <span class="dt">mtry =</span> rf_reg_fits2<span class="op">$</span>mtry[<span class="dv">1</span>], </span>
<span id="cb302-5"><a href="random-forests.html#cb302-5"></a>                                    <span class="dt">min_n =</span> rf_reg_fits2<span class="op">$</span>min_n[<span class="dv">1</span>])</span>
<span id="cb302-6"><a href="random-forests.html#cb302-6"></a>                        )</span>
<span id="cb302-7"><a href="random-forests.html#cb302-7"></a><span class="kw">toc</span>()</span></code></pre></div>
<pre><code>## 81.344 sec elapsed</code></pre>
<p>And now we’ll plot the number of trees against the RMSE, looking for a point of stability.</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="random-forests.html#cb304-1"></a><span class="kw">ggplot</span>(rf_reg_ntrees, <span class="kw">aes</span>(trees, rmse)) <span class="op">+</span></span>
<span id="cb304-2"><a href="random-forests.html#cb304-2"></a><span class="st">  </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-198-1.png" width="672" /></p>
<p>Hmm… that doesn’t look terrifically stable. <strong>BUT WAIT!</strong> Notice the bottom to the top of the y-axis is only about one-tenth of a point difference. So really, this is not a problem with instability, but that it is actually stable across all these values. Just to prove this to ourselves, let’s look at the same plot, but making the y-axis span one point (which is still not very much), going from 87 to 88.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="random-forests.html#cb305-1"></a><span class="kw">ggplot</span>(rf_reg_ntrees, <span class="kw">aes</span>(trees, rmse)) <span class="op">+</span></span>
<span id="cb305-2"><a href="random-forests.html#cb305-2"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb305-3"><a href="random-forests.html#cb305-3"></a><span class="st">  </span><span class="kw">ylim</span>(<span class="dv">87</span>, <span class="dv">88</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-199-1.png" width="672" /></p>
<p>And now it looks more stable. So even with 500 trees we likely would have been fine.</p>
<p>From here, we could continue on to evaluate our final model against the test set using the <code>last_fit()</code> function, but we will leave that as an exercise for the reader. Note that we also only worked with 5% of the total data. The hyperparameters that we settled on may be different if we used more of the data. We could also likely improve performance more by including additional information in the model - e.g., information on staffing and funding, the size of the school or district, indicators of the economic and demographic makeup of the surrounding area, or practices related to school functioning (e.g., use of suspension/expulsion).</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bagged-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="feature-and-model-interpretation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
