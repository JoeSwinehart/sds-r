# Feature Engineering

```{r include = FALSE}
source("_common.R")

knitr::opts_chunk$set(warning = FALSE)
```

Feature engineering is a fancy machine learning way of saying "prepare your data for analysis". But it's also more than just getting your data in the right format. It's about transforming variables to help the model provide more stable predictions; encoding, modeling, or omitting missing data; creating new variables from the available dataset; and otherwise helping improve the model performance through transformations and modifcations to the existing data *before* analysis.

One of the biggest challenges with feature engineering is what's referred to as **data leakage**, which is when information from the test dataset leaks into the training dataset. Consider the simple example of normalizing or standardizing a variable (subtracting the variable mean from each value and then dividing by the standard deviation). Normalizing your variables is neccesary for a wide range of predictive models, including any models using regularization methods, principal components analysis, and $k$-nearest neighbor models, among others. But when we normalize the variable, it is critical we do so relative to the training set, *not* relative to the full dataset. If we normalize the numeric variables in our dataset, then divide it into test and training datasets, then information from the test dataset has *leaked* into the training dataset. This seems simple enough - just wait to standardize until after splitting - but it becomes more complicated when we consider *tuning* a model. If we use a process like $k$-fold cross-validation, then we have to normalize *within* each fold to get an accurate representation of out-of-sample predictive accuracy. 

<!-- In the above, things like k-fold cv will probably have chapters of their own. We should link to them in the text -->

In this chapter, we'll introduce feature engineering using the [{recipes}](https://recipes.tidymodels.org) package from the [{tidymodels}](https://www.tidymodels.org) suite of packages which, as we will see, helps you to be explicit about the decisions you're making, while avoiding potential issues with data leakage.

## Basics of {recipes} 
![](https://github.com/tidymodels/recipes/raw/master/man/figures/logo.png)

The recipes package is designed to replace the [stats::model.matrix](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/model.matrix.html) function that you're probably familiar with. For example, if you fit a model like the below

```{r}
library(palmerpenguins)
m1 <- lm(bill_length_mm ~ species, data = penguins)
summary(m1)
```

You can see that our `species` column, which has the values `r unique(penguins$species)`, is automatically dummy-coded for us, with the first level in the factor variable set as the reference group.

The {recipes} package forces you to be a bit more explicit in these decisions. But it also has a much wider range of modifications it can make to the data. Another piece that is slightly different is that, in the above, you may not have even realized `stats::model.matrix` was doing anything for you because it's wrapped within the `stats::lm` modeling code. But with {recipes}, you make the modifications to your data first, then conduct your analysis.

The {recipes} package allows you to create a blueprint (or recipe) to apply to a given dataset, without actually applying those operations. We can then use this blueprint iteratively across sets of data (e.g., folds) as well as on new (potentially unseen) data that has the same structure (variables). This process helps avoid data leakage because all operations are carried forward and applied together, and no operations are conducted until explicitly requested. 

## Creating a recipe
Let's read in some data and begin creating a basic recipe. We'll work with the simulated statewide testing data introduced previously. This is a fairly decent sized dataset, and since we're just illustrating concepts here, we'll pull a random sample of 2% of the total data to make everything run a bit quicker. We'll also remove the `classification` variable, which is just a categorical version of `score`, our outcome.

In the chunk below, we read in the data, sample a random 2% of the data (being careful to set a seed first so our results are reproducible), split it into training and test sets, and extract just the training dataset. We'll hold off on splitting it into CV folds for now. 

```{r message = FALSE}
library(tidyverse)
library(tidymodels)

set.seed(8675309)
full_train <- read_csv("https://github.com/uo-datasci-specialization/c4-ml-fall-2020/raw/master/data/train.csv") %>% 
  slice_sample(prop = 0.02) %>% 
  select(-classification)

splt <- initial_split(full_train)
train <- training(splt)
```

A quick reminder, the data look like this

```{r echo = FALSE}
reactable(train, filterable = TRUE, highlight = TRUE)
```

And you can see the full data dictionary on the Kaggle website [here](https://www.kaggle.com/c/edld-654-fall-2020/data?select=data_dictionary.csv).

When creating recipes, we can still use the formula interface to define how the data will be modeled. In this case, we'll say that the `score` column is predicted by everything else in the data frame.

```{r}
rec <- recipe(score ~ ., data = train)
```

Notice that I still declare the dataset, even though this is just a blueprint. It uses the dataset I provide to get the names of the columns from the dataset, but it doesn't actually do anything with this dataset (unless we ask it to). Let's look at what this recipe looks like

```{r }
rec
```

Notice it just states that this is a data recipe in which we have specified 1 outcome variable and 39 predictors.

We can `prep` this recipe to learn more

```{r }
prep(rec)
```

Notice we now get an additional message about how many rows are in the data, and how many of these rows contain missing (incomplete data). So the recipe is the blueprint, and we prep the recipe to get it to actually go into the data and conduct the operations. The dataset it has now, however, is just a placeholder than can be substituted in for any other dataset with an equivalent structure.

But of course modeling score as the outcome with everything else predicting it (as is) is not reasonable for multiple reasons. We have many ID variables, for one, and we also multipe categorical variables. For some methods (like tree-based models) it might be okay to leave these as is, but for others (like any model in the linear regression family) we'll wan to encode them somehow (e.g., dummy code).

We can do these operations by adding *steps* to our recipe. In the first step, we'll update the role of all the ID variables so they are not included among the predictors. In the second, we will dummy code all nominal variables.

```{r }
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_dummy(all_nominal())
```

When updating the roles, we can change the variable label (text passed to the `new_role` argument) to be anything we want, so long as it's not `"predictor"` or `"outcome"`.

Notice in the above I am also using helper functions to apply the operations to all variables of a specific type. There are five main helper functions: `all_predictors()`, `all_outcomes()`, `all_nominal()`, `all_numeric()` and `has_role()`. You can use these together, including with negation (e.g., `-all_outcomes` to specify the operation should **not** apply to the outcome variable(s)) to select any set of variables you want to apply the operation to.

Let's try `prep`ping this recipe

```{r error = TRUE, warning = FALSE}
prep(rec)
```

Uh oh! We have an error. Our recipe is trying to dummy code the `lang_cd` variable, but it has only one level. It's kind of hard to dummy-code a constant! 

Luckily, we can expand our recipe to first remove any zero-variance predictors, like so

```{r }
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_zv(all_predictors()) %>% 
  step_dummy(all_nominal())
```

The `zv` part stands for "zero variance" and should take care of this problem. Let's try again.

```{r warning = FALSE}
prep(rec)
```

Beautiful! Note we do still get a warning here, but I've omitted it in the text (we'll take care of it later). Our recipe says we now have 6 ID variables, 1 outcome, and 33 predictors, with 2841 data points (rows of data). The `calc_admn_cd` and `lang_cd` variables have been removed because they have zero variance, and several variables have been dummy coded, including `gndr` and `ethnic_cd`, among others.

Let's dig just a bit deeper here though. What's going on with these zero-variance variables? Let's look back at the training data.

```{r }
train %>% 
  count(calc_admn_cd)

train %>% 
  count(lang_cd)
```

So at least in our sample, `calc_admn_cd` really is just fully missing, which means it might as well be dropped because it's providing us exactly nothing. But that's not the case with `lang_cd`. It has two values, `NA` and `S`. This variable represents the language the test was administered in and the `NA` values are actually meaningful here because they are the the "default" administration, meaning English. So rather than dropping these, let's `mutate` them to transform the `NA` values to `"E"` for English. We could reasonably do this inside or outside the recipe, but a good rule of thumb is, if it *can* go in the recipe, put it in the recipe. It can't hurt, and doing operations outside of the recipe risks data leakage.

```{r }
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd)) %>% 
  step_zv(all_predictors()) %>% 
  step_dummy(all_nominal())
```

Let's take a look at what our data would actually look like when applying this recipe now. First, we'll prep the recipe

```{r warning = FALSE}
prepped <- prep(rec)
prepped
```

And we see that `lang_cd` is no longer being caught by the zero variance filter. Next we'll `bake` the recipe to actually apply it *to our data*. If we specify `new_data = NULL`, `bake` will apply the operation to the data we specified in the recipe. But we can also pass new data as an additional argument and it will apply the operations to *that* data instead of the data specified in the recipe.

```{r }
bake(prepped, new_data = NULL)
```

And now we can actually *see* the dummy-coded categorical variables, along with the other operations we requested. For example, `calc_admn_cd` is not in the dataset. Notice the ID variables are output though, which makes sense because they are often neccessary for joining with other data sources. But it's important to realize that they are output (i.e., all variables are returned, regardless of role) because if we passed this directly to a model they would be included as predictors. Note that there may be reasons you would want to include a school and/or district level ID variable in your modeling, but you certainly would not want redundant variables.

We do still have one minor issue with this recipe though, which is pretty evident when looking at the column names of our baked dataset. The `tst_dt` variable, which specifies the data the test was taken, was treated as a categorical variable because it read in as a character vector. That means all the dates are being dummy coded! Let's fix this by just transforming it to a date within our `step_mutate`.

```{r}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), "E", lang_cd)),
              tst_dt = lubridate::mdy_hms(tst_dt)) %>% 
  step_zv(all_predictors()) %>% 
  step_dummy(all_nominal())
```

And now when we `prep`/`bake` the dataset it's still a date variable, which is what we probably want (it will modeled as a numeric variable).

```{r warning = FALSE}
rec %>% 
  prep() %>% 
  bake(new_data = NULL)
```

### Order matters
It's important to realize that the order of the steps matters. In our recipe, we first declare ID variables as having a different role than predictors or outcomes, we then modify two variables, remove zero-variance predictors, and finally dummy code all categorical (nominal) variables. What happens if we instead dummy code and *then* remove zero-variance predictors?

```{r warning = FALSE, error = TRUE}
rec <- recipe(score ~ ., train) %>% 
  step_dummy(all_nominal()) %>% 
  step_zv(all_predictors()) 

prep(rec)
```

We end up with the error, whereas we don't if we remove zero variance predictors *and then* dummy code

```{r warning = FALSE}
rec <- recipe(score ~ ., train) %>% 
  step_zv(all_predictors()) %>% 
  step_dummy(all_nominal()) 

prep(rec)
```

This is true for all steps, and may occasionally lead to you needing to apply the same operation at multiple steps (e.g., a *near* zero variance filter could be applied before and after dummy-coding).

All of the above serves as a basic introduction to developing a recipe, and the what follows goes into more detail on specific feature engineering pieces. For complete documentation on all possible recipe steps, please see the [documentaion](https://recipes.tidymodels.org/reference/index.html).

## Encoding categorical data
For many (but not all) modeling frameworks, categorical data must be transformed somehow. The most common of these is, as we've already seen, dummy coding, which `stats::model.matrix` will do for you automatically using the `stats::contrasts` function. There are also other coding schemes you can use with base R, such as Helmert and polynomial coding (see `?contrasts` and related functions). Dummy coding leaves one group out (the first level of the factor, by default) and creates new columns for all the other groups coded $0$ or $1$ depending on whether the original variable represented that value or not. For example

```{r }
f <- factor(c("red", "red", "blue", "green", "green", "green"))
contrasts(f)
```

In the above, `"blue"` has been assigned as the reference category (note that factor levels are assigned in alphabetical order by default), and dummy variables have been created for `"green"` and `"red"`. In a linear regression framework, `"blue"` would become the intercept.

We can recreate this same coding scheme with {recipes}, but we need to first put it in a data frame.

```{r }
df <- data.frame(f, score = rnorm(length(f)))
df

recipe(score ~ f, data = df) %>% 
  step_dummy(f) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

In the above, we've created the actual columns we need, while in the base example we only created the contrast matrix (although it's relatively straightforward to then create the columns).

The {recipes} version is, admittedly, a fair amount of additional code, but as we saw in the previous section, {recipes} is capable of making a wide range of transformation in a systematic way.

### Transformations beyond dummy coding
Although less used in inferential statistics, there are a number of additional transformations we can use to encode categorical data. The most straightforward is **one-hot** encoding. One-hot encoding is essentially equivalent to dummy coding except we create the variables for *all* levels in the categorical variable (i.e., we do not leave one out as a reference group). This generally makes them less useful in linear regression frameworks (unless the model intercept is dropped), but they can be highly useful in a number of other frameworks, such as tree-based methods (covered later in the book).

To use one-hot encoding, we pass the additional `one_hot` argument to `step_dummy`.

```{r}
recipe(score ~ f, data = df) %>% 
  step_dummy(f, one_hot = TRUE) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

Another relatively common encoding scheme, particularly within natural language processing frameworks, is **integer encoding**, where each level is associated with a unique integer. For example

```{r }
recipe(score ~ f, data = df) %>% 
  step_integer(f) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

Notice that the syntax is essentially equivalent to the previous dummy-coding example, but we've just swapped out `step_dummy` for `step_integer`. Integer encoding can be useful in natural language processing in particular because words can be encoded as integers, and then the algorithm can search for patterns in the numbers. 

### Handling new levels
One other very common problem with encoding categorical data is how to handle new, unseen levels. For example, if we create a recipe as follows

```{r }
rec <- recipe(score ~ f, data = df) %>% 
  step_dummy(f)
```

we will have no problem creating dummy variables as long as the levels of $f$ are within those contained in `df$f` (or, more mathematically, where $f \in F$). But what if, in a new sample, $f =$ "purple" or $f =$ "gray"? Let's try and see what happens

```{r }
df2 <- data.frame(f = factor(c("blue", "green", "purple", "gray")),
                  score = rnorm(4))
rec %>% 
  prep() %>% 
  bake(new_data = df2)
```

We end up propagating missing data, which is obviously less than ideal. Luckily, the solution is pretty straightforward. We just add a new step to our recipe to handle *novel* (or new) categories, lumping them all in their own level.

```{r }
rec <- recipe(score ~ f, data = df) %>% 
  step_novel(f) %>% 
  step_dummy(f)

rec %>% 
  prep() %>% 
  bake(new_data = df2)
```

This is not perfect, because `"purple"` and `"orange"` may be highly different, and we're modeling them as a single category. But at least we're able to move forward with our model without introducing new missing data. As an aside, this is a small example of why having good training data is so important. If you don't have all the levels of a categorical variable represented, you may end up essentially collapsing levels when there is meaningful variance that could be parsed out. 

You can also use a similar approach with `step_other` if you have a categorical variable with lots of levels (and a small-ish $n$ by comparison). Using `step_other`, you specify a threshold below which levels should be collapsed into a single "other" category. The threshold can be passed as a proportion or a frequency.

### Final thoughts on encoding categorical data
There are, of course, many other ways you can encode categorical data. One important consideration is whether or not the variable is *ordered* (e.g., low, medium, high) in which case it may make sense to have a corresponding ordered numeric variable (e.g., $0$, $1$, $2$). Of course, the method of coding these ordered values will relate to assumptions of your modeling process (in the previous example, that there is a *linear*, constant change across categories). In our experience, however, the combination of dummy coding (with potentially a one-hot alternative used), integer coding, or simply leaving the categorical variables as they are (for specific frameworks, like tree-based methods) is sufficient most (but not all) of the time. For a more complete discussion of encoding categorical data for predictive modeling frameworks, we recommend [Chapter 5](http://www.feat.engineering/encoding-categorical-predictors.html) of Kuhn & Johnson (2019).

<!-- Need to get a bibliography going -->

## Dealing with low variance predictors
Occasionally you have (or can create) variables that are highly imbalanced. A common example might include a gender variable that takes on the values "male", "female", "non-binary", "other", and "refused to answer". Once you dummy-code a variable like this, it is possible that one or more of the categories may be so infrequent that it makes modeling that category difficult. This is not to say that these categories are not important, particularly when considering the representation of your training dataset to real-world applications (and any demographic variable is going to be associated with issues of ethics). Ignoring this variation may lead to systematic biases in model predictions. However, you also regularly have to make compromises to get models to work and be useful. One of those compromises often includes (with many types of variables, not just demographics) dropping highly imbalanced predictors. 

Let's look back at our statewide testing data. Let's `bake` the final recipe from our [Creating a recipe] section on the training data (that we fed to the recipe) and look at the dummy variables that are created.

```{r warning = FALSE}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), "E", lang_cd)),
              tst_dt = lubridate::mdy_hms(tst_dt)) %>% 
  step_zv(all_predictors()) %>% 
  step_dummy(all_nominal())

baked <- rec %>% 
  prep() %>% 
  bake(new_data = NULL)
```

Below is a table of just the categorical variables and the frequency of each value.

```{r echo = FALSE}
tbls <- map(baked, table)
level_freq <- map_df(tbls, ~data.frame(value = names(.x),
                      frequency = as.numeric(.x)),
       .id = "variable") %>% 
  filter(!grepl("id|ncessch|lat|lon|score|tst_dt", variable))

reactable(level_freq, filterable = TRUE, highlight = TRUE)
```

The relative frequency of many of these looks fine, but for some one category has very low frequency. For example, `ayp_lep_M` has 576 observations (from our random 2% sample) that were $0$, and only 2 that were $1$. This is the same for `ayp_lep_S`. We may therefore consider applying a *near-zero variance filter* to drop these columns. Let's try this, and then we'll talk a bit more about what the filter is actually doing.

```{r warning = FALSE}
rec_nzv <- rec %>% 
  step_nzv(all_predictors())

baked_rm_nzv <- rec_nzv %>% 
  prep() %>% 
  bake(new_data = NULL)
```

Let's look at what columns are in `baked` that were removed from `baked_rm_nzv`.

```{r }
removed_columns <- names(baked)[!(names(baked) %in% names(baked_rm_nzv))]
removed_columns
```

As you can see, the near-zero variance filter has been quite aggressive here, removing `r length(removed_columns)` columns. Looking back at our table of variables, we can see that, for example, there are 55 students coded Black out of `r 2786 + 55`, and it could be reasonably argued that this column is worth keeping in the model. 

So how is `step_nzv` working and how can we adjust it to be not quite so aggressive? Variables are flagged for being near-zero variance if they  

* Have very few unique values, and 
* The frequency ratio for the most common value to the second most common value is large

These criteria are implemented in `step_nzv` through the `unique_cut` and `freq_cut` arguments, respectively. The former is estimated as the number of unique values divided by the total number of samples (length of the column) times 100 (i.e., it is a percent), while the latter is estimated by the most common level frequency divided by the second most common level frequency. The default for `unique_cut` is 10, while the default for `freq_cut` is $95/5 = 19$. For a column to be "caught" by a near-zero variance filter, and removed from the training set, it must be *below* the specified `unique_cut` and *above* the specified `freq_cut`.

In the case of `ethnic_cd_B`, we see that there are two unique values, $0$ and $1$ (because it's a dummy-coded variable). There are `r nrow(baked)` rows, so the `unique_cut` value is $(2 / 2841) \times 100 = 0.07$. The frequency ratio is $2786/55 = 50.65$. It therefore meets both of the default criteria (below `unique_cut` and above `freq_cut`) and is removed.

If you're applying a near-zero variance filter on dummy variables, there will always be only 2 values, leading to a small `unique_cut`. This might encourage you to up the `freq_cut` to a higher value. Let's try this approach

```{r }
rec_nzv2 <- rec %>% 
  step_nzv(all_predictors(), 
           freq_cut = 99/1)

baked_rm_nzv2 <- rec_nzv2 %>% 
  prep() %>% 
  bake(new_data = NULL)

removed_columns2 <- names(baked)[!(names(baked) %in% names(baked_rm_nzv2))]
removed_columns2
```

Removing near-zero variance dummy variables can be a bit tricky because they will essentially always meet the `unique_cut` criteria. But it can be achieved by fiddling with the `freq_cut` variable and, actually, could be considered part of your model tuning process. In this case, we've set it so variables will be removed if greater than 99 out of every 100 cases is the same. This led to only 13 variables being flagged and removed. But we could continue on even further specifying, for example, that 499 out of every 500 must be the same for the variable to be flagged. At some point, however, you'll end up with variables that have such low frequency that model estimation becomes difficult, which is the purpose of applying the near-zero variance filter in the first place.

## Missing data
If we look closely at our statewide testing data, we will see that there is a considerable amount of missingness. In fact, every row of the data frame has at least one value that is missing. The amount to which missing data impacts your work varies by field, but in most fields you're likely to run into situations where you have to handle missing data in some way. The purpose of this section is to discuss a few approaches (as implemented through the {recipes} package) to handling missingness for predictive modeling purposes. Note that this is **not** a comprehensive discussion on the topic (for which, we recommend [Little and Rubin (2002)](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)), but is instead an applied discussion of what you *can* do. As with many  aspects of data analysis, generally, there is no single approach that will always work best, and it's worth trying a few different approaches in your model development to see how different choices impact your model performance. 

There are three basic ways of handling missing data:

* **Omit** rows of the data frame that include missing values
* **Encode** or **Impute** the missing data
* **Ignore** the missing data and estimate from the available data

The last option is not always feasible and will depend the modeling framework you're working within. Some estimation procedures can also lend themselves to efficient handling of missing data (for example, imputation via the posterior distribution with Bayesian estimation). In this section, we'll mostly focus on the first three. Additionally, we will only really be concerned with missingness on the predictor variables here, rather than the outcome. Generally, missing data on the predictors is a much more difficult problem than missingness on the outcome, because most models assume you have complete data across your predictors.

### Missing data via {recipes}
#### Omission
We can **omit** missing data with `step_naomit`. This will remove any row that has any missing data. Let's see how this impacts our data, working with our same recipe we finished up with in the [Creating a recipe] section. I've placed the recipe here again so we don't have to go back to remind ourselves what we did previously.

```{r warning = FALSE}
rec <- recipe(score ~ ., train) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% 
  step_mutate(lang_cd = factor(ifelse(is.na(lang_cd), "E", lang_cd)),
              tst_dt = lubridate::mdy_hms(tst_dt)) %>% 
  step_zv(all_predictors())

na_omit_data <- rec %>% 
  step_naomit(all_predictors()) %>% 
  step_dummy(all_nominal()) %>% 
  prep() %>% 
  bake(new_data = NULL)

nrow(na_omit_data)
```

As can be seen above, when we omit any row with missing data we end up with only `r nrow(na_omit_data)` rows out of the original `r nrow(train)` rows in the training data (or approximately `r round((nrow(na_omit_data)/nrow(train))*100)`% of the original data). This level of data omission is highly likely to introduce systematic biases into your model prediction. Generally, `step_naomit` should only be used when developing preliminary models, where you're just trying to get code to run. When you get to the point where you're actually trying to improve performance, you should consider alternative means

#### Encoding and simple imputation
Encoding missing data is similar to imputation. In imputation, we replace the missing value with something we thing could have reasonably been the real value, if it were observed. When we encode missing data we are creating values that will be included in the modeling process. For example, with categorical variables, we could replace the missingess with a "missing" level, which would then get its own dummy code (if we were using dummy coding to encode the categorical variables).

I mentioned in the [Creating a recipe] section that we were getting warnings but I was omitting them in the text. The reason is because some of these columns have missing data. If we want to avoid this warning, we have to add an additional step to our recipe to encode the missing data in the categorical variables. This step is called `step_unknown` and it replaces missing values with `"unknown"`. Let's do this for all categorical variables, and omit any rows that are missing on numeric columns. 

```{r }
na_encode_data <- rec %>% 
  step_unknown(all_nominal()) %>% 
  step_naomit(all_predictors()) %>% 
  step_dummy(all_nominal()) %>% 
  prep() %>% 
  bake(new_data = NULL)

nrow(na_encode_data)
```

Notice in the above that when I call `step_naomit` I state that it should be applied to `all_predictors` because I've already encoded the nominal predictors in the previous step. This approach allows us to capture `r round((nrow(na_encode_data)/nrow(train))*100)`% of the original data. And as a bonus, we've removed ourselves of the warnings (note - we might also want to apply `step_novel` for any future data that had levels outside of our training data - see [Handling new levels]).

Just a slight step up in complexity from omission of rows with missing data is to impute them with sample descriptive statistics, such as the mean or the median. Generally, I've found median imputation works better than mean imputation, but that could be related to the types of data I work with most frequently. Let's switch datasets so we can see what's happening more directly. 

Let's look at the [airquality](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airquality.html) dataset, which ships with R. 

```{r }
head(airquality)
```

As we can see, `Solar.R` is missing for observations 5 and 6. Let's compute the sample mean and median for this column.

```{r }
mean(airquality$Solar.R, na.rm = TRUE)
median(airquality$Solar.R, na.rm = TRUE)
```

If we use mean or median imputation, we just replace the missing values with these sample statistics. Let's do this with {recipes}, assuming we'll be fitting a model where `Ozone` is the outcome, predicted by all other variables in the dataset.

```{r }
recipe(Ozone ~ ., data = airquality) %>% 
  step_meanimpute(all_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

As we can see, the value $186$ has been imputed for rows 5 and 6, which is the integer version of the sample mean (an integer was imputed because the column was already an integer, and not a double).

Let's try the same thing with median imputation

```{r }
recipe(Ozone ~ ., data = airquality) %>% 
  step_medianimpute(all_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

And as we would expect, the missingness has now been replaced with values of $205$.

Sometimes you have time series data, or there is a date variable in the dataset that accounts for a meaningful proportion of the variance. In these cases, you might consider `step_rollimpute`, which provides a conditional median imputation based on time, and you can set the size of the window from which to calculate the median. In still other cases it may make sense to just impute with the lowest observed value (i.e., assume a very small amount of the predictor), which can be accomplished with `step_lowerimpute`.

These simple imputation techniques are fine to use when developing models. However, it's an area that may be worth returning to as you start to refine your model to see if you can improve performance.

#### Modeling the missingness
Another alternative for imputation is to fit a statistical model with the column you want to impute modeled as the outcome, with all other columns (minus the actual outcome) predicting it. We then use that model for the imputation. Let's first consider a linear regression model. We'll fit the same model we specified in our recipe, using the airquality data.

```{r }
m <- lm(Solar.R ~ ., data = airquality[ ,-1])
summary(m)
```

Notice that I've dropped the first column here, which is `Ozone`, our actual outcome. The model above has been fit using the equivalent of `step_naomit`, otherwise known as *listwise deletion*, where any row with any missing data is removed. 

We can now use the coefficients from this model to impute the missing values in `Solar.R`. For example, row 6 in the dataset had a missing value on `Solar.R` and following values for all other variables

```{r }
row6 <- data.frame(Wind = 14.9, 
                   Temp = 66, 
                   Month = 5, 
                   Day = 6)
```

Using our model, we would predict the following score for this missing value

```{r }
predict(m, newdata = row6)
```

Let's try this using {recipes}.

```{r }
recipe(Ozone ~ ., data = airquality) %>% 
  step_impute_linear(all_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

And we see two important things here. First, row 6 for `Solar.R` is indeed as we expected it to be (albeit, in integer form). Second, the imputed values for rows 5 and 6 are now *different*, which is the first time we've seen this via imputation.

The same basic approach can be used for essentially any statistical model. The {recipes} package has currently implemented linear imputation (as above), $k$-nearest neighbor imputation, and bagged imputation (via bagged trees). Let's see how rows 5 and 6 differ with these approaches.

```{r }
recipe(Ozone ~ ., data = airquality) %>% 
  step_knnimpute(all_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)

recipe(Ozone ~ ., data = airquality) %>% 
  step_bagimpute(all_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

These models are quite a bit more flexible than linear regression, and can potentially overfit. You can, however, control some of the parameters to the models through additional arguments (e.g., $k$ for $knn$, which defaults to 5). The benefit of these models is that they may provide better estimates of what the imputed value *would* have been, were it not missing, which may then improve model performance. The downside is that they are quite a bit more computationally intensive. Generally, you use recipes within processes like $k$-fold cross-validation, with the recipe being applied to each fold. In this case, a computationally expensive approach may significantly bog down hyperparameter tuning.

### A few words of caution
Missing data is a highly complex topic. This section was meant to provide a basic overview of some of the options you can choose from when building a predictive model. **None** of these approaches, however, will "fix" data that are missing not at random (MNAR). Unfortunately, it is usually impossible to know if your data are MNAR, and we therefore assume that that are MAR, or missing at random conditional on the observed data. For example, if boys were more likely to have missing data on the outcome than girls, we could account for this by including a gender variable in the model, and the resulting data would be MAR.

If you have significant missing data, this section is surely incomplete. We recommended [Little and Rubin (2002)](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563) previously, and there are a number of other good resources, including [a chapter](http://www.feat.engineering/handling-missing-data.html) in Kuhn and Johnson (2019).

## Transformations
In standard inferential statistics, we are often concerned with the distribution of the outcome. Linear regression, for example, assumes the outcome is at least reasonably normally distributed. If this is not the case, the standard errors (in particular) can be misrepresented. We therefore generally inspect the outcome before modeling it and, if it is not approximately normally distributed, we  either transform it to make it more closely approximate a normal distribution, or we use an analysis technique that does not assume normality in the outcome.

In predictive modeling, transformations of the predictors or the outcome(s) (or both) can sometimes help improve model performance. For example, let's quickly simulate some data.

```{r }
set.seed(3)

# parameters
alpha <- 10
b1 <- 5

# simulate predictor variable
x <- 1:100
log_x <- log(x)

# residual SD
e <- rnorm(length(x), sd = 0.8)

y <- alpha + b1*log_x + e

sim <- data.frame(x, y)
```

```{r echo = FALSE}
sim %>% 
  mutate_all(round, 2) %>% 
  reactable()
```

As you can see from the above, we have simulated the data according to $\log x$, but in our  data frame  only has $x$. This is a common situation where we don't *know* the true functional form. But of course, if we fit a linear regression model to these data, we'll end up with high bias, particularly in the lower tail (and issues with heteroscedasticity).

```{r }
ggplot(sim, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE)
```

But all we have to do, in this case, is use a log transformation to the $x$ variable and our linear model fits great.

```{r }
sim %>% 
  mutate(log_x = log(x)) %>% 
  head()

sim %>% 
  mutate(log_x = log(x)) %>% 
  ggplot(aes(log_x, y)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE)
```

Note that the model is linear in the transformed units, but curvilinear on the raw scale.

```{r }
sim %>% 
  mutate(log_x = log(x)) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              formula = y ~ log(x))
```

So in this case, a log transformation to the x variable works perfect (as we would expect, given that we simulated the data to be this way). But how do we know *how* to transform variables?

### Box-Cox and similar transformations
A more general formula for transforming variables is given by the [Box-Cox transformation](https://en.wikipedia.org/wiki/Power_transform#Box–Cox_transformation), defined by

$$
\begin{equation}
 x^* =
    \begin{cases}
      \frac{x^\lambda-1}{\lambda}, & \text{if}\ \lambda \neq 0 \\
       \log\left(x\right), & \text{if}\ \lambda = 0
    \end{cases}
\end{equation}
$$
where $x$ represents the variable in its raw units, and $x^*$ represents the transformed variable. The Box-Cox transformation is a *power* transformation, where the intent is to estimate $\lambda$. Note that if $\lambda$ is estimated as zero, the power transformation is the same as a log transformation, otherwise the top portion of the equation is used. Helpfully, specific values of $\lambda$ map to common transformations.

* $\lambda = 1$: No transformation
* $\lambda = 0.5$: square root transformation
* $\lambda = 0$: log transformation
* $\lambda = -1$: inverse

Given the above, we would expect that $\lambda$ would be estimated close to zero with our simulated data. Let's try using {recipes}. To access the actual $\lambda$ value, we'll need to take a brief foray into tidying recipes.

#### Tidying recipes
Let's first specify the recipe with a Box-Cox transformation to our x variable

```{r }
rec <- recipe(y ~ x, data = sim) %>% 
  step_BoxCox(all_predictors())
```

Now we can tidy the recipe

```{r }
tidy(rec)
```

In this case, our recipe is incredibly simple. We have one step, which is a Box-Cox transformation. Let's make the recipe a bit more complicated just for completeness.

```{r }
rec <- recipe(y ~ x, data = sim) %>% 
  step_impute_linear(all_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_BoxCox(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes())
```

Most of these steps won't do anything in this case, but let's look at the tidied recipe now.

```{r }
tidy(rec)
```

Now we have four steps. We can look at any one step by declaring the step number. Let's look at the linear imputation

```{r }
tidy(rec, n = 1)
```

Notice there's nothing there, because at this point the recipe is still just a blueprint. We have to `prep` the recipe if we want it to actually do any work. Let's prep the recipe and try again.

```{r }
lm_imputation <- rec %>% 
  prep() %>% 
  tidy(n = 1)

lm_imputation
```

And now we can see a linear model has been fit. We can even access the model itself.

```{r }
lm_imputation$model
```

What we get is actually a list of models, one for each predictor. But in this case there's only one predictor, so the list is only of length 1. 

#### Estimating $\lambda$
We can do the same thing to find $\lambda$ by tidying the Box-Cox step

```{r }
rec %>% 
  prep() %>% 
  tidy(n = 3)
```

And without any further work we can see that we estimated $\lambda = 0.72$, which is pretty much directly between a square-root transformation and no transformation. Why did it not estimate a $log$ transformation as most appropriate? Because the log transformation is only ideal when view *relative* to $y$. Put differently, the Box-Cox transformation is an *unsupervised* approach that attempts to make each variable approximate a univariate normal distribution. As we'll see in the next section, there are other methods that can be used to help with issues of non-linearity. 

For completeness, let's see if the transformation helped us. We'll use $\lambda = 0.72$ to manually transform $x$, then plot the result. 

```{r }
# transform x
sim <- sim %>% 
  mutate(x_bc = ((x^0.72) - 1) / 0.72)

# fit the model using the transformed data
m <- lm(y ~ x_bc, sim)

# add the model predictions to the data frame
sim <- sim %>% 
  mutate(pred = predict(m))

# plot the model fit using raw data on the x-axis
ggplot(sim, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = pred))
```

As we can see, it's better than the raw data, but still insufficient.

### An applied example
Let's look at an applied example. We'll use the `violence` data (see the full data dictionary [here](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized#)), and see if we can predict the neighborhoods where the number of murders are greater than zero, using the percentage of people living in poverty and the percentage of people living in dense housing units (more than one person per room) as predictors. Let's start with a basic plot.

```{r }
violence <- read_csv(here::here("data", "violence.csv"))

violence <- violence %>% 
  mutate(murder = ifelse(murders > 0, "Yes", "No"))

ggplot(violence, aes(pctPoverty, houseVacant)) +
  geom_point(aes(color = murder),
             alpha = 0.5,
             stroke = 0)
```

As you can see, it's pretty difficult to see much separation here. Let's look at the univariate views of each predictor.

```{r message = FALSE}
ggplot(violence, aes(pctPoverty)) +
  geom_histogram()

ggplot(violence, aes(pctPopDenseHous)) +
  geom_histogram()
```

Both predictors are quite skewed. What do they look like after transformation? 

```{r, warning = FALSE}
murder_rec <- recipe(murder ~ ., violence) %>% 
  step_BoxCox(all_numeric(), -all_outcomes()) 

transformed_murder <- murder_rec %>% 
  prep() %>% 
  bake(new_data = NULL) 

ggplot(transformed_murder, aes(pctPoverty)) +
  geom_histogram()

ggplot(transformed_murder, aes(pctPopDenseHous)) +
  geom_histogram()
```

Each of these look considerably better. What about the bivariate view?

```{r }
ggplot(transformed_murder, aes(pctPoverty, pctPopDenseHous)) +
  geom_point(aes(color = murder),
             alpha = 0.5,
             stroke = 0)
```

We can much more clearly see the separation here. We could almost draw a diagonal line in the data separating the classes, as below

```{r echo = FALSE}
ggplot(transformed_murder, aes(pctPoverty, pctPopDenseHous)) +
  geom_point(aes(color = murder),
             alpha = 0.5,
             stroke = 0) +
  geom_abline(intercept = 4, slope = -1.1, 
              color = "#5E8192",
              size = 1.4)
```

There's of course still some misclassification going on here, and that line was drawn by just eye-balling it, but even by hand we can do this much easier after the transformation.

What were the lambda values estimated at for these variables? Let's check.

```{r warning = FALSE}
murder_rec %>% 
  prep() %>% 
  tidy(1) %>% 
  filter(terms %in% c("pctPoverty", "pctPopDenseHous"))
```

Both are fairly close to zero, implying they are similar to log transformations.

## Nonlinearity

## PCA

## Wrapping up
