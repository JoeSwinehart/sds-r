# Boosting and Boosted Trees

```{r include = FALSE, cache = FALSE}
source("_common.R")

library(tidyverse)
library(tidymodels)
knitr::opts_chunk$set(warning = FALSE,
                      cache = TRUE)
```

Similar to bagging, boosting is a general algorithm that can be applied to any model. Frequently, however, it is applied to trees. Recall that when we use bagging (often through bagged tree or random forest models), we needed to have a sufficient number of bags, or bootstrap resamples, of the training data so our model performance metrics stabilize. Including additional bags (trees) once the performance has stabilized will not *hurt* in terms of model performance, but it will take longer to fit (i.e., more computationally expensive).

Similar to bagging, boosting works by building an ensemble of models. However, rather than each model being built independently on a new bootstrap resample of the original data, the *subsequent models are constructed from the residuals of the previous model*. This means that the number of models used controls how much is learned from the data and is an important hyperparameter.

:::lightbulb

:::title
Boosting versus Bagging
:::

* **Bagged** models use $b$ bootstrap resamples of the training data. Models are built independently on each resample, and model results are aggregated to form a single prediction

* **Boosted** models are built *sequentially*, with each model fit to the residuals of the previous model

* For bagged models, $b$ needs to be sufficiently large that performance metrics stabilize. Additional bags beyond this point only hurt computational efficiency.

* For boosted models, the number of models determines how much is *learned* from the data: too few, and the model will underfit (leading to excess bias), too many and the model will overfit (leading to excess variance).

:::

The model that is iteratively fit when applying boosting is called the **base learner**. This can be any model but is regularly a decision tree because, in part, it is relatively straightforward to control how much each individual tree learns from the data (i.e., shallow trees will learn less from the data than deeper trees).
 
:::concept
**Base learner**: The model that is iteratively fit within a boosting algorithm. Often these are shallow decision trees, as in a boosted tree model.

**Weak models & slow learning**: Each model builds off the errors of the previous model. To find an optimal solution, we want our base learner to *learn slowly*, so the amount that is learned is controlled by the number of models. 
:::

We also typically want our base learner to be a **weak model**, otherwise known as a **slow learner**. Imagine, for example, that you were wearing a blindfold in a meadow with many small hills. Some are as high as ten feet above where you are currently, while other dip beneath you up to about five feet. You are challenged to find the *lowest* point in the valley, while remaining blindfolded. You are given an additional directive that the size of your step *must be the same* each time. If you take off at a sprint you may, beyond falling on your face, accidentally skip right over the lowest point because the length of your stride would be sufficient to carry you over. Further, you would not even be able to *get* to the lowest point, because each subsequent stride may carry you over again (remember, each stride must be the same length). This is equivalent to a model that learns fast. You would likely get close quickly (running downhill), but your final solution may not be optimal. 

If, on the other hand, you take smaller, more careful steps, it would take you longer to get to the lowest point, but your stride would be less likely to carry you over the lowest point, and you would be able to feel when the gradient beneath you steepens. This does not guarantee you'll find the lowest point, of course, but there's probably a better chance than you'd have if you were sprinting. This is the idea behind a slow learner. We take more steps (number of models we fit), but each gets us closer to the optimal solution.

How do we determine what's optimal? We us our objective function, along with a process called gradient descent. Our objective function tells us the criteria that we want to minimize or maximize. We can, theoretically, evaluate the objective function for any model parameters. We use gradient descent to optimize the model parameters with respect to the objective function (which we maximize or minimize). 

## Gradient descent
Gradient descent is a general purpose optimization algorithm that is used (or most frequently, a variant is used) throughout many machine learning applications. When thinking about gradient descent conceptually, the scenario described previously of walking around a meadow blindfolded is again useful. If we were trying to find the lowest point, we would probably feel around us and find the direction that seemed the steepest. We would then take a step in that direction. If we were being careful, we would check the ground around us again after each step, evaluating the gradient immediately around us, and then continuing on in the steepest direction. This is the basic idea of gradient descent. We start off in a random location on some surface, and we step in the steepest direction until we can't go down any further.

Consider a simple example with a linear regression model. First, let's simulate some data.

```{r }
set.seed(42)
n <- 1000
x <- rnorm(n)

a <- 5
b <- 1.3
e <- 4

y <- a + b*x + rnorm(n, sd = e)

sim_d <- tibble(x = x, y = y)

ggplot(sim_d, aes(x, y)) +
  geom_point() 
```

We can estimate the relation between $x$ and $y$ using standard OLS, as follows:

```{r }
sim_ols <- lm(y ~ x)
summary(sim_ols)
```

This not only provides us the best linear unbiased estimate (BLUE) of the relation between $x$ and $y$, but it is computed extremely fast (around a tenth of a second on my computer).

Let's see if we can replicate these values using gradient descent. We will be estimating two parameters, the intercept and the slope of the line (as above). Our objective function is the mean squared error. That is, we want to find the line running through the data that minimizes the average distance between the line and the points. In the case of simple linear regression, the mean square error is defined by

$$
\frac{1}{N} \sum_{i=1}^{n} (y_i - (a + bx_i ))^2
$$

where $a$ is the intercept of the line and $b$ is the slope of the line. Let's write a function in R that computes the mean square error for any line. 

```{r }
mse <- function(a, b, x = sim_d$x, y = sim_d$y) {
  prediction <- a + b*x # model prediction, given intercept/slope
  residuals <- y - prediction # distance between prediction & observed
  squared_residuals <- residuals^2 # squared to avoid summation to zero
  ssr <- sum(squared_residuals) # sum of squared distances
  
  mean(ssr) # average of squared distances
}
```

Notice in the above we pre-defined the `x` and the `y` values to be from our simulated data, but the function is general enough to compute the mean squared error for any set of data. 

Just to confirm that our function works, let's check that our MSE with the OLS coefficients matches what we get from the model.

```{r }
mse(a = coef(sim_ols)[1], b = coef(sim_ols)[2])
```

Is this the same thing we get if we compute it from the model residuals?

```{r }
sum(resid(sim_ols)^2)
```

It is! 

So now we have a general function that can be used to evaluate our objective function for *any* intercept/slope combination. We can then, theoretically, evaluate infinite combinations and find the lowest value. Let's look at several hundred combinations. 

```{r }
# candidate values
grid <- expand.grid(a = seq(-5, 10, 0.1), b = seq(-5, 5, 0.1)) %>% 
  as_tibble()
grid
```

We could, of course, overlay all of these on our data, but that would be really difficult to parse through with `r format(nrow(grid), big.mark = ",")` candidate lines. Let's compute the MSE for each candidate.

```{r }
mse_grid <- grid %>% 
  rowwise(a, b) %>% 
  summarize(mse = mse(a, b), .groups = "drop")

mse_grid
```

Let's actually look at this grid

```{r echo = FALSE, eval = FALSE}
surface <- ggplot(mse_grid, aes(a, b)) +
    geom_raster(aes(fill = mse)) +
     colorspace::scale_fill_continuous_sequential(palette = "Terrain") +
  guides(fill = "none")

library(rayshader)

plot_gg(surface, multicore = TRUE)
render_movie(filename = here::here("img", "cost-surface.mp4"), 
             title_text = 'MSE surface', 
             phi = 30 , theta = -45)
```

![](img/cost-surface.mp4)

Notice this is basically just a big valley because this is a very simple (and linear) problem. We want to find the combination that minimizes the MSE which, in this case, is:

```{r }
mse_grid %>% 
  arrange(mse) %>% 
  slice(1)
```

How does this compare to what we estimated with OLS?

```{r }
coef(sim_ols)
```

Pretty similar. 

But this still isn't gradient descent. This is basically just a giant search algorithm that is only feasible because the problem is so simple. 

So what is gradient descent and how do we implement it? Conceptually, it's similar to our blindfolded walk - we start in a random location and try to walk downhill until we get to what we think is the lowest point. A more technical description is given in the box below.

:::dictionary

:::title
Gradient Descent
:::

* Define a cost function (such as MSE). 
* Calculate the partial derivative of each parameter of the cost function. These provide the gradient (steepness), and the direction the algorithm needs to "move" to minimize the cost function.
* Define a learning rate. This is the size of the "step" we take downhill. 
* Multiply the learning rate by the partial derivative value (this is how we actually "step" down).
* Estimate a new gradient and continue iterating ($\text{gradient} \rightarrow \text{step} \rightarrow \text{gradient} \rightarrow \text{step} \dots$) until no further improvements are made. 

:::

Let's try applying gradient descent to our simple linear regression problem. First, we have to take the partial derivative of each parameter, $a$ and $b$, for our cost function. The is defined as

$$
   \begin{bmatrix}
     \frac{d}{da}\\
     \frac{d}{db}\\
    \end{bmatrix}
=
   \begin{bmatrix}
     \frac{1}{N} \sum -2(y_i - (a + bx_i)) \\
     \frac{1}{N} \sum -2x_i(y_i - (a + bx_i)) \\
    \end{bmatrix}
$$

Let's write a function to calculate the gradient (partial derivative) for any values of the two parameters. Similar to the `mse()` function we wrote, we'll define this assuming the `sim_d` data, but have it be general enough that other data could be provided.

```{r }
compute_gradient <- function(a, b, x = sim_d$x, y = sim_d$y) {
  n <- length(y)
  predictions <- a + (b * x)
  residuals <- y - predictions
  
  da <- (1/n) * sum(-2*residuals)
  db <- (1/n) * sum(-2*x*residuals)
  
  c(da, db)
}
```

Great! Next, we'll write a function that uses the above function to calculate the gradient, but then actually takes a step in that direction. We do this by first multiplying our partial derivatives by our learning rate (the size of each step) and then subtracting that value from whatever the parameters are currently. We subtract because we're trying to go "downhill". If we were trying to maximize our objective function, we would add these values to our current parameters (technically gradient *ascent*).

:::concept
**Learning rate** defines the size of the step we take downhill. Higher learning rates will get us closer to the optimal solution faster, but may "step over" the minimum. When training a model, start with a relatively high learning rate (e.g., $0.1$) and adjust as needed. Before finalizing your model, consider reducing the learning rate to ensure you've found the global minimum. 
:::

```{r}
gd_step <- function(a, b, 
                    learning_rate = 0.1, 
                    x = sim_d$x, 
                    y = sim_d$y) {
  grad <- compute_gradient(a, b, x, y)
  step_a <- grad[1] * learning_rate
  step_b <- grad[2] * learning_rate
  
  c(a - step_a, b - step_b)
}
```

And finally, we choose a random location to start, and begin our walk! Let's begin at 0 for each parameter.

```{r }
walk <- gd_step(0, 0)
walk
```

After just a single step, our parameters have changed quite a bit. Remember that our true values are 5 and 1.3. Both parameters appear to be heading in the right direction.
Let's take a few more steps. Notice that in the below we're taking a step from the previous location.

```{r }
walk <- gd_step(walk[1], walk[2])
walk

walk <- gd_step(walk[1], walk[2])
walk

walk <- gd_step(walk[1], walk[2])
walk
```

Our parameters continue to head in the correct direction. However, the amount that the values change gets less with each iteration. This is because the gradient is less steep. So our "stride" is not carrying us as far, even though the size of our step is the same.

Let's speed this up a bit (although you could continue on to "watch" the parameters change) by using a loop to quickly take 25 more steps downhill.

```{r }
for(i in 1:25) {
  walk <- gd_step(walk[1], walk[2])
}
walk
```

And now we are very close! What if we took 25 **more** steps?

```{r }
for(i in 1:25) {
  walk <- gd_step(walk[1], walk[2])
}
walk
```

We get *almost* exactly the same thing. Why? Because we were already basically there. If we continue to try to go downhill we just end up walking around in circles. 

Let's rewrite our functions a bit to make the results a little easier to store and inspect later.

```{r }
estimate_gradient <- function(pars_tbl, learning_rate = 0.1, x = sim_d$x, y = sim_d$y) {
  
  pars <- gd_step(pars_tbl[["a"]], pars_tbl[["b"]],
                  learning_rate)
  
  tibble(a = pars[1], b = pars[2], mse = mse(a, b, x, y))
}

# initialize
grad <- estimate_gradient(tibble(a = 0, b = 0))

# loop through
for(i in 2:50) {
  grad[i, ] <- estimate_gradient(grad[i - 1, ])
}
grad
```

Finally, let's add our iteration number into the data frame, and plot it.

```{r }
grad <- grad %>% 
  rowid_to_column("iteration")

ggplot(grad, aes(iteration, mse)) +
  geom_line()
```

As we would expect, the MSE drops very quickly as we start to walk downhill (with each iteration) but eventually (around 20 or so iterations) starts to level out when no more progress can be made.

Let's look at this slightly differently. by looking at our cost surface, and how we step down the cost surface. 

```{r echo = FALSE}
ggplot(mse_grid, aes(a, b)) +
    geom_raster(aes(fill = mse)) +
     colorspace::scale_fill_continuous_sequential(palette = "Terrain") +
  guides(fill = "none") +
  geom_point(data = grad,
             color = "gray40",
             alpha = 0.5,
             stroke = 0)
```

You can see that, as we would expect, the algorithm takes us straight "downhill". 

Finally, because this is simple linear regression, we can also plot out the line through the iterations (as the algorithm "learns" the optimal intercept/slope combination).

```{r }
ggplot(sim_d, aes(x, y)) +
  geom_point() +
  geom_abline(aes(intercept = a, slope = b),
              data = grad,
              color = "gray60",
              size = 0.3) +
  geom_abline(aes(intercept = a, slope = b),
              data = grad[nrow(grad), ],
              color = "magenta")
```

Or, just for fun, we could animate it. 

```{r }
library(gganimate)
ggplot(grad) +
  geom_point(aes(x, y), sim_d) +
  geom_smooth(aes(x, y), sim_d, 
              method = "lm", se = FALSE) +
  geom_abline(aes(intercept = a,
                  slope = b),
              color = "#de4f60") +
  transition_manual(frames = iteration)
```


So in the end, gradient descent gets us essentially the exact same answer we get with OLS. So why would we use this approach instead? Well, if we're estimating a model with something like regression, we wouldn't. As is probably apparent, this approach is going to be more computationally expensive, particularly if we have a poor starting location. But in many cases we don't have a closed-form solution to the problem. This is where gradient descent (or a variant thereof) can help. In the case of boosted trees, we start by building weak model with only a few splits (including potentially only one). We then build a new (weak) model from the residuals of this model, using gradient descent to optimize each split in the tree relative to the cost function. This ensures that our ensemble of trees builds toward the minimum (or maximum) of our cost function.

As is probably clear, however, gradient descent is a fairly complicated topic. We chose to focus on a relatively "easy" implementation within a simple linear regression framework, which has only two parameters. There are a number of potential pitfalls related to gradient descent, including *exploding* gradients, where the search algorithm gets "lost" and basically wanders out into space. This can happen if the learning rate is too high. Normalizing your features so they are all on a common scale can also help prevent exploding gradients. *Vanishing* gradients is a similar problem, where the gradient is small enough that the new parameters do not change much and the algorithm gets "stuck". This is okay (and expected) if the algorithm has reached the global minimum, but is a problem if it's only at a *local* minimum (i.e., stuck in a valley on a hill). There are numerous variants of gradient descent that can help with these challenges, as we will discuss in subsequent chapters.

## Boosted trees
Like all boosted models, boosted trees are built with **weak models** so the overall algorithm **learns slowly**. We want the model to learn slowly so that it can pick up on the unique characteristics of the data and find the overall optimal solution. If it learns too quickly it may "jump over" the optimal solution.

In the [feature engineering] chapter, we discussed methods for handling things like nonlinearity through, for example, splines. When working through a linear regression framework, splines can create smooth curves that model any non-linear relations. Recall, however, that decision trees work by splitting the feature space into distinct regions. In other words, decision trees do not, by themselves, create curves. But if the model learns slowly enough it is possible to approximate curves, non-parametrically, even if we do not know the underlying functional form of the data a priori. 

For example, the figure below shows a boosted tree model fit to the same log data we saw previously using 1, 5, 50, 100, 200 and 500 trees. 

```{r echo = FALSE, fig.height = 12}
set.seed(3)

# parameters
alpha <- 10
b1 <- 5
x <- 1:100
log_x <- log(x)
e <- rnorm(length(x), sd = 0.8)

#outcome
y <- alpha + b1*log_x + e

sim <- data.frame(x, y)

library(gbm)
m <- gbm(
  formula = y ~ x,
  data = sim,
  interaction.depth = 1,
  distribution = "gaussian",  # SSE loss function
  n.trees = 500
)
preds <- map_df(c(1, 5, 50, 100, 200, 500), ~{
  tibble(trees = .x, 
         x = sim$x, 
         prediction = predict.gbm(m, n.trees = .x))
})

ggplot(preds, aes(x, prediction)) +
  geom_point(aes(y = y), data = sim, size = 1.5) +
  geom_line() +
  facet_wrap(~trees, ncol = 2)
```

Initially, the errors (average distance between a point the line) are extremely high. But as more trees are built from the residuals of the previous, the algorithm starts to learn the underlying functional form. Note that this is still not as good as what we would get if we just log transformed the $x$ variable and fit a simple linear regression model because, of course, that is how the data were generated. Once we get to 500 trees we could argue that the model is starting to overfit in some regions, while still badly underfitting at the bottom tail. This is an important example of the [no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) where, in this case, we have fit a relatively advanced machine learning model to a simple set of data, the results of which would be easily outperformed by simple linear regression. No single model will *always* work best across all applications. However, with proper hyperparameter tuning, boosted decision trees are regularly among the most performant "out of the box". 

### Hyperparameters and engines

:::lightbulb

:::title
Standard boosted tree hyperparameters
:::

* Number of trees
* Number of splits for each tree
* Minimum $n$ size for a terminal node
* Learning rate

:::

We have talked about each of the hyperparameters listed above previously. However, it's worth thinking more about how they work together in a boosted tree model. 

Generally, the minimum sample size for a terminal node will not factor in heavily with boosted tree models, because the trees are typically grown quite shallow (so the model learns slowly). The number of splits is often between one (a stump) and six, although occasionally models with deeper trees can work well. The depth of the tree (generally controlled more heavily by the number of splits than the terminal $n$ size) will determine the number of trees needed to arrive at an optimal solution. Deeper trees will learn more from the data, and will therefore generally require fewer trees. Deeper trees also allow for interactions among variables - i.e., the splits are conditional on previous splits, and thus the prediction depends on the interaction among the features in the tree. If important interactions in the data exist, the trees will need to be grown to a sufficient depth to capture them. An ensemble of stumps (trees with just a single split) are relatively common and computationally efficient, although they do generally require more trees, as each tree learn very little about the data (as illustrated above). Because they learn more slowly, they are less likely to overfit. These models will, however, miss any interactions that may be present in the data. 

:::concept
**Standard boosted tree tuning strategy**

1. Set learning rate around $0.1$
2. Tune the number of trees
3. Fix the number of trees, tune number of splits (and potentially minimum terminal $n$ size)
4. Fix tree depth & terminal $n$, tune learning rate
5. Fix all other hyperparameters, evaluate the number of trees again. If (more than marginally) different, set the number of trees to the new value and continue tuning.

:::

The learning rate is a new parameter that we haven't seen with any tree-based model previously (although we also didn't have to tune the *number* of trees previously, we just had to make sure we had a sufficient number to reach a stable estimate). We talked about the learning rate some in the section on [gradient descent] but it's worth reiterating that, along with the number of trees, the learning rate is perhaps the most important hyperparameter. If the learning rate is too high, the model is likely to "jump over" the optimal solution, and if it is too low the model may take far too long to reach an optimal solution. The learning rate is also called the *shrinkage* parameter because it is the amount by which we multiply the partial derivatives of our cost function (the gradient). Smaller values will result in smaller "steps" downhill, and in turn, the amount each tree learns. As a practical recommendation, we suggest starting with a learning rate around $0.1$. Values higher than this do not typically work well, in our experience, but it is high enough that the model should find a value *close* to the minimum relatively fast. Once you've conducted your model tuning on the other hyperparameters with this learning rate, consider reducing the learning rate to see if you can increase model performance further (e.g., to values of $0.01$ or $0.001$).

The [**{gbm}**](https://cran.r-project.org/package=gbm) package (for **g**radient **b**oosted **m**odels) is perhaps the most common package for estimating standard boosted trees. Throughout this book, however, we have been been using the [**{tidymodels}**](https://www.tidymodels.org) suite of packages and, although multiple boosting engines have been implemented, **{gbm}** is not one of them. Although it is possible to [add new engines or models](https://www.tidymodels.org/learn/develop/models/) to [**{parsnip}**](https://parsnip.tidymodels.org), the **{gbm}** package is in many ways a simpler implementation of boosted models than those that are already implemented in **{parsnip}**, specifically the [**{xgboost}**](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html) package, which includes a number of additional hyperparameters, as follows.

:::lightbulb

:::title
**{xgboost}** hyperparameters
:::

* Number of trees
* Number of splits for each tree
* Minimum $n$ size for a terminal node
* Learning rate

-----

* Number of predictors to be randomly sampled at each split
* Proportion of cases to sample for each *tree*
* Limiting tree depth by the cost function (reduction required to continue splitting the tree)
* Early stopping criteria (stop growing additional trees during training if there is no change in the cost function **on the validation set** for $k$ consecutive trees)

:::

As can be seen, the same hyperparameters for standard boosted trees are available when using the **{xgboost}** engine, but additional stochastic based parameters are also available (randomly sampling columns at each split and cases for each tree). The stochastic parameters are based in the same underlying principles that help improve the performance of bagged trees and random forests. Exposing only a sample of the data and features to the algorithm at a time may make the algorithm take a bit more time, as each tree is likely to learn less (making it learn even more slowly). But it may also pick up on meaningful aspects of the data that are otherwise missed. Of course, the extent to which including these stochastic components helps you model performance will depend on your data. For example, random selection of columns will likely help more when you have a few features that are strongly related to the outcome and many that are weakly related (i.e., so each subsequent tree isn't built by starting with the same features as the root node).

:::concept
**Tuning {xgboost} models**

The early stopping rules change the way we tune our model. We can use a very large number of trees with early stopping to 

* Tune the learning rate, then
* Tune tree depth, then
* Evaluate if stochastic components help, then
* Re-tune learning rate

If the cross-validation curve suggests overfitting, include regularization methods (e.g., limiting tree depth by changes in the cost function, L1 or L2 regularizers)

:::

Beyond these additional stochastic-based hyperparameters, **{xgboost}** provides an alternative method by which tree depth can be limited. Specifically, if the cost function is not reduced below a given threshold, the tree stops growing. Further, when tuning a model, **{xgboost}** allows for the use of *early stopping rule*. This works by evaluating changes in the gradient across iterations (trees) on the validation set during training. If no improvements are made after a specified number of trees, then the algorithm stops. Early stopping can provide *immense* gains in computational efficiency (by not continuing to grow a tree when no improvements are being) while also helping to prevent overfitting. Rather than being an additional parameter to tune, you can use early stopping to your advantage to only fit the number trees necessary, while being adaptive across different model specifications (e.g., tree depth). 

The **{xgboost}** package itself includes *even more* hyperparameters, including slightly alternative methods for fitting the model altogether. For example, the `dart` booster includes a random *dropout* component. The idea, which originated with neural nets, is that trees added early in the ensemble will be more important than trees added late in the ensemble. So, particularly in cases where the model overfits quickly, it can be helpful to randomly remove (drop) specific trees from the overall ensemble. This is a rather extreme regularization approach, but has been shown to improve performance in some situations. 

In what follows, we discuss implementation beginning with **{tidymodels}**. However, we will also provide examples with **{xgboost}** directly, including direct comparisons between the two implementations. We discuss both implementations because using **{xgboost}** directly can often lead to benefits in computational time. Further, as mentioned above, using **{xgboost}** directly allows for the inclusion of even more hyperparameters easily.

## Fitting boosted tree models
Let's use our same statewide testing data we have now used at numerous points, along with our same recipe.

```{r }
library(tidyverse)
library(tidymodels)
library(tictoc)
set.seed(41920)
full_train <- read_csv("https://github.com/uo-datasci-specialization/c4-ml-fall-2020/raw/master/data/train.csv") %>% 
  slice_sample(prop = 0.02) %>% 
  select(-classification)

splt <- initial_split(full_train)
train <- training(splt)
cv <- vfold_cv(train)
```

Next we'll specify a basic recipe. One thing to note about the below, generally we don't have to worry about dummy-coding variables for decision trees. However, dummy-coding (or one-hot encoding) *is* neccessary when using **{xgboost}**.

```{r }
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% 
  update_role(contains("id"), -ncessch, new_role = "id vars") %>%
  step_zv(all_predictors()) %>% 
  step_novel(all_nominal()) %>% 
  step_unknown(all_nominal()) %>% 
  step_medianimpute(all_numeric(), 
                    -all_outcomes(), 
                    -has_role("id vars"))  %>% 
  step_dummy(all_nominal(), 
             -has_role("id vars"),
             one_hot = TRUE) %>% 
  step_nzv(all_predictors(), freq_cut = 995/5)
```

XGBoost is "an optimized distributed gradient boosting library designed to be highly **efficient**" (emphasis in the original, from the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/)). The distributed part means that the models are built to work efficiently with parallel processing, and is (relative to alternative methods) extremely fast. It defaults to using the maximum number of threads available, but you can specify this if you want it to use fewer with the `nthread` argument passed to `set_engine()`. We'll also add additional arguments to specify a large number of trees with early stopping through the `trees` and `stop_iter` arguments.

```{r }
mod <- boost_tree() %>% 
  set_engine("xgboost") %>% # will use the maximum number of threads
  set_mode("regression") %>% 
  set_args(trees = 5000,
           stop_iter = 20)
```

Let's now estimate this model using the default hyperparameter values.

```{r }
tic()
boosted_fit1_tm <- fit_resamples(mod, rec, cv)
toc(log = TRUE)
```

Notice we're only working with a small portion of the overall sample and, despite what we just mentioned about speed, it still took a while. How does the default performance look?

```{r }
collect_metrics(boosted_fit1_tm)
```

