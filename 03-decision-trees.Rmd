# Decision Trees

```{r include = FALSE}
source("_common.R")

knitr::opts_chunk$set(warning = FALSE)
```

One of our favorite machine learning algorithms is decision trees. They are not the most complex models, but they are quite intuitive. A single decision tree generally doesn't have great "out of the box" model performance, and even with considerable model tuning they are unlikely to perform as well as other approaches. They do, however, form the building blocks for more complicated models that *do* have high out-of-the-box model performance and can produce state-of-the-art level predictions.

Decision trees are non-parametric, meaning they do not make any assumptions about the underlying data generating process. By contrast, models like linear regression assume that the underlying data were generated by a standard normal distribution (and, if this is not the case, the model will result is systematic biases - although we can also use transformations and other strategies to help; see [Feature Engineering]). Note that assuming an underlying data generating distribution is not a *weakness* of linear regression - often it's a tenable assumption and can regularly lead to better model performance, if the assumption holds. But decision trees do not require that you make any such assumptions, which is particularly helpful when it's difficult to assume a specific underlying data generating distribution.

At their core, decision trees work by *splitting* the features into a series of yes/no decisions. These splits divide the feature space into a series of non-overlapping regions, where the cases are similar in each region. To understand how a given prediction is made, one simply "follows" the splits of the tree to the terminal node. 

### A simple decision tree
Initially, we think it's easiest to think about decision trees through a classification lens. One of the most common example datasets for decision trees is the *titanic* dataset, which includes information on passengers aboard the titanic. The data look like this

<!-- Note - the data below is from kaggle: https://www.kaggle.com/c/titanic/data?select=train.csv -->

```{r }
library(tidyverse)
titanic <- read_csv(here::here("data", "titanic.csv"))
```

```{r echo = FALSE}
reactable::reactable(titanic, filterable = TRUE, highlight = TRUE)
```

Imagine we wanted to create a model that would predict if passengers aboard the titanic survived. A decision tree model might look something like this.

```{r echo = FALSE}
library(rpart)
library(rpart.plot)
titanic$Survived <- factor(titanic$Survived,
                           levels = c(0, 1),
                           labels = c("Died", "Survived"))
m <- rpart(Survived ~ Sex + Age + SibSp, data = titanic)
rpart.plot(m)
```

where `Sibsp` indicates the number of siblings/spouses onboard with the passenger. 

Let's talk about vocabulary here for a bit. At the **root node** is the sex of the passenger, which in this case is a binary indicator with two levels (`male` and `female`). Approximately 65% of all passengers were coded as male, while the remaining 35% were coded as female. For passengers coded female, we split again based on the number of siblings/spouses they had on-board the titanic. Those with three or more, we predict to survive (`Survived == 1`) 


